# Import necessary libraries
import pandas as pd
import numpy as np

# Visualizationdf_e.shape
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("fivethirtyeight")

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score




# Load the dataset
df_e = pd.read_csv('../data/processed/additional_data.csv')



import warnings
warnings.filterwarnings("ignore")









sns.histplot(df_e['price'],kde = True,bins=50)



q = df_e['price'].quantile(0.95)

df_e = df_e[df_e['price']<q]

sns.histplot(df_e['price'],kde = True,bins=50)





sns.histplot(df_e['rooms'],kde = True,bins=50)



df_e = df_e[df_e['rooms']<10]

sns.histplot(df_e['rooms'],kde = True,bins=50)





sns.histplot(df_e['m2'], kde = True, bins = 50)


q = df_e['m2'].quantile(0.95)

df_e = df_e[df_e['m2']<q]

sns.histplot(df_e['m2'],kde = True,bins=50)





df_e_without_duplicates = df_e.drop_duplicates()


df_e_without_duplicates.isna().sum()


df_e_without_duplicates_without_na = df_e_without_duplicates.dropna()


df_e_without_duplicates_without_na.isna().sum()


df_e_without_duplicates_without_na.shape


df = df_e_without_duplicates_without_na


# Create the new columns
df['house_type_'] = df['house_type']
df['floor'] = 'n/a'


import re
# Function to categorize house_type_ and floor
def categorize_house_type(house_type):
    match = re.match(r'^planta\s*(-?\d+)$', house_type)
    if match:
        return 'Apartment', match.group(1)
    else:
        return house_type, 'n/a'

# Apply the function to the DataFrame and create new columns
df[['house_type_', 'floor']] = df['house_type'].apply(lambda x: pd.Series(categorize_house_type(x)))

print(df)


df['house_type_'].value_counts()


df.drop(columns=['house_type'], inplace=True)


df['floor'].replace('n/a','0',inplace=True)


df['floor'].value_counts()


# Convert 'floor' column to numeric, setting errors='coerce' to handle invalid values
df['floor'] = pd.to_numeric(df['floor'], errors='coerce')

print(df['floor'].dtype)  # This should now print 'int64'



# Correlation heatmap
numeric_cols = df.select_dtypes(include=['float64','int64']).columns
df[numeric_cols].corr()['price'].sort_values(ascending=False)


#conda install -c conda-forge pycaret


from pycaret.regression import *


# Setup the environment in PyCaret
regression_setup = setup(
    data=df,
    #test_data=test_data,
    target = 'price',
    session_id=9,
    ignore_features=[],
    numeric_imputation = 'mean',
    categorical_imputation = 'mode',
    remove_multicollinearity=True,
    transformation= True,
    transformation_method = 'yeo-johnson',
    normalize = True,
    normalize_method = 'zscore'
    )


# compare baseline models
best_model = compare_models()


# predict on test set
holdout_pred = predict_model(best_model)



# show predictions df
holdout_pred[['price','prediction_label']].head()



