{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.22.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.26.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from selenium) (4.10.0)\n",
      "Collecting websocket-client>=1.8.0 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: exceptiongroup in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, websocket-client, sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.26.0 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/namratamayekar/anaconda3/envs/viz/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndriver = webdriver.Safari()\\nurl = \\'https://www.aproperties.es/en/\\'\\ndriver.get(url)\\nprint(driver.title)\\n\\nMadrid_data = []\\n\\naccept_all_button = driver.find_element(By.ID, \"CookiePolicyAcceptAll\")\\naccept_all_button.click()\\n\\nsearch_box = driver.find_element(By.ID, \"geoSearchText\")\\nsearch_box.send_keys(\"Madrid \")\\nsearch_box.send_keys(Keys.RETURN)\\n\\ntime.sleep(20)\\n\\npage_source = driver.page_source\\n\\n\\n# Parse the page source with BeautifulSoup\\nsoup = BeautifulSoup(driver.page_source, \\'html.parser\\')\\n\\n\\n# Find all ads (Replace \\'.house-ad-class\\' with the actual class)\\nads = soup.find_all(\\'div\\', class_=\\'propertyBlock\\') \\n\\nfor ad in ads:\\n    \\n    data = {}\\n    \\n    # Extract each field using the appropriate selectors\\n    data[\\'heading\\'] = ad.find(\\'h3\\', class_=\\'propertyBlock__title\\').text.strip() \\n    data[\\'location\\'] = ad.find(\\'div\\', class_=\\'propertyBlock__location\\').text.strip() \\n    data[\\'content\\'] = ad.find(\\'div\\', class_=\\'propertyBlock__content\\').text.strip() \\n\\n    id_tag = ad.find(\\'span\\', class_=\\'propertyBlock__reference\\')\\n    if id_tag:\\n        data[\\'id\\'] = id_tag.text.strip()\\n    else:\\n        data[\\'id\\'] = None\\n    \\n    area_tag = ad.find(\\'div\\', class_=\\'propertyBlock__surface\\')\\n    if area_tag:\\n        data[\\'area\\'] = area_tag.find(\\'span\\', class_=\\'propertyBlock__value\\').text.strip()\\n    else:\\n        data[\\'area\\'] = None\\n        \\n       \\n    bedroom_tag = ad.find(\\'div\\', class_=\\'propertyBlock__bedrooms\\')\\n    if bedroom_tag:\\n        data[\\'n_bedrooms\\'] = bedroom_tag.find(\\'span\\', class_=\\'propertyBlock__value\\').text.strip()\\n    else:\\n        data[\\'n_bedrooms\\'] = None\\n\\n    bathroom_tag = ad.find(\\'div\\', class_=\\'propertyBlock__bathrooms\\')\\n    if bathroom_tag:\\n        data[\\'n_bathrooms\\'] = bathroom_tag.find(\\'span\\', class_=\\'propertyBlock__value\\').text.strip()\\n    else:\\n        data[\\'n_bathrooms\\'] = None        \\n    \\n    price_tag = ad.find(\\'div\\', class_=\\'propertyBlock__price\\')\\n    if price_tag:\\n        data[\\'Price\\'] = price_tag.text.strip()\\n    else:\\n        data[\\'Price\\'] = None\\n    \\n    # Append data to the list\\n    Madrid_data.append(data)\\n\\n# Convert list to DataFrame for better manipulation and export\\ndf = pd.DataFrame(Madrid_data)\\n\\n# Export DataFrame to CSV\\ndf.to_csv(\\'Madrid_properties.csv\\', index=False)\\n\\ndriver.quit()\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "driver = webdriver.Safari()\n",
    "url = 'https://www.aproperties.es/en/'\n",
    "driver.get(url)\n",
    "print(driver.title)\n",
    "\n",
    "Madrid_data = []\n",
    "\n",
    "accept_all_button = driver.find_element(By.ID, \"CookiePolicyAcceptAll\")\n",
    "accept_all_button.click()\n",
    "\n",
    "search_box = driver.find_element(By.ID, \"geoSearchText\")\n",
    "search_box.send_keys(\"Madrid \")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(20)\n",
    "\n",
    "page_source = driver.page_source\n",
    "\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "# Find all ads (Replace '.house-ad-class' with the actual class)\n",
    "ads = soup.find_all('div', class_='propertyBlock') \n",
    "\n",
    "for ad in ads:\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Extract each field using the appropriate selectors\n",
    "    data['heading'] = ad.find('h3', class_='propertyBlock__title').text.strip() \n",
    "    data['location'] = ad.find('div', class_='propertyBlock__location').text.strip() \n",
    "    data['content'] = ad.find('div', class_='propertyBlock__content').text.strip() \n",
    "\n",
    "    id_tag = ad.find('span', class_='propertyBlock__reference')\n",
    "    if id_tag:\n",
    "        data['id'] = id_tag.text.strip()\n",
    "    else:\n",
    "        data['id'] = None\n",
    "    \n",
    "    area_tag = ad.find('div', class_='propertyBlock__surface')\n",
    "    if area_tag:\n",
    "        data['area'] = area_tag.find('span', class_='propertyBlock__value').text.strip()\n",
    "    else:\n",
    "        data['area'] = None\n",
    "        \n",
    "       \n",
    "    bedroom_tag = ad.find('div', class_='propertyBlock__bedrooms')\n",
    "    if bedroom_tag:\n",
    "        data['n_bedrooms'] = bedroom_tag.find('span', class_='propertyBlock__value').text.strip()\n",
    "    else:\n",
    "        data['n_bedrooms'] = None\n",
    "\n",
    "    bathroom_tag = ad.find('div', class_='propertyBlock__bathrooms')\n",
    "    if bathroom_tag:\n",
    "        data['n_bathrooms'] = bathroom_tag.find('span', class_='propertyBlock__value').text.strip()\n",
    "    else:\n",
    "        data['n_bathrooms'] = None        \n",
    "    \n",
    "    price_tag = ad.find('div', class_='propertyBlock__price')\n",
    "    if price_tag:\n",
    "        data['Price'] = price_tag.text.strip()\n",
    "    else:\n",
    "        data['Price'] = None\n",
    "    \n",
    "    # Append data to the list\n",
    "    Madrid_data.append(data)\n",
    "\n",
    "# Convert list to DataFrame for better manipulation and export\n",
    "df = pd.DataFrame(Madrid_data)\n",
    "\n",
    "# Export DataFrame to CSV\n",
    "df.to_csv('Madrid_properties.csv', index=False)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luxury Real Estate Agency - Barcelona Madrid Valencia Mallorca | aProperties\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Safari driver\n",
    "driver = webdriver.Safari()\n",
    "url = 'https://www.aproperties.es/en/'\n",
    "driver.get(url)\n",
    "print(driver.title)\n",
    "\n",
    "Madrid_data = []\n",
    "\n",
    "# Accept cookies\n",
    "accept_all_button = driver.find_element(By.ID, \"CookiePolicyAcceptAll\")\n",
    "accept_all_button.click()\n",
    "\n",
    "# Enter \"Spain\" in the search box and submit\n",
    "search_box = driver.find_element(By.ID, \"geoSearchText\")\n",
    "search_box.send_keys(\"Madrid city\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(20)\n",
    "\n",
    "# Find the dropdown div element\n",
    "area_box = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.ID, \"selectlook-area\"))\n",
    ")\n",
    "    \n",
    "# Click to open the dropdown\n",
    "area_box.click()\n",
    "    \n",
    "# Find the \"All\" option\n",
    "all_option = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located((By.CSS_SELECTOR, 'li.zones-option[data-area=\"0\"]'))\n",
    ")\n",
    "    \n",
    "# Scroll the \"All\" option into view\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", all_option)\n",
    "    \n",
    "# Use JavaScript to click the \"All\" option\n",
    "driver.execute_script(\"arguments[0].click();\", all_option)\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse the page and extract data\n",
    "def parse_page():\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    ads = soup.find_all('div', class_='propertyBlock') \n",
    "\n",
    "    for ad in ads:\n",
    "        data = {}\n",
    "        data['heading'] = ad.find('h3', class_='propertyBlock__title').text.strip() \n",
    "        data['location'] = ad.find('div', class_='propertyBlock__location').text.strip() \n",
    "        data['content'] = ad.find('div', class_='propertyBlock__content').text.strip() \n",
    "\n",
    "        id_tag = ad.find('span', class_='propertyBlock__reference')\n",
    "        if id_tag:\n",
    "            data['id'] = id_tag.text.strip()\n",
    "        else:\n",
    "            data['id'] = None\n",
    "\n",
    "        area_tag = ad.find('div', class_='propertyBlock__surface')\n",
    "        if area_tag:\n",
    "            data['area'] = area_tag.find('span', class_='propertyBlock__value').text.strip()\n",
    "        else:\n",
    "            data['area'] = None\n",
    "\n",
    "        bedroom_tag = ad.find('div', class_='propertyBlock__bedrooms')\n",
    "        if bedroom_tag:\n",
    "            data['n_bedrooms'] = bedroom_tag.find('span', class_='propertyBlock__value').text.strip()\n",
    "        else:\n",
    "            data['n_bedrooms'] = None\n",
    "\n",
    "        bathroom_tag = ad.find('div', class_='propertyBlock__bathrooms')\n",
    "        if bathroom_tag:\n",
    "            data['n_bathrooms'] = bathroom_tag.find('span', class_='propertyBlock__value').text.strip()\n",
    "        else:\n",
    "            data['n_bathrooms'] = None        \n",
    "\n",
    "        price_tag = ad.find('div', class_='propertyBlock__price')\n",
    "        if price_tag:\n",
    "            data['Price'] = price_tag.text.strip()\n",
    "        else:\n",
    "            data['Price'] = None\n",
    "\n",
    "        Madrid_data.append(data)\n",
    "\n",
    "# Loop through pages\n",
    "while True:\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//li[@class=\"pagination__next\"]/a')\n",
    "        if 'disabled' in next_button.get_attribute('class'):\n",
    "            break\n",
    "        next_button.click()\n",
    "        time.sleep(20)  # Adjust the waiting time based on your internet speed\n",
    "        parse_page()\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "\n",
    "\n",
    "# Convert list to DataFrame for better manipulation and export\n",
    "df = pd.DataFrame(Madrid_data)\n",
    "\n",
    "df.to_csv('../data/raw/Madrid_aproperties.csv', index=False)\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viz",
   "language": "python",
   "name": "viz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
