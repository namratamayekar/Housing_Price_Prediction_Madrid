2024-07-08 11:24:43,612:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-08 11:24:43,612:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-08 11:24:43,612:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-08 11:24:43,612:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-08 11:24:57,445:INFO:PyCaret RegressionExperiment
2024-07-08 11:24:57,445:INFO:Logging name: reg-default-name
2024-07-08 11:24:57,445:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-08 11:24:57,445:INFO:version 3.3.1
2024-07-08 11:24:57,445:INFO:Initializing setup()
2024-07-08 11:24:57,445:INFO:self.USI: f847
2024-07-08 11:24:57,445:INFO:self._variable_keys: {'exp_name_log', 'memory', 'target_param', 'USI', 'fold_groups_param', 'pipeline', 'fold_shuffle_param', 'X_train', 'html_param', 'X_test', '_ml_usecase', 'X', 'data', 'log_plots_param', 'y_test', 'idx', 'logging_param', '_available_plots', 'exp_id', 'gpu_n_jobs_param', 'seed', 'transform_target_param', 'y', 'y_train', 'fold_generator', 'gpu_param', 'n_jobs_param'}
2024-07-08 11:24:57,445:INFO:Checking environment
2024-07-08 11:24:57,445:INFO:python_version: 3.11.9
2024-07-08 11:24:57,445:INFO:python_build: ('main', 'Apr 19 2024 18:34:54')
2024-07-08 11:24:57,445:INFO:machine: arm64
2024-07-08 11:24:57,445:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-08 11:24:57,445:INFO:Memory: svmem(total=34359738368, available=15609626624, percent=54.6, used=15418687488, free=1026424832, active=13196722176, inactive=13939884032, wired=2221965312)
2024-07-08 11:24:57,445:INFO:Physical Core: 10
2024-07-08 11:24:57,445:INFO:Logical Core: 10
2024-07-08 11:24:57,445:INFO:Checking libraries
2024-07-08 11:24:57,445:INFO:System:
2024-07-08 11:24:57,445:INFO:    python: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:34:54) [Clang 16.0.6 ]
2024-07-08 11:24:57,445:INFO:executable: /Users/namratamayekar/anaconda3/envs/pycaret/bin/python
2024-07-08 11:24:57,445:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-08 11:24:57,445:INFO:PyCaret required dependencies:
2024-07-08 11:24:57,703:INFO:                 pip: 24.0
2024-07-08 11:24:57,703:INFO:          setuptools: 69.5.1
2024-07-08 11:24:57,703:INFO:             pycaret: 3.3.1
2024-07-08 11:24:57,703:INFO:             IPython: 8.24.0
2024-07-08 11:24:57,703:INFO:          ipywidgets: 8.1.2
2024-07-08 11:24:57,703:INFO:                tqdm: 4.66.4
2024-07-08 11:24:57,703:INFO:               numpy: 1.26.4
2024-07-08 11:24:57,703:INFO:              pandas: 2.1.4
2024-07-08 11:24:57,703:INFO:              jinja2: 3.1.4
2024-07-08 11:24:57,703:INFO:               scipy: 1.11.4
2024-07-08 11:24:57,703:INFO:              joblib: 1.3.2
2024-07-08 11:24:57,703:INFO:             sklearn: 1.4.2
2024-07-08 11:24:57,703:INFO:                pyod: 1.1.3
2024-07-08 11:24:57,703:INFO:            imblearn: 0.12.2
2024-07-08 11:24:57,703:INFO:   category_encoders: 2.6.3
2024-07-08 11:24:57,703:INFO:            lightgbm: 4.3.0
2024-07-08 11:24:57,703:INFO:               numba: 0.59.1
2024-07-08 11:24:57,703:INFO:            requests: 2.31.0
2024-07-08 11:24:57,703:INFO:          matplotlib: 3.8.4
2024-07-08 11:24:57,703:INFO:          scikitplot: 0.3.7
2024-07-08 11:24:57,703:INFO:         yellowbrick: 1.5
2024-07-08 11:24:57,703:INFO:              plotly: 5.22.0
2024-07-08 11:24:57,703:INFO:    plotly-resampler: Not installed
2024-07-08 11:24:57,703:INFO:             kaleido: 0.2.1
2024-07-08 11:24:57,703:INFO:           schemdraw: 0.15
2024-07-08 11:24:57,703:INFO:         statsmodels: 0.14.2
2024-07-08 11:24:57,703:INFO:              sktime: 0.26.0
2024-07-08 11:24:57,703:INFO:               tbats: 1.1.3
2024-07-08 11:24:57,703:INFO:            pmdarima: 2.0.4
2024-07-08 11:24:57,703:INFO:              psutil: 5.9.8
2024-07-08 11:24:57,703:INFO:          markupsafe: 2.1.5
2024-07-08 11:24:57,703:INFO:             pickle5: Not installed
2024-07-08 11:24:57,703:INFO:         cloudpickle: 3.0.0
2024-07-08 11:24:57,703:INFO:         deprecation: 2.1.0
2024-07-08 11:24:57,703:INFO:              xxhash: 3.4.1
2024-07-08 11:24:57,703:INFO:           wurlitzer: 3.1.0
2024-07-08 11:24:57,703:INFO:PyCaret optional dependencies:
2024-07-08 11:24:57,710:INFO:                shap: 0.45.1
2024-07-08 11:24:57,710:INFO:           interpret: 0.6.1
2024-07-08 11:24:57,711:INFO:                umap: 0.5.5
2024-07-08 11:24:57,711:INFO:     ydata_profiling: Not installed
2024-07-08 11:24:57,711:INFO:  explainerdashboard: Not installed
2024-07-08 11:24:57,711:INFO:             autoviz: Not installed
2024-07-08 11:24:57,711:INFO:           fairlearn: Not installed
2024-07-08 11:24:57,711:INFO:          deepchecks: Not installed
2024-07-08 11:24:57,711:INFO:             xgboost: Not installed
2024-07-08 11:24:57,711:INFO:            catboost: Not installed
2024-07-08 11:24:57,711:INFO:              kmodes: Not installed
2024-07-08 11:24:57,711:INFO:             mlxtend: Not installed
2024-07-08 11:24:57,711:INFO:       statsforecast: Not installed
2024-07-08 11:24:57,711:INFO:        tune_sklearn: Not installed
2024-07-08 11:24:57,711:INFO:                 ray: Not installed
2024-07-08 11:24:57,711:INFO:            hyperopt: Not installed
2024-07-08 11:24:57,711:INFO:              optuna: Not installed
2024-07-08 11:24:57,711:INFO:               skopt: Not installed
2024-07-08 11:24:57,711:INFO:              mlflow: 2.12.2
2024-07-08 11:24:57,711:INFO:              gradio: Not installed
2024-07-08 11:24:57,711:INFO:             fastapi: Not installed
2024-07-08 11:24:57,711:INFO:             uvicorn: Not installed
2024-07-08 11:24:57,711:INFO:              m2cgen: Not installed
2024-07-08 11:24:57,711:INFO:           evidently: Not installed
2024-07-08 11:24:57,711:INFO:               fugue: Not installed
2024-07-08 11:24:57,711:INFO:           streamlit: Not installed
2024-07-08 11:24:57,711:INFO:             prophet: Not installed
2024-07-08 11:24:57,711:INFO:None
2024-07-08 11:24:57,711:INFO:Set up data.
2024-07-08 11:24:57,727:INFO:Set up folding strategy.
2024-07-08 11:24:57,727:INFO:Set up train/test split.
2024-07-08 11:24:57,874:INFO:Set up index.
2024-07-08 11:24:57,875:INFO:Assigning column types.
2024-07-08 11:24:57,879:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-08 11:24:57,879:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,883:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,885:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,920:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,939:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,939:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:57,939:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:57,939:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,941:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,943:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,968:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,987:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,987:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:57,987:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:57,987:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-08 11:24:57,989:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-08 11:24:57,991:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,017:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,035:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,036:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,036:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,038:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,039:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,065:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,083:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,084:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,084:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,084:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-08 11:24:58,088:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,113:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,132:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,132:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,134:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,138:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,163:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,182:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,182:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,182:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,182:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-08 11:24:58,212:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,230:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,231:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,231:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,260:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,280:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,280:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,280:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,281:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-08 11:24:58,311:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,330:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,330:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,360:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-08 11:24:58,379:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,380:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,380:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-08 11:24:58,429:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,429:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,479:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,479:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,480:INFO:Preparing preprocessing pipeline...
2024-07-08 11:24:58,480:INFO:Set up simple imputation.
2024-07-08 11:24:58,483:INFO:Set up encoding of categorical features.
2024-07-08 11:24:58,632:INFO:Finished creating preprocessing pipeline.
2024-07-08 11:24:58,638:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/dn/cb8jd8r12bqdcnd1vmqkbgrw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms', 'buy_price'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['has_central_heating',
                                             'has_individual_heating',
                                             'has_lift', 'is_exterio...
                                             'HouseType'],
                                    transformer=OneHotEncoder(cols=['has_central_heating',
                                                                    'has_individual_heating',
                                                                    'has_lift',
                                                                    'is_exterior',
                                                                    'energy_certificate',
                                                                    'District',
                                                                    'HouseType'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['neighborhood'],
                                    transformer=TargetEncoder(cols=['neighborhood'],
                                                              handle_missing='return_nan')))])
2024-07-08 11:24:58,638:INFO:Creating final display dataframe.
2024-07-08 11:24:58,918:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (20885, 16)
4        Transformed data shape        (20885, 56)
5   Transformed train set shape        (14619, 56)
6    Transformed test set shape         (6266, 56)
7              Numeric features                  4
8          Categorical features                  8
9                    Preprocess               True
10              Imputation type             simple
11           Numeric imputation               mean
12       Categorical imputation               mode
13     Maximum one-hot encoding                 25
14              Encoding method               None
15               Fold Generator              KFold
16                  Fold Number                 10
17                     CPU Jobs                 -1
18                      Use GPU              False
19               Log Experiment              False
20              Experiment Name   reg-default-name
21                          USI               f847
2024-07-08 11:24:58,973:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:58,973:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:59,024:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:59,024:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-08 11:24:59,025:INFO:setup() successfully completed in 1.58s...............
2024-07-08 11:25:08,785:INFO:Initializing compare_models()
2024-07-08 11:25:08,785:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-08 11:25:08,786:INFO:Checking exceptions
2024-07-08 11:25:08,792:INFO:Preparing display monitor
2024-07-08 11:25:08,874:INFO:Initializing Linear Regression
2024-07-08 11:25:08,875:INFO:Total runtime is 3.0954678853352863e-06 minutes
2024-07-08 11:25:08,876:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:08,876:INFO:Initializing create_model()
2024-07-08 11:25:08,876:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:08,876:INFO:Checking exceptions
2024-07-08 11:25:08,876:INFO:Importing libraries
2024-07-08 11:25:08,876:INFO:Copying training dataset
2024-07-08 11:25:08,882:INFO:Defining folds
2024-07-08 11:25:08,882:INFO:Declaring metric variables
2024-07-08 11:25:08,884:INFO:Importing untrained model
2024-07-08 11:25:08,886:INFO:Linear Regression Imported successfully
2024-07-08 11:25:08,889:INFO:Starting cross validation
2024-07-08 11:25:08,895:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:11,707:INFO:Calculating mean and std
2024-07-08 11:25:11,709:INFO:Creating metrics dataframe
2024-07-08 11:25:11,713:INFO:Uploading results into container
2024-07-08 11:25:11,714:INFO:Uploading model into container now
2024-07-08 11:25:11,715:INFO:_master_model_container: 1
2024-07-08 11:25:11,715:INFO:_display_container: 2
2024-07-08 11:25:11,715:INFO:LinearRegression(n_jobs=-1)
2024-07-08 11:25:11,715:INFO:create_model() successfully completed......................................
2024-07-08 11:25:11,785:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:11,786:INFO:Creating metrics dataframe
2024-07-08 11:25:11,790:INFO:Initializing Lasso Regression
2024-07-08 11:25:11,790:INFO:Total runtime is 0.04859656492869059 minutes
2024-07-08 11:25:11,791:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:11,792:INFO:Initializing create_model()
2024-07-08 11:25:11,792:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:11,792:INFO:Checking exceptions
2024-07-08 11:25:11,792:INFO:Importing libraries
2024-07-08 11:25:11,792:INFO:Copying training dataset
2024-07-08 11:25:11,797:INFO:Defining folds
2024-07-08 11:25:11,797:INFO:Declaring metric variables
2024-07-08 11:25:11,799:INFO:Importing untrained model
2024-07-08 11:25:11,800:INFO:Lasso Regression Imported successfully
2024-07-08 11:25:11,803:INFO:Starting cross validation
2024-07-08 11:25:11,805:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:12,266:INFO:Calculating mean and std
2024-07-08 11:25:12,267:INFO:Creating metrics dataframe
2024-07-08 11:25:12,268:INFO:Uploading results into container
2024-07-08 11:25:12,268:INFO:Uploading model into container now
2024-07-08 11:25:12,268:INFO:_master_model_container: 2
2024-07-08 11:25:12,268:INFO:_display_container: 2
2024-07-08 11:25:12,269:INFO:Lasso(random_state=9)
2024-07-08 11:25:12,269:INFO:create_model() successfully completed......................................
2024-07-08 11:25:12,323:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:12,323:INFO:Creating metrics dataframe
2024-07-08 11:25:12,326:INFO:Initializing Ridge Regression
2024-07-08 11:25:12,326:INFO:Total runtime is 0.057529783248901366 minutes
2024-07-08 11:25:12,328:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:12,328:INFO:Initializing create_model()
2024-07-08 11:25:12,328:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:12,328:INFO:Checking exceptions
2024-07-08 11:25:12,328:INFO:Importing libraries
2024-07-08 11:25:12,328:INFO:Copying training dataset
2024-07-08 11:25:12,334:INFO:Defining folds
2024-07-08 11:25:12,334:INFO:Declaring metric variables
2024-07-08 11:25:12,336:INFO:Importing untrained model
2024-07-08 11:25:12,337:INFO:Ridge Regression Imported successfully
2024-07-08 11:25:12,340:INFO:Starting cross validation
2024-07-08 11:25:12,342:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:12,627:INFO:Calculating mean and std
2024-07-08 11:25:12,628:INFO:Creating metrics dataframe
2024-07-08 11:25:12,629:INFO:Uploading results into container
2024-07-08 11:25:12,629:INFO:Uploading model into container now
2024-07-08 11:25:12,629:INFO:_master_model_container: 3
2024-07-08 11:25:12,629:INFO:_display_container: 2
2024-07-08 11:25:12,629:INFO:Ridge(random_state=9)
2024-07-08 11:25:12,629:INFO:create_model() successfully completed......................................
2024-07-08 11:25:12,681:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:12,681:INFO:Creating metrics dataframe
2024-07-08 11:25:12,685:INFO:Initializing Elastic Net
2024-07-08 11:25:12,685:INFO:Total runtime is 0.06350844701131185 minutes
2024-07-08 11:25:12,686:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:12,686:INFO:Initializing create_model()
2024-07-08 11:25:12,687:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:12,687:INFO:Checking exceptions
2024-07-08 11:25:12,687:INFO:Importing libraries
2024-07-08 11:25:12,687:INFO:Copying training dataset
2024-07-08 11:25:12,692:INFO:Defining folds
2024-07-08 11:25:12,692:INFO:Declaring metric variables
2024-07-08 11:25:12,693:INFO:Importing untrained model
2024-07-08 11:25:12,695:INFO:Elastic Net Imported successfully
2024-07-08 11:25:12,698:INFO:Starting cross validation
2024-07-08 11:25:12,700:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:13,107:INFO:Calculating mean and std
2024-07-08 11:25:13,107:INFO:Creating metrics dataframe
2024-07-08 11:25:13,108:INFO:Uploading results into container
2024-07-08 11:25:13,108:INFO:Uploading model into container now
2024-07-08 11:25:13,109:INFO:_master_model_container: 4
2024-07-08 11:25:13,109:INFO:_display_container: 2
2024-07-08 11:25:13,109:INFO:ElasticNet(random_state=9)
2024-07-08 11:25:13,109:INFO:create_model() successfully completed......................................
2024-07-08 11:25:13,156:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:13,156:INFO:Creating metrics dataframe
2024-07-08 11:25:13,159:INFO:Initializing Least Angle Regression
2024-07-08 11:25:13,159:INFO:Total runtime is 0.07141139904658 minutes
2024-07-08 11:25:13,161:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:13,161:INFO:Initializing create_model()
2024-07-08 11:25:13,161:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:13,161:INFO:Checking exceptions
2024-07-08 11:25:13,161:INFO:Importing libraries
2024-07-08 11:25:13,161:INFO:Copying training dataset
2024-07-08 11:25:13,166:INFO:Defining folds
2024-07-08 11:25:13,166:INFO:Declaring metric variables
2024-07-08 11:25:13,167:INFO:Importing untrained model
2024-07-08 11:25:13,168:INFO:Least Angle Regression Imported successfully
2024-07-08 11:25:13,171:INFO:Starting cross validation
2024-07-08 11:25:13,172:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:13,353:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.606e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,353:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.197e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,355:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.291e+03, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,357:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=7.699e+03, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,360:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.988e+04, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,366:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.988e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,366:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.778e+00, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,367:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.456e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,367:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.199e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,367:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.107e+00, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,367:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.049e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,368:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.270e+00, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,369:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.198e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,379:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.321e+01, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,384:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.099e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,386:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=6.848e+00, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,386:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.019e+00, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,387:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=8.082e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,387:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=8.062e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,400:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=6.769e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,400:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=3.018e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,401:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.049e+04, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,411:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.737e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,412:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.572e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,412:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.006e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,412:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.319e+00, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,413:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.034e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,413:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=3.398e+00, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,413:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.784e+00, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,414:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.041e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,414:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.343e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,414:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.000e+01, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,446:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.103e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,447:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.722e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,447:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.110e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,448:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.963e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,449:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=6.233e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,449:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.177e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,477:INFO:Calculating mean and std
2024-07-08 11:25:13,478:INFO:Creating metrics dataframe
2024-07-08 11:25:13,479:INFO:Uploading results into container
2024-07-08 11:25:13,479:INFO:Uploading model into container now
2024-07-08 11:25:13,479:INFO:_master_model_container: 5
2024-07-08 11:25:13,479:INFO:_display_container: 2
2024-07-08 11:25:13,479:INFO:Lars(random_state=9)
2024-07-08 11:25:13,479:INFO:create_model() successfully completed......................................
2024-07-08 11:25:13,526:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:13,526:INFO:Creating metrics dataframe
2024-07-08 11:25:13,530:INFO:Initializing Lasso Least Angle Regression
2024-07-08 11:25:13,530:INFO:Total runtime is 0.07759488423665366 minutes
2024-07-08 11:25:13,531:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:13,531:INFO:Initializing create_model()
2024-07-08 11:25:13,531:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:13,531:INFO:Checking exceptions
2024-07-08 11:25:13,532:INFO:Importing libraries
2024-07-08 11:25:13,532:INFO:Copying training dataset
2024-07-08 11:25:13,537:INFO:Defining folds
2024-07-08 11:25:13,537:INFO:Declaring metric variables
2024-07-08 11:25:13,539:INFO:Importing untrained model
2024-07-08 11:25:13,540:INFO:Lasso Least Angle Regression Imported successfully
2024-07-08 11:25:13,542:INFO:Starting cross validation
2024-07-08 11:25:13,545:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:13,729:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.988e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,730:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:718: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=6.947e+00, previous alpha=6.938e+00, with an active set of 24 regressors.
  warnings.warn(

2024-07-08 11:25:13,746:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.161e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,747:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.798e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,747:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.456e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,747:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.604e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,759:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.737e+00, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,760:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.572e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,761:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:718: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=3.287e+00, previous alpha=2.081e+00, with an active set of 35 regressors.
  warnings.warn(

2024-07-08 11:25:13,767:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:718: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 44 iterations, alpha=2.032e+00, previous alpha=2.032e+00, with an active set of 41 regressors.
  warnings.warn(

2024-07-08 11:25:13,771:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:718: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 48 iterations, alpha=1.695e+00, previous alpha=1.695e+00, with an active set of 41 regressors.
  warnings.warn(

2024-07-08 11:25:13,807:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.103e+01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,807:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.722e+00, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,808:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.110e+00, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,808:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.963e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,808:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:718: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=4.028e+00, previous alpha=2.901e+00, with an active set of 36 regressors.
  warnings.warn(

2024-07-08 11:25:13,809:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:718: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=4.465e+00, previous alpha=4.465e+00, with an active set of 28 regressors.
  warnings.warn(

2024-07-08 11:25:13,813:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.321e+01, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,814:WARNING:/Users/namratamayekar/anaconda3/envs/pycaret/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.099e+00, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-08 11:25:13,839:INFO:Calculating mean and std
2024-07-08 11:25:13,839:INFO:Creating metrics dataframe
2024-07-08 11:25:13,840:INFO:Uploading results into container
2024-07-08 11:25:13,840:INFO:Uploading model into container now
2024-07-08 11:25:13,840:INFO:_master_model_container: 6
2024-07-08 11:25:13,841:INFO:_display_container: 2
2024-07-08 11:25:13,841:INFO:LassoLars(random_state=9)
2024-07-08 11:25:13,841:INFO:create_model() successfully completed......................................
2024-07-08 11:25:13,890:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:13,890:INFO:Creating metrics dataframe
2024-07-08 11:25:13,893:INFO:Initializing Orthogonal Matching Pursuit
2024-07-08 11:25:13,893:INFO:Total runtime is 0.08365054925282797 minutes
2024-07-08 11:25:13,895:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:13,895:INFO:Initializing create_model()
2024-07-08 11:25:13,896:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:13,896:INFO:Checking exceptions
2024-07-08 11:25:13,896:INFO:Importing libraries
2024-07-08 11:25:13,896:INFO:Copying training dataset
2024-07-08 11:25:13,900:INFO:Defining folds
2024-07-08 11:25:13,900:INFO:Declaring metric variables
2024-07-08 11:25:13,902:INFO:Importing untrained model
2024-07-08 11:25:13,903:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-08 11:25:13,905:INFO:Starting cross validation
2024-07-08 11:25:13,907:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:14,180:INFO:Calculating mean and std
2024-07-08 11:25:14,181:INFO:Creating metrics dataframe
2024-07-08 11:25:14,182:INFO:Uploading results into container
2024-07-08 11:25:14,182:INFO:Uploading model into container now
2024-07-08 11:25:14,182:INFO:_master_model_container: 7
2024-07-08 11:25:14,182:INFO:_display_container: 2
2024-07-08 11:25:14,182:INFO:OrthogonalMatchingPursuit()
2024-07-08 11:25:14,182:INFO:create_model() successfully completed......................................
2024-07-08 11:25:14,229:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:14,230:INFO:Creating metrics dataframe
2024-07-08 11:25:14,233:INFO:Initializing Bayesian Ridge
2024-07-08 11:25:14,234:INFO:Total runtime is 0.08932290077209473 minutes
2024-07-08 11:25:14,235:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:14,235:INFO:Initializing create_model()
2024-07-08 11:25:14,235:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:14,236:INFO:Checking exceptions
2024-07-08 11:25:14,236:INFO:Importing libraries
2024-07-08 11:25:14,236:INFO:Copying training dataset
2024-07-08 11:25:14,241:INFO:Defining folds
2024-07-08 11:25:14,241:INFO:Declaring metric variables
2024-07-08 11:25:14,242:INFO:Importing untrained model
2024-07-08 11:25:14,244:INFO:Bayesian Ridge Imported successfully
2024-07-08 11:25:14,246:INFO:Starting cross validation
2024-07-08 11:25:14,248:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:14,560:INFO:Calculating mean and std
2024-07-08 11:25:14,561:INFO:Creating metrics dataframe
2024-07-08 11:25:14,562:INFO:Uploading results into container
2024-07-08 11:25:14,562:INFO:Uploading model into container now
2024-07-08 11:25:14,562:INFO:_master_model_container: 8
2024-07-08 11:25:14,562:INFO:_display_container: 2
2024-07-08 11:25:14,562:INFO:BayesianRidge()
2024-07-08 11:25:14,562:INFO:create_model() successfully completed......................................
2024-07-08 11:25:14,612:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:14,612:INFO:Creating metrics dataframe
2024-07-08 11:25:14,616:INFO:Initializing Passive Aggressive Regressor
2024-07-08 11:25:14,616:INFO:Total runtime is 0.09569007953008017 minutes
2024-07-08 11:25:14,617:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:14,617:INFO:Initializing create_model()
2024-07-08 11:25:14,617:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:14,617:INFO:Checking exceptions
2024-07-08 11:25:14,617:INFO:Importing libraries
2024-07-08 11:25:14,617:INFO:Copying training dataset
2024-07-08 11:25:14,623:INFO:Defining folds
2024-07-08 11:25:14,623:INFO:Declaring metric variables
2024-07-08 11:25:14,624:INFO:Importing untrained model
2024-07-08 11:25:14,626:INFO:Passive Aggressive Regressor Imported successfully
2024-07-08 11:25:14,628:INFO:Starting cross validation
2024-07-08 11:25:14,630:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:14,953:INFO:Calculating mean and std
2024-07-08 11:25:14,953:INFO:Creating metrics dataframe
2024-07-08 11:25:14,954:INFO:Uploading results into container
2024-07-08 11:25:14,954:INFO:Uploading model into container now
2024-07-08 11:25:14,955:INFO:_master_model_container: 9
2024-07-08 11:25:14,955:INFO:_display_container: 2
2024-07-08 11:25:14,955:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-08 11:25:14,955:INFO:create_model() successfully completed......................................
2024-07-08 11:25:15,006:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:15,006:INFO:Creating metrics dataframe
2024-07-08 11:25:15,010:INFO:Initializing Huber Regressor
2024-07-08 11:25:15,010:INFO:Total runtime is 0.10225627819697064 minutes
2024-07-08 11:25:15,011:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:15,011:INFO:Initializing create_model()
2024-07-08 11:25:15,011:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:15,011:INFO:Checking exceptions
2024-07-08 11:25:15,011:INFO:Importing libraries
2024-07-08 11:25:15,011:INFO:Copying training dataset
2024-07-08 11:25:15,016:INFO:Defining folds
2024-07-08 11:25:15,016:INFO:Declaring metric variables
2024-07-08 11:25:15,017:INFO:Importing untrained model
2024-07-08 11:25:15,019:INFO:Huber Regressor Imported successfully
2024-07-08 11:25:15,021:INFO:Starting cross validation
2024-07-08 11:25:15,023:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:15,604:INFO:Calculating mean and std
2024-07-08 11:25:15,604:INFO:Creating metrics dataframe
2024-07-08 11:25:15,605:INFO:Uploading results into container
2024-07-08 11:25:15,605:INFO:Uploading model into container now
2024-07-08 11:25:15,605:INFO:_master_model_container: 10
2024-07-08 11:25:15,605:INFO:_display_container: 2
2024-07-08 11:25:15,606:INFO:HuberRegressor()
2024-07-08 11:25:15,606:INFO:create_model() successfully completed......................................
2024-07-08 11:25:15,656:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:15,656:INFO:Creating metrics dataframe
2024-07-08 11:25:15,661:INFO:Initializing K Neighbors Regressor
2024-07-08 11:25:15,661:INFO:Total runtime is 0.1131057659784953 minutes
2024-07-08 11:25:15,662:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:15,662:INFO:Initializing create_model()
2024-07-08 11:25:15,662:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:15,662:INFO:Checking exceptions
2024-07-08 11:25:15,662:INFO:Importing libraries
2024-07-08 11:25:15,662:INFO:Copying training dataset
2024-07-08 11:25:15,668:INFO:Defining folds
2024-07-08 11:25:15,668:INFO:Declaring metric variables
2024-07-08 11:25:15,669:INFO:Importing untrained model
2024-07-08 11:25:15,670:INFO:K Neighbors Regressor Imported successfully
2024-07-08 11:25:15,673:INFO:Starting cross validation
2024-07-08 11:25:15,674:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:16,056:INFO:Calculating mean and std
2024-07-08 11:25:16,056:INFO:Creating metrics dataframe
2024-07-08 11:25:16,057:INFO:Uploading results into container
2024-07-08 11:25:16,058:INFO:Uploading model into container now
2024-07-08 11:25:16,058:INFO:_master_model_container: 11
2024-07-08 11:25:16,058:INFO:_display_container: 2
2024-07-08 11:25:16,058:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-08 11:25:16,058:INFO:create_model() successfully completed......................................
2024-07-08 11:25:16,106:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:16,107:INFO:Creating metrics dataframe
2024-07-08 11:25:16,110:INFO:Initializing Decision Tree Regressor
2024-07-08 11:25:16,110:INFO:Total runtime is 0.12059903144836427 minutes
2024-07-08 11:25:16,111:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:16,112:INFO:Initializing create_model()
2024-07-08 11:25:16,112:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:16,112:INFO:Checking exceptions
2024-07-08 11:25:16,112:INFO:Importing libraries
2024-07-08 11:25:16,112:INFO:Copying training dataset
2024-07-08 11:25:16,117:INFO:Defining folds
2024-07-08 11:25:16,117:INFO:Declaring metric variables
2024-07-08 11:25:16,118:INFO:Importing untrained model
2024-07-08 11:25:16,120:INFO:Decision Tree Regressor Imported successfully
2024-07-08 11:25:16,122:INFO:Starting cross validation
2024-07-08 11:25:16,123:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:16,482:INFO:Calculating mean and std
2024-07-08 11:25:16,482:INFO:Creating metrics dataframe
2024-07-08 11:25:16,483:INFO:Uploading results into container
2024-07-08 11:25:16,483:INFO:Uploading model into container now
2024-07-08 11:25:16,484:INFO:_master_model_container: 12
2024-07-08 11:25:16,484:INFO:_display_container: 2
2024-07-08 11:25:16,484:INFO:DecisionTreeRegressor(random_state=9)
2024-07-08 11:25:16,484:INFO:create_model() successfully completed......................................
2024-07-08 11:25:16,530:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:16,530:INFO:Creating metrics dataframe
2024-07-08 11:25:16,535:INFO:Initializing Random Forest Regressor
2024-07-08 11:25:16,535:INFO:Total runtime is 0.12767241795857748 minutes
2024-07-08 11:25:16,536:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:16,536:INFO:Initializing create_model()
2024-07-08 11:25:16,536:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:16,536:INFO:Checking exceptions
2024-07-08 11:25:16,536:INFO:Importing libraries
2024-07-08 11:25:16,536:INFO:Copying training dataset
2024-07-08 11:25:16,541:INFO:Defining folds
2024-07-08 11:25:16,541:INFO:Declaring metric variables
2024-07-08 11:25:16,542:INFO:Importing untrained model
2024-07-08 11:25:16,544:INFO:Random Forest Regressor Imported successfully
2024-07-08 11:25:16,546:INFO:Starting cross validation
2024-07-08 11:25:16,547:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:23,010:INFO:Calculating mean and std
2024-07-08 11:25:23,011:INFO:Creating metrics dataframe
2024-07-08 11:25:23,013:INFO:Uploading results into container
2024-07-08 11:25:23,013:INFO:Uploading model into container now
2024-07-08 11:25:23,013:INFO:_master_model_container: 13
2024-07-08 11:25:23,013:INFO:_display_container: 2
2024-07-08 11:25:23,013:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-08 11:25:23,013:INFO:create_model() successfully completed......................................
2024-07-08 11:25:23,065:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:23,065:INFO:Creating metrics dataframe
2024-07-08 11:25:23,070:INFO:Initializing Extra Trees Regressor
2024-07-08 11:25:23,070:INFO:Total runtime is 0.23659491539001465 minutes
2024-07-08 11:25:23,072:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:23,072:INFO:Initializing create_model()
2024-07-08 11:25:23,072:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:23,072:INFO:Checking exceptions
2024-07-08 11:25:23,072:INFO:Importing libraries
2024-07-08 11:25:23,072:INFO:Copying training dataset
2024-07-08 11:25:23,078:INFO:Defining folds
2024-07-08 11:25:23,078:INFO:Declaring metric variables
2024-07-08 11:25:23,080:INFO:Importing untrained model
2024-07-08 11:25:23,081:INFO:Extra Trees Regressor Imported successfully
2024-07-08 11:25:23,084:INFO:Starting cross validation
2024-07-08 11:25:23,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:30,706:INFO:Calculating mean and std
2024-07-08 11:25:30,708:INFO:Creating metrics dataframe
2024-07-08 11:25:30,710:INFO:Uploading results into container
2024-07-08 11:25:30,710:INFO:Uploading model into container now
2024-07-08 11:25:30,710:INFO:_master_model_container: 14
2024-07-08 11:25:30,710:INFO:_display_container: 2
2024-07-08 11:25:30,710:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-08 11:25:30,710:INFO:create_model() successfully completed......................................
2024-07-08 11:25:30,774:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:30,774:INFO:Creating metrics dataframe
2024-07-08 11:25:30,779:INFO:Initializing AdaBoost Regressor
2024-07-08 11:25:30,779:INFO:Total runtime is 0.3650836984316508 minutes
2024-07-08 11:25:30,781:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:30,781:INFO:Initializing create_model()
2024-07-08 11:25:30,781:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:30,781:INFO:Checking exceptions
2024-07-08 11:25:30,781:INFO:Importing libraries
2024-07-08 11:25:30,781:INFO:Copying training dataset
2024-07-08 11:25:30,787:INFO:Defining folds
2024-07-08 11:25:30,788:INFO:Declaring metric variables
2024-07-08 11:25:30,789:INFO:Importing untrained model
2024-07-08 11:25:30,791:INFO:AdaBoost Regressor Imported successfully
2024-07-08 11:25:30,794:INFO:Starting cross validation
2024-07-08 11:25:30,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:32,285:INFO:Calculating mean and std
2024-07-08 11:25:32,286:INFO:Creating metrics dataframe
2024-07-08 11:25:32,287:INFO:Uploading results into container
2024-07-08 11:25:32,287:INFO:Uploading model into container now
2024-07-08 11:25:32,287:INFO:_master_model_container: 15
2024-07-08 11:25:32,287:INFO:_display_container: 2
2024-07-08 11:25:32,287:INFO:AdaBoostRegressor(random_state=9)
2024-07-08 11:25:32,288:INFO:create_model() successfully completed......................................
2024-07-08 11:25:32,336:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:32,336:INFO:Creating metrics dataframe
2024-07-08 11:25:32,340:INFO:Initializing Gradient Boosting Regressor
2024-07-08 11:25:32,340:INFO:Total runtime is 0.39110021193822225 minutes
2024-07-08 11:25:32,342:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:32,342:INFO:Initializing create_model()
2024-07-08 11:25:32,342:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:32,342:INFO:Checking exceptions
2024-07-08 11:25:32,342:INFO:Importing libraries
2024-07-08 11:25:32,342:INFO:Copying training dataset
2024-07-08 11:25:32,349:INFO:Defining folds
2024-07-08 11:25:32,349:INFO:Declaring metric variables
2024-07-08 11:25:32,350:INFO:Importing untrained model
2024-07-08 11:25:32,351:INFO:Gradient Boosting Regressor Imported successfully
2024-07-08 11:25:32,354:INFO:Starting cross validation
2024-07-08 11:25:32,355:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:34,198:INFO:Calculating mean and std
2024-07-08 11:25:34,199:INFO:Creating metrics dataframe
2024-07-08 11:25:34,200:INFO:Uploading results into container
2024-07-08 11:25:34,200:INFO:Uploading model into container now
2024-07-08 11:25:34,200:INFO:_master_model_container: 16
2024-07-08 11:25:34,200:INFO:_display_container: 2
2024-07-08 11:25:34,200:INFO:GradientBoostingRegressor(random_state=9)
2024-07-08 11:25:34,200:INFO:create_model() successfully completed......................................
2024-07-08 11:25:34,246:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:34,246:INFO:Creating metrics dataframe
2024-07-08 11:25:34,250:INFO:Initializing Light Gradient Boosting Machine
2024-07-08 11:25:34,250:INFO:Total runtime is 0.4229323784510295 minutes
2024-07-08 11:25:34,251:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:34,252:INFO:Initializing create_model()
2024-07-08 11:25:34,252:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:34,252:INFO:Checking exceptions
2024-07-08 11:25:34,252:INFO:Importing libraries
2024-07-08 11:25:34,252:INFO:Copying training dataset
2024-07-08 11:25:34,257:INFO:Defining folds
2024-07-08 11:25:34,257:INFO:Declaring metric variables
2024-07-08 11:25:34,258:INFO:Importing untrained model
2024-07-08 11:25:34,260:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-08 11:25:34,262:INFO:Starting cross validation
2024-07-08 11:25:34,263:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:40,005:INFO:Calculating mean and std
2024-07-08 11:25:40,006:INFO:Creating metrics dataframe
2024-07-08 11:25:40,007:INFO:Uploading results into container
2024-07-08 11:25:40,007:INFO:Uploading model into container now
2024-07-08 11:25:40,007:INFO:_master_model_container: 17
2024-07-08 11:25:40,007:INFO:_display_container: 2
2024-07-08 11:25:40,008:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-08 11:25:40,008:INFO:create_model() successfully completed......................................
2024-07-08 11:25:40,058:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:40,058:INFO:Creating metrics dataframe
2024-07-08 11:25:40,063:INFO:Initializing Dummy Regressor
2024-07-08 11:25:40,063:INFO:Total runtime is 0.5198044975598654 minutes
2024-07-08 11:25:40,064:INFO:SubProcess create_model() called ==================================
2024-07-08 11:25:40,064:INFO:Initializing create_model()
2024-07-08 11:25:40,064:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17adbc850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:40,064:INFO:Checking exceptions
2024-07-08 11:25:40,064:INFO:Importing libraries
2024-07-08 11:25:40,064:INFO:Copying training dataset
2024-07-08 11:25:40,070:INFO:Defining folds
2024-07-08 11:25:40,070:INFO:Declaring metric variables
2024-07-08 11:25:40,071:INFO:Importing untrained model
2024-07-08 11:25:40,073:INFO:Dummy Regressor Imported successfully
2024-07-08 11:25:40,075:INFO:Starting cross validation
2024-07-08 11:25:40,076:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-08 11:25:40,343:INFO:Calculating mean and std
2024-07-08 11:25:40,344:INFO:Creating metrics dataframe
2024-07-08 11:25:40,345:INFO:Uploading results into container
2024-07-08 11:25:40,345:INFO:Uploading model into container now
2024-07-08 11:25:40,345:INFO:_master_model_container: 18
2024-07-08 11:25:40,345:INFO:_display_container: 2
2024-07-08 11:25:40,345:INFO:DummyRegressor()
2024-07-08 11:25:40,345:INFO:create_model() successfully completed......................................
2024-07-08 11:25:40,393:INFO:SubProcess create_model() end ==================================
2024-07-08 11:25:40,393:INFO:Creating metrics dataframe
2024-07-08 11:25:40,401:INFO:Initializing create_model()
2024-07-08 11:25:40,401:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-08 11:25:40,401:INFO:Checking exceptions
2024-07-08 11:25:40,402:INFO:Importing libraries
2024-07-08 11:25:40,402:INFO:Copying training dataset
2024-07-08 11:25:40,407:INFO:Defining folds
2024-07-08 11:25:40,407:INFO:Declaring metric variables
2024-07-08 11:25:40,407:INFO:Importing untrained model
2024-07-08 11:25:40,407:INFO:Declaring custom model
2024-07-08 11:25:40,407:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-08 11:25:40,408:INFO:Cross validation set to False
2024-07-08 11:25:40,408:INFO:Fitting Model
2024-07-08 11:25:40,532:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:25:40,535:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001173 seconds.
2024-07-08 11:25:40,535:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:25:40,535:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:25:40,535:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:25:40,535:INFO:[LightGBM] [Info] Number of data points in the train set: 14619, number of used features: 55
2024-07-08 11:25:40,535:INFO:[LightGBM] [Info] Start training from score 4001.811410
2024-07-08 11:25:40,961:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-08 11:25:40,961:INFO:create_model() successfully completed......................................
2024-07-08 11:25:41,023:INFO:_master_model_container: 18
2024-07-08 11:25:41,023:INFO:_display_container: 2
2024-07-08 11:25:41,023:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-08 11:25:41,023:INFO:compare_models() successfully completed......................................
2024-07-08 11:26:32,545:INFO:Initializing plot_model()
2024-07-08 11:26:32,546:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=residuals, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-08 11:26:32,546:INFO:Checking exceptions
2024-07-08 11:26:32,550:INFO:Preloading libraries
2024-07-08 11:26:32,555:INFO:Copying training dataset
2024-07-08 11:26:32,555:INFO:Plot type: residuals
2024-07-08 11:26:32,858:INFO:Fitting Model
2024-07-08 11:26:32,917:INFO:Scoring test/hold-out set
2024-07-08 11:26:33,284:INFO:Visual Rendered Successfully
2024-07-08 11:26:33,335:INFO:plot_model() successfully completed......................................
2024-07-08 11:26:48,224:INFO:Initializing plot_model()
2024-07-08 11:26:48,224:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-08 11:26:48,224:INFO:Checking exceptions
2024-07-08 11:26:48,230:INFO:Preloading libraries
2024-07-08 11:26:48,233:INFO:Copying training dataset
2024-07-08 11:26:48,233:INFO:Plot type: error
2024-07-08 11:26:48,543:INFO:Fitting Model
2024-07-08 11:26:48,543:INFO:Scoring test/hold-out set
2024-07-08 11:26:48,658:INFO:Visual Rendered Successfully
2024-07-08 11:26:48,716:INFO:plot_model() successfully completed......................................
2024-07-08 11:26:59,044:INFO:Initializing plot_model()
2024-07-08 11:26:59,044:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-08 11:26:59,044:INFO:Checking exceptions
2024-07-08 11:26:59,050:INFO:Preloading libraries
2024-07-08 11:26:59,054:INFO:Copying training dataset
2024-07-08 11:26:59,054:INFO:Plot type: feature
2024-07-08 11:26:59,055:WARNING:No coef_ found. Trying feature_importances_
2024-07-08 11:26:59,221:INFO:Visual Rendered Successfully
2024-07-08 11:26:59,276:INFO:plot_model() successfully completed......................................
2024-07-08 11:27:18,733:INFO:Initializing evaluate_model()
2024-07-08 11:27:18,733:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-07-08 11:27:18,744:INFO:Initializing plot_model()
2024-07-08 11:27:18,744:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=pipeline, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-08 11:27:18,744:INFO:Checking exceptions
2024-07-08 11:27:18,747:INFO:Preloading libraries
2024-07-08 11:27:18,750:INFO:Copying training dataset
2024-07-08 11:27:18,750:INFO:Plot type: pipeline
2024-07-08 11:27:18,818:INFO:Visual Rendered Successfully
2024-07-08 11:27:18,872:INFO:plot_model() successfully completed......................................
2024-07-08 11:38:02,855:INFO:Initializing plot_model()
2024-07-08 11:38:02,855:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=feature_all, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-08 11:38:02,855:INFO:Checking exceptions
2024-07-08 11:38:02,859:INFO:Preloading libraries
2024-07-08 11:38:02,865:INFO:Copying training dataset
2024-07-08 11:38:02,865:INFO:Plot type: feature_all
2024-07-08 11:38:02,960:WARNING:No coef_ found. Trying feature_importances_
2024-07-08 11:38:03,224:INFO:Visual Rendered Successfully
2024-07-08 11:38:03,285:INFO:plot_model() successfully completed......................................
2024-07-08 11:38:08,501:INFO:Initializing plot_model()
2024-07-08 11:38:08,501:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-08 11:38:08,501:INFO:Checking exceptions
2024-07-08 11:38:08,507:INFO:Preloading libraries
2024-07-08 11:38:08,511:INFO:Copying training dataset
2024-07-08 11:38:08,511:INFO:Plot type: error
2024-07-08 11:38:08,790:INFO:Fitting Model
2024-07-08 11:38:08,790:INFO:Scoring test/hold-out set
2024-07-08 11:38:08,907:INFO:Visual Rendered Successfully
2024-07-08 11:38:08,993:INFO:plot_model() successfully completed......................................
2024-07-08 11:38:10,033:INFO:Initializing plot_model()
2024-07-08 11:38:10,033:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=vc, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-08 11:38:10,033:INFO:Checking exceptions
2024-07-08 11:38:10,041:INFO:Preloading libraries
2024-07-08 11:38:10,047:INFO:Copying training dataset
2024-07-08 11:38:10,047:INFO:Plot type: vc
2024-07-08 11:38:10,047:INFO:Determining param_name
2024-07-08 11:38:10,047:INFO:param_name: max_depth
2024-07-08 11:38:10,331:INFO:Fitting Model
2024-07-08 11:38:11,998:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:11,998:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:11,998:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,001:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,001:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,002:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,018:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,018:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,018:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,018:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,018:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,019:INFO:
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,019:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,020:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006465 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006370 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006637 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006803 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006691 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006453 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006782 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006848 seconds.
2024-07-08 11:38:12,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005843 seconds.
2024-07-08 11:38:12,032:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,032:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,032:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006386 seconds.
2024-07-08 11:38:12,033:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,033:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,033:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:12,034:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,035:INFO:[LightGBM] [Info] Start training from score 3999.389070
2024-07-08 11:38:12,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,564:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,646:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077494 seconds.
2024-07-08 11:38:12,646:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,646:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,646:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:12,646:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:12,647:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:12,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,675:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,700:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016851 seconds.
2024-07-08 11:38:12,700:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,700:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,700:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:12,700:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:12,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,704:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:12,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,974:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:12,979:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002555 seconds.
2024-07-08 11:38:12,980:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:12,980:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:12,980:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:12,980:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:12,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:12,980:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:12,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,090:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:13,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,146:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032243 seconds.
2024-07-08 11:38:13,146:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:13,146:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:13,146:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:13,146:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:13,148:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:13,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,328:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:13,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,342:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:13,342:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:13,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,461:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054426 seconds.
2024-07-08 11:38:13,461:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:13,465:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:13,465:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:13,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,474:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:13,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,510:INFO:[LightGBM] [Info] Start training from score 3999.464924[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,517:INFO:
2024-07-08 11:38:13,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,911:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:13,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,922:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:13,922:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:13,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,947:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012820 seconds.
2024-07-08 11:38:13,947:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:13,947:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:13,947:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:13,949:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:13,953:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:13,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:13,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,017:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:14,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,039:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:14,040:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:14,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,146:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103781 seconds.
2024-07-08 11:38:14,146:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:14,146:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:14,146:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:14,146:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:14,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,147:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:14,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:14,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,003:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:15,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,010:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:15,010:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:15,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,016:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002867 seconds.
2024-07-08 11:38:15,016:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:15,016:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:15,016:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:15,016:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:15,017:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:15,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,117:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:15,122:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:15,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,212:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:15,269:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:15,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,270:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:15,279:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005472 seconds.
2024-07-08 11:38:15,279:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:15,279:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:15,279:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:15,280:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:15,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:15,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,075:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,079:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,155:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,164:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:16,164:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,176:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006481 seconds.
2024-07-08 11:38:16,176:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:16,176:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:16,176:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:16,177:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:16,178:INFO:[LightGBM] [Info] Start training from score 3999.464924
2024-07-08 11:38:16,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,512:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,516:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,616:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,620:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,649:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,654:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,663:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:16,692:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:16,706:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021828 seconds.
2024-07-08 11:38:16,706:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:16,706:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:16,706:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:16,711:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,712:INFO:
2024-07-08 11:38:16,724:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:16,764:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,768:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:16,771:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055414 seconds.
2024-07-08 11:38:16,771:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:16,771:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:16,771:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:16,771:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:16,772:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:16,828:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:16,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,893:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026343 seconds.
2024-07-08 11:38:16,893:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:16,893:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:16,893:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:16,893:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:16,894:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:16,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:16,908:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:16,916:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003694 seconds.
2024-07-08 11:38:16,916:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:16,916:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:16,916:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:16,916:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:16,917:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:17,004:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,013:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:17,013:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,058:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,064:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,144:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004443 seconds.
2024-07-08 11:38:17,144:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:17,144:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:17,144:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:17,144:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:17,145:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:17,156:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,167:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:17,167:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,188:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011216 seconds.
2024-07-08 11:38:17,188:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:17,188:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:17,188:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:17,189:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:17,192:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:17,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,356:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,364:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:17,365:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,370:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003269 seconds.
2024-07-08 11:38:17,370:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:17,370:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:17,370:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:17,371:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:17,371:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:17,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,867:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,874:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,905:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,921:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:17,921:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,933:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,941:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:17,941:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:17,947:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003206 seconds.
2024-07-08 11:38:17,947:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:17,947:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:17,947:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:17,948:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:17,948:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:17,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:17,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,023:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098495 seconds.
2024-07-08 11:38:18,023:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:18,023:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:18,023:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:18,024:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:18,025:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:18,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,493:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:18,498:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:18,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,557:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:18,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,643:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:18,644:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:18,652:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004732 seconds.
2024-07-08 11:38:18,652:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:18,652:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:18,652:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:18,653:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:18,654:INFO:[LightGBM] [Info] Start training from score 4000.459223
2024-07-08 11:38:18,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,842:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:18,847:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002467 seconds.
2024-07-08 11:38:18,847:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:18,847:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:18,847:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:18,847:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:18,847:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:18,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:18,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,247:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:19,260:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006709 seconds.
2024-07-08 11:38:19,260:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:19,260:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:19,260:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:19,260:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:19,261:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:19,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,816:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:19,822:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:19,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,856:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:19,863:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003397 seconds.
2024-07-08 11:38:19,863:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:19,863:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:19,863:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:19,863:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:19,863:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:19,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:19,951:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:19,960:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005086 seconds.
2024-07-08 11:38:19,961:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:19,961:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:19,961:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:19,962:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:20,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,330:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:20,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,334:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:20,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,405:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:20,413:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:20,414:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:20,423:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004840 seconds.
2024-07-08 11:38:20,423:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:20,423:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:20,423:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:20,423:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:20,424:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:20,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,924:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:20,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,933:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:20,933:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:20,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,952:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010976 seconds.
2024-07-08 11:38:20,952:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:20,952:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:20,952:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:20,955:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:20,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:20,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,078:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,082:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,104:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,104:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,108:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,114:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,134:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,141:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,142:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:21,142:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,161:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:21,161:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,161:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,170:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:21,171:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,196:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018504 seconds.
2024-07-08 11:38:21,196:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:21,196:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:21,196:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:21,197:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:21,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,202:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:21,203:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018921 seconds.
2024-07-08 11:38:21,203:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:21,203:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:21,203:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:21,205:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:21,212:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:21,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,269:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040515 seconds.
2024-07-08 11:38:21,269:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:21,270:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:21,270:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:21,270:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:21,270:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:21,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,790:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,795:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,820:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,831:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:21,831:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:21,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,857:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013592 seconds.
2024-07-08 11:38:21,857:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:21,857:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:21,857:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:21,858:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:21,861:INFO:[LightGBM] [Info] Start training from score 4004.705860
2024-07-08 11:38:21,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:21,988:INFO:
2024-07-08 11:38:22,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,191:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:22,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,196:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002620 seconds.
2024-07-08 11:38:22,196:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:22,196:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:22,196:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:22,196:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:22,197:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:22,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,529:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:22,545:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009805 seconds.
2024-07-08 11:38:22,545:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:22,545:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:22,545:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:22,546:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:22,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,736:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:22,740:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:22,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,777:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:22,781:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:22,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,822:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:22,826:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:22,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,849:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012324 seconds.
2024-07-08 11:38:22,849:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:22,849:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:22,852:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:22,854:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:22,859:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:22,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,896:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047052 seconds.
2024-07-08 11:38:22,896:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:22,896:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:22,896:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:22,896:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:22,897:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:22,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:22,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,308:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,316:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:23,316:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,322:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003365 seconds.
2024-07-08 11:38:23,322:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:23,322:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:23,322:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:23,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,322:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:23,323:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:23,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,412:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,418:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,437:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,440:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,461:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,465:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,470:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:23,470:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,473:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:23,473:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,476:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003378 seconds.
2024-07-08 11:38:23,476:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:23,476:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:23,476:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:23,476:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:23,476:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:23,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,524:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028132 seconds.
2024-07-08 11:38:23,524:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:23,524:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:23,524:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:23,524:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:23,525:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:23,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,919:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,928:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:23,928:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:23,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:23,958:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017672 seconds.
2024-07-08 11:38:23,958:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:23,958:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:23,960:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:23,962:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:24,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,639:INFO:
2024-07-08 11:38:24,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,866:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:24,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,920:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:24,921:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:24,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:24,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,022:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098209 seconds.
2024-07-08 11:38:25,022:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:25,022:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:25,022:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:25,022:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:25,023:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:25,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,581:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,585:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,611:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,620:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:25,620:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,626:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002812 seconds.
2024-07-08 11:38:25,626:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:25,626:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:25,626:INFO:[LightGBM] [Info] Total Bins 762
2024-07-08 11:38:25,626:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:25,627:INFO:[LightGBM] [Info] Start training from score 4006.244357
2024-07-08 11:38:25,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,786:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,790:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,821:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:25,827:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002918 seconds.
2024-07-08 11:38:25,827:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:25,827:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:25,827:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:25,827:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:25,827:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:25,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,948:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,953:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:25,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:25,997:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:26,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,074:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066505 seconds.
2024-07-08 11:38:26,074:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:26,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,086:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:26,089:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:26,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,143:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:26,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,244:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,248:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,254:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:26,260:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003404 seconds.
2024-07-08 11:38:26,260:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:26,260:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:26,260:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:26,260:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:26,261:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:26,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,287:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:26,297:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005052 seconds.
2024-07-08 11:38:26,297:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:26,297:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:26,297:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:26,298:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:26,299:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:26,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,449:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,453:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,475:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,483:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:26,484:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,490:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003700 seconds.
2024-07-08 11:38:26,491:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:26,491:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:26,491:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:26,491:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:26,491:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:26,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,758:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,761:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,762:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,791:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:26,791:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,796:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,811:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:26,811:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,836:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012764 seconds.
2024-07-08 11:38:26,836:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:26,837:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:26,838:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:26,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,840:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:26,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,898:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102746 seconds.
2024-07-08 11:38:26,898:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:26,898:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:26,898:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:26,898:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:26,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,899:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:26,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:26,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,291:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:27,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,394:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:27,395:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:27,403:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004331 seconds.
2024-07-08 11:38:27,403:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:27,403:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:27,403:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:27,403:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:27,404:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:27,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,527:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:27,532:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:27,555:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:27,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,562:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:27,563:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:27,576:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008164 seconds.
2024-07-08 11:38:27,576:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:27,576:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:27,576:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:27,577:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:27,578:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:27,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:27,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,072:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:28,079:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:28,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,164:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:28,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,217:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:28,217:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:28,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,238:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:28,248:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005148 seconds.
2024-07-08 11:38:28,248:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:28,248:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:28,248:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:28,248:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:28,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,249:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:28,274:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052480 seconds.
2024-07-08 11:38:28,274:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:28,274:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:28,274:INFO:[LightGBM] [Info] Total Bins 760
2024-07-08 11:38:28,274:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 54
2024-07-08 11:38:28,275:INFO:[LightGBM] [Info] Start training from score 4001.726913
2024-07-08 11:38:28,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,718:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:28,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,725:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003594 seconds.
2024-07-08 11:38:28,725:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:28,725:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:28,726:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:28,726:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:28,727:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:28,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:28,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,393:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:29,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,402:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004810 seconds.
2024-07-08 11:38:29,402:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:29,402:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:29,402:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:29,402:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:29,404:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:29,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,590:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,595:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,630:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:29,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,682:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046687 seconds.
2024-07-08 11:38:29,682:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:29,682:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:29,684:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:29,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,685:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,690:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,699:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:29,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,711:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,719:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:29,719:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,724:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002640 seconds.
2024-07-08 11:38:29,725:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:29,725:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:29,725:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:29,725:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:29,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,726:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:29,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,823:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,829:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,879:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,916:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:29,916:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:29,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:29,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,002:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054652 seconds.
2024-07-08 11:38:30,002:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:30,002:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:30,002:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:30,003:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:30,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,017:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:30,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,399:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,407:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:30,408:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,415:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003605 seconds.
2024-07-08 11:38:30,415:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:30,415:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:30,415:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:30,415:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:30,416:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:30,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,590:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,595:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,661:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,673:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:30,673:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,694:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012060 seconds.
2024-07-08 11:38:30,694:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-07-08 11:38:30,694:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:30,695:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:30,697:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:30,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,900:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,904:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,927:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,936:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:30,937:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:30,944:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003722 seconds.
2024-07-08 11:38:30,944:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:30,944:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:30,944:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:30,944:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:30,945:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:30,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:30,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:31,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,166:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,171:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,205:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,209:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:32,215:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002576 seconds.
2024-07-08 11:38:32,215:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:32,215:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:32,215:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:32,215:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:32,215:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:32,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,270:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:32,270:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,283:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006315 seconds.
2024-07-08 11:38:32,283:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:32,283:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:32,283:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:32,283:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:32,285:INFO:[LightGBM] [Info] Start training from score 3999.274607
2024-07-08 11:38:32,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,517:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,523:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,588:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,589:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:32,593:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:32,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,643:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:32,643:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:32,648:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040119 seconds.
2024-07-08 11:38:32,648:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:32,649:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:32,649:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:32,649:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002602 seconds.
2024-07-08 11:38:32,649:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:32,649:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:32,649:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:32,649:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:32,649:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:32,649:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:32,650:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:32,651:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005032 seconds.
2024-07-08 11:38:32,651:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:32,651:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:32,651:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:32,652:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:32,653:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:32,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:32,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,163:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,166:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,187:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,195:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:33,196:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,201:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003122 seconds.
2024-07-08 11:38:33,202:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:33,202:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:33,202:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:33,202:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:33,202:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:33,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,271:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,310:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:33,311:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,396:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057588 seconds.
2024-07-08 11:38:33,396:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:33,396:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:33,396:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:33,396:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:33,397:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:33,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,595:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,600:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,662:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,674:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:33,674:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,703:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014640 seconds.
2024-07-08 11:38:33,703:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:33,703:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:33,703:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:33,704:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:33,707:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:33,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,720:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,727:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:33,727:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:33,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,733:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002679 seconds.
2024-07-08 11:38:33,733:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:33,733:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:33,733:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:33,733:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:33,734:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:33,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:33,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,427:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:34,432:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:34,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,454:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:34,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,463:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:34,463:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:34,472:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004488 seconds.
2024-07-08 11:38:34,472:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:34,472:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:34,472:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:34,472:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:34,473:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:34,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,557:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:34,566:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:34,567:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:34,572:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002484 seconds.
2024-07-08 11:38:34,572:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:34,572:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:34,572:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:34,572:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:34,573:INFO:[LightGBM] [Info] Start training from score 3998.891009
2024-07-08 11:38:34,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:34,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,335:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:35,340:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:35,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,412:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:35,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,521:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083397 seconds.
2024-07-08 11:38:35,522:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:35,522:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:35,522:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:35,522:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:35,522:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:35,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,851:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:35,857:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002997 seconds.
2024-07-08 11:38:35,857:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:35,857:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:35,857:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:35,857:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:35,858:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:35,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:35,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,105:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,109:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,167:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:36,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,204:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020967 seconds.
2024-07-08 11:38:36,204:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:36,204:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:36,204:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:36,204:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:36,208:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:36,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,476:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,479:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,541:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:36,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,561:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,566:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014495 seconds.
2024-07-08 11:38:36,566:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:36,566:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:36,566:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:36,567:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:36,571:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:36,571:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:36,571:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,579:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003583 seconds.
2024-07-08 11:38:36,579:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:36,579:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:36,579:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:36,579:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:36,580:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:36,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,786:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,791:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,834:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,850:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:36,851:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:36,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,874:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014071 seconds.
2024-07-08 11:38:36,874:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:36,874:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:36,874:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:36,876:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:36,877:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:36,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:36,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,376:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,385:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:37,386:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,400:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009141 seconds.
2024-07-08 11:38:37,400:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:37,400:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:37,401:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:37,401:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:37,402:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:37,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,419:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,423:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,448:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,451:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,456:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,466:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:37,466:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,474:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003823 seconds.
2024-07-08 11:38:37,474:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:37,474:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:37,474:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:37,474:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:37,475:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:37,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,533:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,540:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:37,541:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:37,548:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003869 seconds.
2024-07-08 11:38:37,548:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:37,548:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:37,548:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:37,548:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:37,549:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:37,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:37,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,409:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:38,413:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:38,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,454:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:38,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,465:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:38,466:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:38,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,495:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:38,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,501:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002874 seconds.
2024-07-08 11:38:38,501:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:38,501:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:38,501:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:38,501:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:38,502:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:38,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,525:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055051 seconds.
2024-07-08 11:38:38,525:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:38,525:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:38,525:INFO:[LightGBM] [Info] Total Bins 763
2024-07-08 11:38:38,526:INFO:[LightGBM] [Info] Number of data points in the train set: 13157, number of used features: 55
2024-07-08 11:38:38,528:INFO:[LightGBM] [Info] Start training from score 4005.230068
2024-07-08 11:38:38,547:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:38,552:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:38,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,671:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:38,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,772:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098262 seconds.
2024-07-08 11:38:38,772:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:38,772:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:38,773:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:38,773:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:38,774:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:38,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,953:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:38,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,959:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002657 seconds.
2024-07-08 11:38:38,959:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:38,959:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:38,959:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:38,960:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:38,960:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:38,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:38,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,342:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:39,347:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002635 seconds.
2024-07-08 11:38:39,347:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:39,347:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:39,347:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:39,347:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:39,347:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:39,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,357:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,363:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,406:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,415:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:39,415:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,425:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005525 seconds.
2024-07-08 11:38:39,425:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:39,425:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:39,426:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:39,426:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:39,427:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:39,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,512:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,517:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,551:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,551:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,554:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,564:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:39,565:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,579:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,580:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008714 seconds.
2024-07-08 11:38:39,580:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:39,580:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:39,580:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:39,581:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:39,582:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:39,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,611:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:39,611:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:39,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,662:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039552 seconds.
2024-07-08 11:38:39,662:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:39,662:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:39,662:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:39,663:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:39,665:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:39,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:39,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,011:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:40,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,020:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:40,020:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:40,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,030:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006641 seconds.
2024-07-08 11:38:40,030:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:40,030:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:40,030:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:40,031:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:40,032:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:40,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,904:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:40,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,908:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:40,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,934:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:40,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:40,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,019:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:41,019:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:41,026:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003902 seconds.
2024-07-08 11:38:41,026:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:41,026:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:41,027:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:41,027:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:41,027:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:41,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,362:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:41,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,375:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-08 11:38:41,375:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:41,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,425:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028625 seconds.
2024-07-08 11:38:41,425:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-08 11:38:41,425:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-08 11:38:41,425:INFO:[LightGBM] [Info] Total Bins 761
2024-07-08 11:38:41,436:INFO:[LightGBM] [Info] Number of data points in the train set: 13158, number of used features: 54
2024-07-08 11:38:41,440:INFO:[LightGBM] [Info] Start training from score 4002.727998
2024-07-08 11:38:41,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:41,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,059:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,064:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,232:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,236:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,545:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,549:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,802:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,806:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:42,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:42,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,206:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,210:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,352:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,357:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,558:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,563:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-07-08 11:38:43,792:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,796:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,925:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,931:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:43,999:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:44,003:INFO:[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
2024-07-08 11:38:44,200:INFO:Visual Rendered Successfully
2024-07-08 11:38:44,259:INFO:plot_model() successfully completed......................................
2024-07-08 11:38:44,296:INFO:Initializing plot_model()
2024-07-08 11:38:44,297:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x16de10390>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=pipeline, scale=1, save=False, fold=KFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-07-08 11:38:44,297:INFO:Checking exceptions
2024-07-08 11:38:44,312:INFO:Preloading libraries
2024-07-08 11:38:44,315:INFO:Copying training dataset
2024-07-08 11:38:44,315:INFO:Plot type: pipeline
2024-07-08 11:38:44,369:INFO:Visual Rendered Successfully
2024-07-08 11:38:44,422:INFO:plot_model() successfully completed......................................
2024-07-10 13:58:20,297:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 13:58:20,297:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 13:58:20,297:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 13:58:20,297:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 13:58:22,537:INFO:PyCaret RegressionExperiment
2024-07-10 13:58:22,537:INFO:Logging name: reg-default-name
2024-07-10 13:58:22,537:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 13:58:22,537:INFO:version 3.3.1
2024-07-10 13:58:22,537:INFO:Initializing setup()
2024-07-10 13:58:22,537:INFO:self.USI: 399b
2024-07-10 13:58:22,537:INFO:self._variable_keys: {'log_plots_param', 'USI', 'memory', 'X_train', 'X_test', 'fold_generator', 'data', '_available_plots', 'idx', 'n_jobs_param', 'transform_target_param', 'logging_param', 'html_param', 'y_test', 'gpu_param', 'exp_id', 'y', 'target_param', 'fold_groups_param', 'seed', 'X', 'fold_shuffle_param', '_ml_usecase', 'exp_name_log', 'gpu_n_jobs_param', 'pipeline', 'y_train'}
2024-07-10 13:58:22,537:INFO:Checking environment
2024-07-10 13:58:22,538:INFO:python_version: 3.11.8
2024-07-10 13:58:22,538:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 13:58:22,538:INFO:machine: arm64
2024-07-10 13:58:22,538:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 13:58:22,538:INFO:Memory: svmem(total=17179869184, available=4195303424, percent=75.6, used=5709152256, free=124059648, active=4083400704, inactive=4031496192, wired=1625751552)
2024-07-10 13:58:22,538:INFO:Physical Core: 8
2024-07-10 13:58:22,538:INFO:Logical Core: 8
2024-07-10 13:58:22,538:INFO:Checking libraries
2024-07-10 13:58:22,538:INFO:System:
2024-07-10 13:58:22,538:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 13:58:22,538:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 13:58:22,538:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 13:58:22,538:INFO:PyCaret required dependencies:
2024-07-10 13:58:22,999:INFO:                 pip: 23.3.1
2024-07-10 13:58:22,999:INFO:          setuptools: 68.2.2
2024-07-10 13:58:22,999:INFO:             pycaret: 3.3.1
2024-07-10 13:58:22,999:INFO:             IPython: 8.20.0
2024-07-10 13:58:22,999:INFO:          ipywidgets: 7.6.5
2024-07-10 13:58:22,999:INFO:                tqdm: 4.65.0
2024-07-10 13:58:22,999:INFO:               numpy: 1.26.4
2024-07-10 13:58:22,999:INFO:              pandas: 2.1.4
2024-07-10 13:58:22,999:INFO:              jinja2: 3.1.3
2024-07-10 13:58:22,999:INFO:               scipy: 1.11.4
2024-07-10 13:58:22,999:INFO:              joblib: 1.2.0
2024-07-10 13:58:22,999:INFO:             sklearn: 1.4.2
2024-07-10 13:58:22,999:INFO:                pyod: 1.1.3
2024-07-10 13:58:22,999:INFO:            imblearn: 0.12.2
2024-07-10 13:58:22,999:INFO:   category_encoders: 2.6.3
2024-07-10 13:58:22,999:INFO:            lightgbm: 4.3.0
2024-07-10 13:58:22,999:INFO:               numba: 0.59.0
2024-07-10 13:58:22,999:INFO:            requests: 2.31.0
2024-07-10 13:58:22,999:INFO:          matplotlib: 3.8.0
2024-07-10 13:58:22,999:INFO:          scikitplot: 0.3.7
2024-07-10 13:58:22,999:INFO:         yellowbrick: 1.5
2024-07-10 13:58:22,999:INFO:              plotly: 5.22.0
2024-07-10 13:58:22,999:INFO:    plotly-resampler: Not installed
2024-07-10 13:58:22,999:INFO:             kaleido: 0.2.1
2024-07-10 13:58:22,999:INFO:           schemdraw: 0.15
2024-07-10 13:58:22,999:INFO:         statsmodels: 0.14.0
2024-07-10 13:58:22,999:INFO:              sktime: 0.26.0
2024-07-10 13:58:22,999:INFO:               tbats: 1.1.3
2024-07-10 13:58:22,999:INFO:            pmdarima: 2.0.4
2024-07-10 13:58:22,999:INFO:              psutil: 5.9.0
2024-07-10 13:58:22,999:INFO:          markupsafe: 2.1.3
2024-07-10 13:58:22,999:INFO:             pickle5: Not installed
2024-07-10 13:58:22,999:INFO:         cloudpickle: 2.2.1
2024-07-10 13:58:22,999:INFO:         deprecation: 2.1.0
2024-07-10 13:58:22,999:INFO:              xxhash: 3.4.1
2024-07-10 13:58:22,999:INFO:           wurlitzer: 3.0.2
2024-07-10 13:58:22,999:INFO:PyCaret optional dependencies:
2024-07-10 13:58:23,005:INFO:                shap: Not installed
2024-07-10 13:58:23,005:INFO:           interpret: Not installed
2024-07-10 13:58:23,005:INFO:                umap: 0.5.5
2024-07-10 13:58:23,005:INFO:     ydata_profiling: Not installed
2024-07-10 13:58:23,005:INFO:  explainerdashboard: Not installed
2024-07-10 13:58:23,005:INFO:             autoviz: Not installed
2024-07-10 13:58:23,005:INFO:           fairlearn: Not installed
2024-07-10 13:58:23,005:INFO:          deepchecks: Not installed
2024-07-10 13:58:23,005:INFO:             xgboost: Not installed
2024-07-10 13:58:23,005:INFO:            catboost: Not installed
2024-07-10 13:58:23,005:INFO:              kmodes: Not installed
2024-07-10 13:58:23,005:INFO:             mlxtend: Not installed
2024-07-10 13:58:23,005:INFO:       statsforecast: Not installed
2024-07-10 13:58:23,005:INFO:        tune_sklearn: Not installed
2024-07-10 13:58:23,005:INFO:                 ray: Not installed
2024-07-10 13:58:23,005:INFO:            hyperopt: Not installed
2024-07-10 13:58:23,005:INFO:              optuna: Not installed
2024-07-10 13:58:23,005:INFO:               skopt: Not installed
2024-07-10 13:58:23,005:INFO:              mlflow: 2.13.0
2024-07-10 13:58:23,005:INFO:              gradio: Not installed
2024-07-10 13:58:23,006:INFO:             fastapi: Not installed
2024-07-10 13:58:23,006:INFO:             uvicorn: Not installed
2024-07-10 13:58:23,006:INFO:              m2cgen: Not installed
2024-07-10 13:58:23,006:INFO:           evidently: Not installed
2024-07-10 13:58:23,006:INFO:               fugue: Not installed
2024-07-10 13:58:23,006:INFO:           streamlit: 1.30.0
2024-07-10 13:58:23,006:INFO:             prophet: Not installed
2024-07-10 13:58:23,006:INFO:None
2024-07-10 13:58:23,006:INFO:Set up data.
2024-07-10 13:58:23,014:INFO:Set up folding strategy.
2024-07-10 13:58:23,014:INFO:Set up train/test split.
2024-07-10 13:58:23,032:INFO:Set up index.
2024-07-10 13:58:23,032:INFO:Assigning column types.
2024-07-10 13:58:23,035:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 13:58:23,035:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,037:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,039:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,064:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,081:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,081:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,081:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,082:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,083:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,085:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,109:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,126:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,126:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,126:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,126:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 13:58:23,128:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,129:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,153:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,170:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,170:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,170:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,172:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,173:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,197:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,214:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,214:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,214:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,214:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 13:58:23,217:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,241:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,258:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,258:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,258:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,261:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,285:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,301:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,302:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,302:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,302:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 13:58:23,329:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,345:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,346:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,346:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,374:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,391:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,391:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,391:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,391:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 13:58:23,418:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,435:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,435:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,462:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 13:58:23,480:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,480:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,480:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 13:58:23,526:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,526:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,571:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,572:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,573:INFO:Preparing preprocessing pipeline...
2024-07-10 13:58:23,573:INFO:Set up simple imputation.
2024-07-10 13:58:23,576:INFO:Set up encoding of categorical features.
2024-07-10 13:58:23,642:INFO:Finished creating preprocessing pipeline.
2024-07-10 13:58:23,646:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms',
                                             'is_exact_address_hidden',
                                             'is_floor_under', 'rent_price',
                                             'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitte...
                 TransformerWrapper(include=['floor', 'energy_certificate',
                                             'district', 'house_type'],
                                    transformer=OneHotEncoder(cols=['floor',
                                                                    'energy_certificate',
                                                                    'district',
                                                                    'house_type'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan')))])
2024-07-10 13:58:23,646:INFO:Creating final display dataframe.
2024-07-10 13:58:23,799:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape         (9406, 26)
4        Transformed data shape         (9406, 70)
5   Transformed train set shape         (6584, 70)
6    Transformed test set shape         (2822, 70)
7              Numeric features                 20
8          Categorical features                  5
9      Rows with missing values              51.3%
10                   Preprocess               True
11              Imputation type             simple
12           Numeric imputation               mean
13       Categorical imputation               mode
14     Maximum one-hot encoding                 25
15              Encoding method               None
16               Fold Generator              KFold
17                  Fold Number                 10
18                     CPU Jobs                 -1
19                      Use GPU              False
20               Log Experiment              False
21              Experiment Name   reg-default-name
22                          USI               399b
2024-07-10 13:58:23,851:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,851:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,898:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,898:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 13:58:23,899:INFO:setup() successfully completed in 1.37s...............
2024-07-10 14:17:00,717:INFO:PyCaret RegressionExperiment
2024-07-10 14:17:00,717:INFO:Logging name: reg-default-name
2024-07-10 14:17:00,717:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 14:17:00,717:INFO:version 3.3.1
2024-07-10 14:17:00,717:INFO:Initializing setup()
2024-07-10 14:17:00,717:INFO:self.USI: 9db1
2024-07-10 14:17:00,717:INFO:self._variable_keys: {'log_plots_param', 'USI', 'memory', 'X_train', 'X_test', 'fold_generator', 'data', '_available_plots', 'idx', 'n_jobs_param', 'transform_target_param', 'logging_param', 'html_param', 'y_test', 'gpu_param', 'exp_id', 'y', 'target_param', 'fold_groups_param', 'seed', 'X', 'fold_shuffle_param', '_ml_usecase', 'exp_name_log', 'gpu_n_jobs_param', 'pipeline', 'y_train'}
2024-07-10 14:17:00,717:INFO:Checking environment
2024-07-10 14:17:00,717:INFO:python_version: 3.11.8
2024-07-10 14:17:00,717:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 14:17:00,718:INFO:machine: arm64
2024-07-10 14:17:00,718:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 14:17:00,718:INFO:Memory: svmem(total=17179869184, available=5214224384, percent=69.6, used=6464012288, free=358989824, active=4914987008, inactive=4790173696, wired=1549025280)
2024-07-10 14:17:00,718:INFO:Physical Core: 8
2024-07-10 14:17:00,718:INFO:Logical Core: 8
2024-07-10 14:17:00,718:INFO:Checking libraries
2024-07-10 14:17:00,718:INFO:System:
2024-07-10 14:17:00,718:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 14:17:00,718:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 14:17:00,718:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 14:17:00,718:INFO:PyCaret required dependencies:
2024-07-10 14:17:00,718:INFO:                 pip: 23.3.1
2024-07-10 14:17:00,718:INFO:          setuptools: 68.2.2
2024-07-10 14:17:00,718:INFO:             pycaret: 3.3.1
2024-07-10 14:17:00,718:INFO:             IPython: 8.20.0
2024-07-10 14:17:00,719:INFO:          ipywidgets: 7.6.5
2024-07-10 14:17:00,719:INFO:                tqdm: 4.65.0
2024-07-10 14:17:00,719:INFO:               numpy: 1.26.4
2024-07-10 14:17:00,719:INFO:              pandas: 2.1.4
2024-07-10 14:17:00,719:INFO:              jinja2: 3.1.3
2024-07-10 14:17:00,719:INFO:               scipy: 1.11.4
2024-07-10 14:17:00,719:INFO:              joblib: 1.2.0
2024-07-10 14:17:00,719:INFO:             sklearn: 1.4.2
2024-07-10 14:17:00,719:INFO:                pyod: 1.1.3
2024-07-10 14:17:00,719:INFO:            imblearn: 0.12.2
2024-07-10 14:17:00,719:INFO:   category_encoders: 2.6.3
2024-07-10 14:17:00,719:INFO:            lightgbm: 4.3.0
2024-07-10 14:17:00,719:INFO:               numba: 0.59.0
2024-07-10 14:17:00,719:INFO:            requests: 2.31.0
2024-07-10 14:17:00,719:INFO:          matplotlib: 3.8.0
2024-07-10 14:17:00,719:INFO:          scikitplot: 0.3.7
2024-07-10 14:17:00,719:INFO:         yellowbrick: 1.5
2024-07-10 14:17:00,719:INFO:              plotly: 5.22.0
2024-07-10 14:17:00,719:INFO:    plotly-resampler: Not installed
2024-07-10 14:17:00,719:INFO:             kaleido: 0.2.1
2024-07-10 14:17:00,719:INFO:           schemdraw: 0.15
2024-07-10 14:17:00,719:INFO:         statsmodels: 0.14.0
2024-07-10 14:17:00,719:INFO:              sktime: 0.26.0
2024-07-10 14:17:00,719:INFO:               tbats: 1.1.3
2024-07-10 14:17:00,719:INFO:            pmdarima: 2.0.4
2024-07-10 14:17:00,719:INFO:              psutil: 5.9.0
2024-07-10 14:17:00,719:INFO:          markupsafe: 2.1.3
2024-07-10 14:17:00,719:INFO:             pickle5: Not installed
2024-07-10 14:17:00,719:INFO:         cloudpickle: 2.2.1
2024-07-10 14:17:00,719:INFO:         deprecation: 2.1.0
2024-07-10 14:17:00,720:INFO:              xxhash: 3.4.1
2024-07-10 14:17:00,720:INFO:           wurlitzer: 3.0.2
2024-07-10 14:17:00,720:INFO:PyCaret optional dependencies:
2024-07-10 14:17:00,720:INFO:                shap: Not installed
2024-07-10 14:17:00,720:INFO:           interpret: Not installed
2024-07-10 14:17:00,720:INFO:                umap: 0.5.5
2024-07-10 14:17:00,720:INFO:     ydata_profiling: Not installed
2024-07-10 14:17:00,720:INFO:  explainerdashboard: Not installed
2024-07-10 14:17:00,720:INFO:             autoviz: Not installed
2024-07-10 14:17:00,720:INFO:           fairlearn: Not installed
2024-07-10 14:17:00,720:INFO:          deepchecks: Not installed
2024-07-10 14:17:00,720:INFO:             xgboost: Not installed
2024-07-10 14:17:00,720:INFO:            catboost: Not installed
2024-07-10 14:17:00,720:INFO:              kmodes: Not installed
2024-07-10 14:17:00,720:INFO:             mlxtend: Not installed
2024-07-10 14:17:00,720:INFO:       statsforecast: Not installed
2024-07-10 14:17:00,720:INFO:        tune_sklearn: Not installed
2024-07-10 14:17:00,720:INFO:                 ray: Not installed
2024-07-10 14:17:00,720:INFO:            hyperopt: Not installed
2024-07-10 14:17:00,720:INFO:              optuna: Not installed
2024-07-10 14:17:00,720:INFO:               skopt: Not installed
2024-07-10 14:17:00,720:INFO:              mlflow: 2.13.0
2024-07-10 14:17:00,720:INFO:              gradio: Not installed
2024-07-10 14:17:00,720:INFO:             fastapi: Not installed
2024-07-10 14:17:00,720:INFO:             uvicorn: Not installed
2024-07-10 14:17:00,720:INFO:              m2cgen: Not installed
2024-07-10 14:17:00,720:INFO:           evidently: Not installed
2024-07-10 14:17:00,720:INFO:               fugue: Not installed
2024-07-10 14:17:00,720:INFO:           streamlit: 1.30.0
2024-07-10 14:17:00,721:INFO:             prophet: Not installed
2024-07-10 14:17:00,721:INFO:None
2024-07-10 14:17:00,721:INFO:Set up data.
2024-07-10 14:17:00,733:INFO:Set up folding strategy.
2024-07-10 14:17:00,733:INFO:Set up train/test split.
2024-07-10 14:17:00,740:INFO:Set up index.
2024-07-10 14:17:00,740:INFO:Assigning column types.
2024-07-10 14:17:00,745:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 14:17:00,745:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,748:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,751:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,785:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,804:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,804:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,804:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,806:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,808:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,831:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,848:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,849:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,849:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,849:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 14:17:00,850:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,852:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,876:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,893:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,893:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,893:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,895:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,896:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,920:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,937:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,937:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,937:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,937:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 14:17:00,941:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,964:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,981:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:00,982:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,982:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:00,985:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,010:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,028:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,028:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,028:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,028:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 14:17:01,055:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,073:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,073:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,073:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,101:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,118:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,118:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,118:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,118:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 14:17:01,147:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,165:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,165:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,194:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:17:01,212:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,212:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,212:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 14:17:01,259:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,306:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:01,307:INFO:Preparing preprocessing pipeline...
2024-07-10 14:17:01,308:INFO:Set up simple imputation.
2024-07-10 14:17:01,310:INFO:Set up encoding of categorical features.
2024-07-10 14:17:01,310:INFO:Set up removing multicollinearity.
2024-07-10 14:17:01,310:INFO:Set up removing outliers.
2024-07-10 14:17:01,310:INFO:Set up column transformation.
2024-07-10 14:17:01,310:INFO:Set up feature normalization.
2024-07-10 14:17:01,670:INFO:Finished creating preprocessing pipeline.
2024-07-10 14:17:01,675:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms',
                                             'is_exact_address_hidden',
                                             'is_floor_under', 'rent_price',
                                             'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitte...
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('remove_outliers',
                 TransformerWrapper(transformer=RemoveOutliers(random_state=9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 14:17:01,675:INFO:Creating final display dataframe.
2024-07-10 14:17:02,045:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape         (9406, 26)
4        Transformed data shape         (9076, 69)
5   Transformed train set shape         (6254, 69)
6    Transformed test set shape         (2822, 69)
7              Numeric features                 20
8          Categorical features                  5
9      Rows with missing values              51.3%
10                   Preprocess               True
11              Imputation type             simple
12           Numeric imputation               mean
13       Categorical imputation               mode
14     Maximum one-hot encoding                 25
15              Encoding method               None
16     Remove multicollinearity               True
17  Multicollinearity threshold                0.9
18              Remove outliers               True
19           Outliers threshold               0.05
20               Transformation               True
21        Transformation method        yeo-johnson
22                    Normalize               True
23             Normalize method             zscore
24               Fold Generator              KFold
25                  Fold Number                 10
26                     CPU Jobs                 -1
27                      Use GPU              False
28               Log Experiment              False
29              Experiment Name   reg-default-name
30                          USI               9db1
2024-07-10 14:17:02,097:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:02,098:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:02,144:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:02,144:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:17:02,144:INFO:setup() successfully completed in 1.44s...............
2024-07-10 14:17:06,421:INFO:Initializing compare_models()
2024-07-10 14:17:06,421:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 14:17:06,422:INFO:Checking exceptions
2024-07-10 14:17:06,434:INFO:Preparing display monitor
2024-07-10 14:17:06,480:INFO:Initializing Linear Regression
2024-07-10 14:17:06,481:INFO:Total runtime is 4.482269287109375e-06 minutes
2024-07-10 14:17:06,482:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:06,483:INFO:Initializing create_model()
2024-07-10 14:17:06,483:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:06,483:INFO:Checking exceptions
2024-07-10 14:17:06,483:INFO:Importing libraries
2024-07-10 14:17:06,483:INFO:Copying training dataset
2024-07-10 14:17:06,492:INFO:Defining folds
2024-07-10 14:17:06,492:INFO:Declaring metric variables
2024-07-10 14:17:06,494:INFO:Importing untrained model
2024-07-10 14:17:06,495:INFO:Linear Regression Imported successfully
2024-07-10 14:17:06,498:INFO:Starting cross validation
2024-07-10 14:17:06,507:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:09,885:INFO:Calculating mean and std
2024-07-10 14:17:09,886:INFO:Creating metrics dataframe
2024-07-10 14:17:09,887:INFO:Uploading results into container
2024-07-10 14:17:09,888:INFO:Uploading model into container now
2024-07-10 14:17:09,888:INFO:_master_model_container: 1
2024-07-10 14:17:09,888:INFO:_display_container: 2
2024-07-10 14:17:09,888:INFO:LinearRegression(n_jobs=-1)
2024-07-10 14:17:09,888:INFO:create_model() successfully completed......................................
2024-07-10 14:17:13,207:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:13,207:INFO:Creating metrics dataframe
2024-07-10 14:17:13,210:INFO:Initializing Lasso Regression
2024-07-10 14:17:13,210:INFO:Total runtime is 0.1121641993522644 minutes
2024-07-10 14:17:13,211:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:13,212:INFO:Initializing create_model()
2024-07-10 14:17:13,212:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:13,212:INFO:Checking exceptions
2024-07-10 14:17:13,212:INFO:Importing libraries
2024-07-10 14:17:13,212:INFO:Copying training dataset
2024-07-10 14:17:13,217:INFO:Defining folds
2024-07-10 14:17:13,217:INFO:Declaring metric variables
2024-07-10 14:17:13,218:INFO:Importing untrained model
2024-07-10 14:17:13,219:INFO:Lasso Regression Imported successfully
2024-07-10 14:17:13,221:INFO:Starting cross validation
2024-07-10 14:17:13,226:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:14,117:INFO:Calculating mean and std
2024-07-10 14:17:14,118:INFO:Creating metrics dataframe
2024-07-10 14:17:14,119:INFO:Uploading results into container
2024-07-10 14:17:14,119:INFO:Uploading model into container now
2024-07-10 14:17:14,119:INFO:_master_model_container: 2
2024-07-10 14:17:14,119:INFO:_display_container: 2
2024-07-10 14:17:14,119:INFO:Lasso(random_state=9)
2024-07-10 14:17:14,120:INFO:create_model() successfully completed......................................
2024-07-10 14:17:14,256:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:14,256:INFO:Creating metrics dataframe
2024-07-10 14:17:14,258:INFO:Initializing Ridge Regression
2024-07-10 14:17:14,258:INFO:Total runtime is 0.12963306506474814 minutes
2024-07-10 14:17:14,259:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:14,260:INFO:Initializing create_model()
2024-07-10 14:17:14,260:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:14,260:INFO:Checking exceptions
2024-07-10 14:17:14,260:INFO:Importing libraries
2024-07-10 14:17:14,260:INFO:Copying training dataset
2024-07-10 14:17:14,264:INFO:Defining folds
2024-07-10 14:17:14,264:INFO:Declaring metric variables
2024-07-10 14:17:14,266:INFO:Importing untrained model
2024-07-10 14:17:14,267:INFO:Ridge Regression Imported successfully
2024-07-10 14:17:14,269:INFO:Starting cross validation
2024-07-10 14:17:14,274:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:15,143:INFO:Calculating mean and std
2024-07-10 14:17:15,144:INFO:Creating metrics dataframe
2024-07-10 14:17:15,145:INFO:Uploading results into container
2024-07-10 14:17:15,145:INFO:Uploading model into container now
2024-07-10 14:17:15,145:INFO:_master_model_container: 3
2024-07-10 14:17:15,145:INFO:_display_container: 2
2024-07-10 14:17:15,145:INFO:Ridge(random_state=9)
2024-07-10 14:17:15,145:INFO:create_model() successfully completed......................................
2024-07-10 14:17:15,259:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:15,259:INFO:Creating metrics dataframe
2024-07-10 14:17:15,262:INFO:Initializing Elastic Net
2024-07-10 14:17:15,262:INFO:Total runtime is 0.14636123180389404 minutes
2024-07-10 14:17:15,263:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:15,263:INFO:Initializing create_model()
2024-07-10 14:17:15,263:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:15,263:INFO:Checking exceptions
2024-07-10 14:17:15,264:INFO:Importing libraries
2024-07-10 14:17:15,264:INFO:Copying training dataset
2024-07-10 14:17:15,268:INFO:Defining folds
2024-07-10 14:17:15,268:INFO:Declaring metric variables
2024-07-10 14:17:15,269:INFO:Importing untrained model
2024-07-10 14:17:15,270:INFO:Elastic Net Imported successfully
2024-07-10 14:17:15,272:INFO:Starting cross validation
2024-07-10 14:17:15,277:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:16,159:INFO:Calculating mean and std
2024-07-10 14:17:16,159:INFO:Creating metrics dataframe
2024-07-10 14:17:16,160:INFO:Uploading results into container
2024-07-10 14:17:16,160:INFO:Uploading model into container now
2024-07-10 14:17:16,161:INFO:_master_model_container: 4
2024-07-10 14:17:16,161:INFO:_display_container: 2
2024-07-10 14:17:16,161:INFO:ElasticNet(random_state=9)
2024-07-10 14:17:16,161:INFO:create_model() successfully completed......................................
2024-07-10 14:17:16,275:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:16,275:INFO:Creating metrics dataframe
2024-07-10 14:17:16,278:INFO:Initializing Least Angle Regression
2024-07-10 14:17:16,278:INFO:Total runtime is 0.16328974962234497 minutes
2024-07-10 14:17:16,279:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:16,279:INFO:Initializing create_model()
2024-07-10 14:17:16,279:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:16,279:INFO:Checking exceptions
2024-07-10 14:17:16,279:INFO:Importing libraries
2024-07-10 14:17:16,279:INFO:Copying training dataset
2024-07-10 14:17:16,284:INFO:Defining folds
2024-07-10 14:17:16,284:INFO:Declaring metric variables
2024-07-10 14:17:16,285:INFO:Importing untrained model
2024-07-10 14:17:16,286:INFO:Least Angle Regression Imported successfully
2024-07-10 14:17:16,288:INFO:Starting cross validation
2024-07-10 14:17:16,294:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:17,192:INFO:Calculating mean and std
2024-07-10 14:17:17,193:INFO:Creating metrics dataframe
2024-07-10 14:17:17,194:INFO:Uploading results into container
2024-07-10 14:17:17,194:INFO:Uploading model into container now
2024-07-10 14:17:17,194:INFO:_master_model_container: 5
2024-07-10 14:17:17,194:INFO:_display_container: 2
2024-07-10 14:17:17,194:INFO:Lars(random_state=9)
2024-07-10 14:17:17,194:INFO:create_model() successfully completed......................................
2024-07-10 14:17:17,309:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:17,309:INFO:Creating metrics dataframe
2024-07-10 14:17:17,311:INFO:Initializing Lasso Least Angle Regression
2024-07-10 14:17:17,311:INFO:Total runtime is 0.18052071730295816 minutes
2024-07-10 14:17:17,313:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:17,313:INFO:Initializing create_model()
2024-07-10 14:17:17,313:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:17,313:INFO:Checking exceptions
2024-07-10 14:17:17,313:INFO:Importing libraries
2024-07-10 14:17:17,313:INFO:Copying training dataset
2024-07-10 14:17:17,318:INFO:Defining folds
2024-07-10 14:17:17,318:INFO:Declaring metric variables
2024-07-10 14:17:17,319:INFO:Importing untrained model
2024-07-10 14:17:17,320:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 14:17:17,322:INFO:Starting cross validation
2024-07-10 14:17:17,327:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:18,211:INFO:Calculating mean and std
2024-07-10 14:17:18,211:INFO:Creating metrics dataframe
2024-07-10 14:17:18,212:INFO:Uploading results into container
2024-07-10 14:17:18,212:INFO:Uploading model into container now
2024-07-10 14:17:18,213:INFO:_master_model_container: 6
2024-07-10 14:17:18,213:INFO:_display_container: 2
2024-07-10 14:17:18,213:INFO:LassoLars(random_state=9)
2024-07-10 14:17:18,213:INFO:create_model() successfully completed......................................
2024-07-10 14:17:18,327:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:18,327:INFO:Creating metrics dataframe
2024-07-10 14:17:18,330:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 14:17:18,330:INFO:Total runtime is 0.19748981793721515 minutes
2024-07-10 14:17:18,331:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:18,331:INFO:Initializing create_model()
2024-07-10 14:17:18,331:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:18,331:INFO:Checking exceptions
2024-07-10 14:17:18,331:INFO:Importing libraries
2024-07-10 14:17:18,331:INFO:Copying training dataset
2024-07-10 14:17:18,336:INFO:Defining folds
2024-07-10 14:17:18,336:INFO:Declaring metric variables
2024-07-10 14:17:18,337:INFO:Importing untrained model
2024-07-10 14:17:18,338:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 14:17:18,340:INFO:Starting cross validation
2024-07-10 14:17:18,346:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:19,277:INFO:Calculating mean and std
2024-07-10 14:17:19,278:INFO:Creating metrics dataframe
2024-07-10 14:17:19,279:INFO:Uploading results into container
2024-07-10 14:17:19,279:INFO:Uploading model into container now
2024-07-10 14:17:19,279:INFO:_master_model_container: 7
2024-07-10 14:17:19,279:INFO:_display_container: 2
2024-07-10 14:17:19,279:INFO:OrthogonalMatchingPursuit()
2024-07-10 14:17:19,279:INFO:create_model() successfully completed......................................
2024-07-10 14:17:19,393:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:19,394:INFO:Creating metrics dataframe
2024-07-10 14:17:19,396:INFO:Initializing Bayesian Ridge
2024-07-10 14:17:19,397:INFO:Total runtime is 0.21527144908905027 minutes
2024-07-10 14:17:19,398:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:19,398:INFO:Initializing create_model()
2024-07-10 14:17:19,398:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:19,398:INFO:Checking exceptions
2024-07-10 14:17:19,398:INFO:Importing libraries
2024-07-10 14:17:19,398:INFO:Copying training dataset
2024-07-10 14:17:19,403:INFO:Defining folds
2024-07-10 14:17:19,403:INFO:Declaring metric variables
2024-07-10 14:17:19,404:INFO:Importing untrained model
2024-07-10 14:17:19,405:INFO:Bayesian Ridge Imported successfully
2024-07-10 14:17:19,407:INFO:Starting cross validation
2024-07-10 14:17:19,413:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:20,338:INFO:Calculating mean and std
2024-07-10 14:17:20,339:INFO:Creating metrics dataframe
2024-07-10 14:17:20,340:INFO:Uploading results into container
2024-07-10 14:17:20,340:INFO:Uploading model into container now
2024-07-10 14:17:20,340:INFO:_master_model_container: 8
2024-07-10 14:17:20,340:INFO:_display_container: 2
2024-07-10 14:17:20,340:INFO:BayesianRidge()
2024-07-10 14:17:20,340:INFO:create_model() successfully completed......................................
2024-07-10 14:17:20,454:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:20,455:INFO:Creating metrics dataframe
2024-07-10 14:17:20,457:INFO:Initializing Passive Aggressive Regressor
2024-07-10 14:17:20,458:INFO:Total runtime is 0.23295516570409136 minutes
2024-07-10 14:17:20,459:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:20,459:INFO:Initializing create_model()
2024-07-10 14:17:20,459:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:20,459:INFO:Checking exceptions
2024-07-10 14:17:20,459:INFO:Importing libraries
2024-07-10 14:17:20,459:INFO:Copying training dataset
2024-07-10 14:17:20,464:INFO:Defining folds
2024-07-10 14:17:20,464:INFO:Declaring metric variables
2024-07-10 14:17:20,465:INFO:Importing untrained model
2024-07-10 14:17:20,466:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 14:17:20,468:INFO:Starting cross validation
2024-07-10 14:17:20,473:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:21,378:INFO:Calculating mean and std
2024-07-10 14:17:21,378:INFO:Creating metrics dataframe
2024-07-10 14:17:21,379:INFO:Uploading results into container
2024-07-10 14:17:21,379:INFO:Uploading model into container now
2024-07-10 14:17:21,379:INFO:_master_model_container: 9
2024-07-10 14:17:21,379:INFO:_display_container: 2
2024-07-10 14:17:21,380:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 14:17:21,380:INFO:create_model() successfully completed......................................
2024-07-10 14:17:21,494:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:21,494:INFO:Creating metrics dataframe
2024-07-10 14:17:21,498:INFO:Initializing Huber Regressor
2024-07-10 14:17:21,498:INFO:Total runtime is 0.2502896149953206 minutes
2024-07-10 14:17:21,499:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:21,499:INFO:Initializing create_model()
2024-07-10 14:17:21,499:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:21,499:INFO:Checking exceptions
2024-07-10 14:17:21,499:INFO:Importing libraries
2024-07-10 14:17:21,499:INFO:Copying training dataset
2024-07-10 14:17:21,504:INFO:Defining folds
2024-07-10 14:17:21,504:INFO:Declaring metric variables
2024-07-10 14:17:21,505:INFO:Importing untrained model
2024-07-10 14:17:21,506:INFO:Huber Regressor Imported successfully
2024-07-10 14:17:21,508:INFO:Starting cross validation
2024-07-10 14:17:21,513:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:22,567:INFO:Calculating mean and std
2024-07-10 14:17:22,567:INFO:Creating metrics dataframe
2024-07-10 14:17:22,568:INFO:Uploading results into container
2024-07-10 14:17:22,568:INFO:Uploading model into container now
2024-07-10 14:17:22,569:INFO:_master_model_container: 10
2024-07-10 14:17:22,569:INFO:_display_container: 2
2024-07-10 14:17:22,569:INFO:HuberRegressor()
2024-07-10 14:17:22,569:INFO:create_model() successfully completed......................................
2024-07-10 14:17:22,683:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:22,683:INFO:Creating metrics dataframe
2024-07-10 14:17:22,686:INFO:Initializing K Neighbors Regressor
2024-07-10 14:17:22,686:INFO:Total runtime is 0.27009768486022945 minutes
2024-07-10 14:17:22,687:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:22,687:INFO:Initializing create_model()
2024-07-10 14:17:22,687:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:22,687:INFO:Checking exceptions
2024-07-10 14:17:22,687:INFO:Importing libraries
2024-07-10 14:17:22,688:INFO:Copying training dataset
2024-07-10 14:17:22,692:INFO:Defining folds
2024-07-10 14:17:22,692:INFO:Declaring metric variables
2024-07-10 14:17:22,694:INFO:Importing untrained model
2024-07-10 14:17:22,695:INFO:K Neighbors Regressor Imported successfully
2024-07-10 14:17:22,697:INFO:Starting cross validation
2024-07-10 14:17:22,702:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:23,655:INFO:Calculating mean and std
2024-07-10 14:17:23,656:INFO:Creating metrics dataframe
2024-07-10 14:17:23,657:INFO:Uploading results into container
2024-07-10 14:17:23,657:INFO:Uploading model into container now
2024-07-10 14:17:23,657:INFO:_master_model_container: 11
2024-07-10 14:17:23,657:INFO:_display_container: 2
2024-07-10 14:17:23,658:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 14:17:23,658:INFO:create_model() successfully completed......................................
2024-07-10 14:17:23,771:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:23,771:INFO:Creating metrics dataframe
2024-07-10 14:17:23,774:INFO:Initializing Decision Tree Regressor
2024-07-10 14:17:23,774:INFO:Total runtime is 0.28823736906051634 minutes
2024-07-10 14:17:23,776:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:23,776:INFO:Initializing create_model()
2024-07-10 14:17:23,776:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:23,776:INFO:Checking exceptions
2024-07-10 14:17:23,776:INFO:Importing libraries
2024-07-10 14:17:23,776:INFO:Copying training dataset
2024-07-10 14:17:23,780:INFO:Defining folds
2024-07-10 14:17:23,781:INFO:Declaring metric variables
2024-07-10 14:17:23,782:INFO:Importing untrained model
2024-07-10 14:17:23,783:INFO:Decision Tree Regressor Imported successfully
2024-07-10 14:17:23,785:INFO:Starting cross validation
2024-07-10 14:17:23,790:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:24,745:INFO:Calculating mean and std
2024-07-10 14:17:24,745:INFO:Creating metrics dataframe
2024-07-10 14:17:24,746:INFO:Uploading results into container
2024-07-10 14:17:24,746:INFO:Uploading model into container now
2024-07-10 14:17:24,747:INFO:_master_model_container: 12
2024-07-10 14:17:24,747:INFO:_display_container: 2
2024-07-10 14:17:24,747:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 14:17:24,747:INFO:create_model() successfully completed......................................
2024-07-10 14:17:24,861:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:24,861:INFO:Creating metrics dataframe
2024-07-10 14:17:24,865:INFO:Initializing Random Forest Regressor
2024-07-10 14:17:24,865:INFO:Total runtime is 0.3064104159673055 minutes
2024-07-10 14:17:24,866:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:24,866:INFO:Initializing create_model()
2024-07-10 14:17:24,866:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:24,866:INFO:Checking exceptions
2024-07-10 14:17:24,866:INFO:Importing libraries
2024-07-10 14:17:24,866:INFO:Copying training dataset
2024-07-10 14:17:24,871:INFO:Defining folds
2024-07-10 14:17:24,871:INFO:Declaring metric variables
2024-07-10 14:17:24,872:INFO:Importing untrained model
2024-07-10 14:17:24,873:INFO:Random Forest Regressor Imported successfully
2024-07-10 14:17:24,875:INFO:Starting cross validation
2024-07-10 14:17:24,880:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:29,828:INFO:Calculating mean and std
2024-07-10 14:17:29,828:INFO:Creating metrics dataframe
2024-07-10 14:17:29,829:INFO:Uploading results into container
2024-07-10 14:17:29,830:INFO:Uploading model into container now
2024-07-10 14:17:29,830:INFO:_master_model_container: 13
2024-07-10 14:17:29,830:INFO:_display_container: 2
2024-07-10 14:17:29,830:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:17:29,830:INFO:create_model() successfully completed......................................
2024-07-10 14:17:29,944:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:29,944:INFO:Creating metrics dataframe
2024-07-10 14:17:29,948:INFO:Initializing Extra Trees Regressor
2024-07-10 14:17:29,948:INFO:Total runtime is 0.39112391471862795 minutes
2024-07-10 14:17:29,949:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:29,949:INFO:Initializing create_model()
2024-07-10 14:17:29,949:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:29,949:INFO:Checking exceptions
2024-07-10 14:17:29,949:INFO:Importing libraries
2024-07-10 14:17:29,949:INFO:Copying training dataset
2024-07-10 14:17:29,954:INFO:Defining folds
2024-07-10 14:17:29,954:INFO:Declaring metric variables
2024-07-10 14:17:29,955:INFO:Importing untrained model
2024-07-10 14:17:29,956:INFO:Extra Trees Regressor Imported successfully
2024-07-10 14:17:29,958:INFO:Starting cross validation
2024-07-10 14:17:29,963:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:35,182:INFO:Calculating mean and std
2024-07-10 14:17:35,182:INFO:Creating metrics dataframe
2024-07-10 14:17:35,183:INFO:Uploading results into container
2024-07-10 14:17:35,183:INFO:Uploading model into container now
2024-07-10 14:17:35,184:INFO:_master_model_container: 14
2024-07-10 14:17:35,184:INFO:_display_container: 2
2024-07-10 14:17:35,184:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:17:35,184:INFO:create_model() successfully completed......................................
2024-07-10 14:17:35,298:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:35,298:INFO:Creating metrics dataframe
2024-07-10 14:17:35,302:INFO:Initializing AdaBoost Regressor
2024-07-10 14:17:35,302:INFO:Total runtime is 0.4803701519966126 minutes
2024-07-10 14:17:35,304:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:35,304:INFO:Initializing create_model()
2024-07-10 14:17:35,304:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:35,304:INFO:Checking exceptions
2024-07-10 14:17:35,304:INFO:Importing libraries
2024-07-10 14:17:35,304:INFO:Copying training dataset
2024-07-10 14:17:35,308:INFO:Defining folds
2024-07-10 14:17:35,309:INFO:Declaring metric variables
2024-07-10 14:17:35,310:INFO:Importing untrained model
2024-07-10 14:17:35,311:INFO:AdaBoost Regressor Imported successfully
2024-07-10 14:17:35,313:INFO:Starting cross validation
2024-07-10 14:17:35,318:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:37,251:INFO:Calculating mean and std
2024-07-10 14:17:37,252:INFO:Creating metrics dataframe
2024-07-10 14:17:37,253:INFO:Uploading results into container
2024-07-10 14:17:37,253:INFO:Uploading model into container now
2024-07-10 14:17:37,253:INFO:_master_model_container: 15
2024-07-10 14:17:37,253:INFO:_display_container: 2
2024-07-10 14:17:37,253:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 14:17:37,253:INFO:create_model() successfully completed......................................
2024-07-10 14:17:37,368:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:37,368:INFO:Creating metrics dataframe
2024-07-10 14:17:37,372:INFO:Initializing Gradient Boosting Regressor
2024-07-10 14:17:37,372:INFO:Total runtime is 0.5148614803949992 minutes
2024-07-10 14:17:37,373:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:37,373:INFO:Initializing create_model()
2024-07-10 14:17:37,373:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:37,373:INFO:Checking exceptions
2024-07-10 14:17:37,373:INFO:Importing libraries
2024-07-10 14:17:37,373:INFO:Copying training dataset
2024-07-10 14:17:37,378:INFO:Defining folds
2024-07-10 14:17:37,378:INFO:Declaring metric variables
2024-07-10 14:17:37,379:INFO:Importing untrained model
2024-07-10 14:17:37,380:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 14:17:37,382:INFO:Starting cross validation
2024-07-10 14:17:37,387:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:39,880:INFO:Calculating mean and std
2024-07-10 14:17:39,880:INFO:Creating metrics dataframe
2024-07-10 14:17:39,881:INFO:Uploading results into container
2024-07-10 14:17:39,881:INFO:Uploading model into container now
2024-07-10 14:17:39,882:INFO:_master_model_container: 16
2024-07-10 14:17:39,882:INFO:_display_container: 2
2024-07-10 14:17:39,882:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 14:17:39,882:INFO:create_model() successfully completed......................................
2024-07-10 14:17:39,996:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:39,996:INFO:Creating metrics dataframe
2024-07-10 14:17:40,000:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 14:17:40,000:INFO:Total runtime is 0.5586586157480876 minutes
2024-07-10 14:17:40,001:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:40,001:INFO:Initializing create_model()
2024-07-10 14:17:40,001:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:40,001:INFO:Checking exceptions
2024-07-10 14:17:40,001:INFO:Importing libraries
2024-07-10 14:17:40,001:INFO:Copying training dataset
2024-07-10 14:17:40,006:INFO:Defining folds
2024-07-10 14:17:40,006:INFO:Declaring metric variables
2024-07-10 14:17:40,007:INFO:Importing untrained model
2024-07-10 14:17:40,008:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 14:17:40,010:INFO:Starting cross validation
2024-07-10 14:17:40,015:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:43,400:INFO:Calculating mean and std
2024-07-10 14:17:43,401:INFO:Creating metrics dataframe
2024-07-10 14:17:43,401:INFO:Uploading results into container
2024-07-10 14:17:43,402:INFO:Uploading model into container now
2024-07-10 14:17:43,402:INFO:_master_model_container: 17
2024-07-10 14:17:43,402:INFO:_display_container: 2
2024-07-10 14:17:43,402:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:17:43,402:INFO:create_model() successfully completed......................................
2024-07-10 14:17:43,516:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:43,516:INFO:Creating metrics dataframe
2024-07-10 14:17:43,520:INFO:Initializing Dummy Regressor
2024-07-10 14:17:43,520:INFO:Total runtime is 0.6173316478729248 minutes
2024-07-10 14:17:43,521:INFO:SubProcess create_model() called ==================================
2024-07-10 14:17:43,521:INFO:Initializing create_model()
2024-07-10 14:17:43,521:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x36d0b0190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:43,521:INFO:Checking exceptions
2024-07-10 14:17:43,522:INFO:Importing libraries
2024-07-10 14:17:43,522:INFO:Copying training dataset
2024-07-10 14:17:43,526:INFO:Defining folds
2024-07-10 14:17:43,526:INFO:Declaring metric variables
2024-07-10 14:17:43,527:INFO:Importing untrained model
2024-07-10 14:17:43,528:INFO:Dummy Regressor Imported successfully
2024-07-10 14:17:43,530:INFO:Starting cross validation
2024-07-10 14:17:43,536:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:17:44,418:INFO:Calculating mean and std
2024-07-10 14:17:44,419:INFO:Creating metrics dataframe
2024-07-10 14:17:44,420:INFO:Uploading results into container
2024-07-10 14:17:44,420:INFO:Uploading model into container now
2024-07-10 14:17:44,420:INFO:_master_model_container: 18
2024-07-10 14:17:44,420:INFO:_display_container: 2
2024-07-10 14:17:44,420:INFO:DummyRegressor()
2024-07-10 14:17:44,420:INFO:create_model() successfully completed......................................
2024-07-10 14:17:44,536:INFO:SubProcess create_model() end ==================================
2024-07-10 14:17:44,536:INFO:Creating metrics dataframe
2024-07-10 14:17:44,543:INFO:Initializing create_model()
2024-07-10 14:17:44,543:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:17:44,543:INFO:Checking exceptions
2024-07-10 14:17:44,544:INFO:Importing libraries
2024-07-10 14:17:44,544:INFO:Copying training dataset
2024-07-10 14:17:44,548:INFO:Defining folds
2024-07-10 14:17:44,548:INFO:Declaring metric variables
2024-07-10 14:17:44,548:INFO:Importing untrained model
2024-07-10 14:17:44,548:INFO:Declaring custom model
2024-07-10 14:17:44,549:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 14:17:44,552:INFO:Cross validation set to False
2024-07-10 14:17:44,552:INFO:Fitting Model
2024-07-10 14:17:44,885:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-10 14:17:44,889:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001789 seconds.
2024-07-10 14:17:44,889:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-10 14:17:44,889:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-10 14:17:44,889:INFO:[LightGBM] [Info] Total Bins 962
2024-07-10 14:17:44,889:INFO:[LightGBM] [Info] Number of data points in the train set: 6254, number of used features: 66
2024-07-10 14:17:44,889:INFO:[LightGBM] [Info] Start training from score 3683.536457
2024-07-10 14:17:45,185:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:17:45,185:INFO:create_model() successfully completed......................................
2024-07-10 14:17:45,309:INFO:_master_model_container: 18
2024-07-10 14:17:45,309:INFO:_display_container: 2
2024-07-10 14:17:45,309:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:17:45,309:INFO:compare_models() successfully completed......................................
2024-07-10 14:18:27,622:INFO:Initializing plot_model()
2024-07-10 14:18:27,623:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=residuals, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 14:18:27,623:INFO:Checking exceptions
2024-07-10 14:18:27,633:INFO:Preloading libraries
2024-07-10 14:18:27,640:INFO:Copying training dataset
2024-07-10 14:18:27,640:INFO:Plot type: residuals
2024-07-10 14:18:27,853:INFO:Fitting Model
2024-07-10 14:18:27,879:INFO:Scoring test/hold-out set
2024-07-10 14:18:28,115:INFO:Visual Rendered Successfully
2024-07-10 14:18:28,253:INFO:plot_model() successfully completed......................................
2024-07-10 14:18:30,977:INFO:Initializing plot_model()
2024-07-10 14:18:30,978:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x36b6c3410>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 14:18:30,978:INFO:Checking exceptions
2024-07-10 14:18:30,990:INFO:Preloading libraries
2024-07-10 14:18:30,995:INFO:Copying training dataset
2024-07-10 14:18:30,995:INFO:Plot type: error
2024-07-10 14:18:31,193:INFO:Fitting Model
2024-07-10 14:18:31,193:INFO:Scoring test/hold-out set
2024-07-10 14:18:31,290:INFO:Visual Rendered Successfully
2024-07-10 14:18:31,427:INFO:plot_model() successfully completed......................................
2024-07-10 14:21:53,191:INFO:PyCaret RegressionExperiment
2024-07-10 14:21:53,192:INFO:Logging name: reg-default-name
2024-07-10 14:21:53,192:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 14:21:53,192:INFO:version 3.3.1
2024-07-10 14:21:53,192:INFO:Initializing setup()
2024-07-10 14:21:53,192:INFO:self.USI: 7805
2024-07-10 14:21:53,192:INFO:self._variable_keys: {'log_plots_param', 'USI', 'memory', 'X_train', 'X_test', 'fold_generator', 'data', '_available_plots', 'idx', 'n_jobs_param', 'transform_target_param', 'logging_param', 'html_param', 'y_test', 'gpu_param', 'exp_id', 'y', 'target_param', 'fold_groups_param', 'seed', 'X', 'fold_shuffle_param', '_ml_usecase', 'exp_name_log', 'gpu_n_jobs_param', 'pipeline', 'y_train'}
2024-07-10 14:21:53,192:INFO:Checking environment
2024-07-10 14:21:53,192:INFO:python_version: 3.11.8
2024-07-10 14:21:53,192:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 14:21:53,192:INFO:machine: arm64
2024-07-10 14:21:53,192:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 14:21:53,192:INFO:Memory: svmem(total=17179869184, available=5405425664, percent=68.5, used=6246416384, free=694059008, active=4836589568, inactive=4708696064, wired=1409826816)
2024-07-10 14:21:53,192:INFO:Physical Core: 8
2024-07-10 14:21:53,193:INFO:Logical Core: 8
2024-07-10 14:21:53,193:INFO:Checking libraries
2024-07-10 14:21:53,193:INFO:System:
2024-07-10 14:21:53,193:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 14:21:53,193:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 14:21:53,193:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 14:21:53,193:INFO:PyCaret required dependencies:
2024-07-10 14:21:53,193:INFO:                 pip: 23.3.1
2024-07-10 14:21:53,193:INFO:          setuptools: 68.2.2
2024-07-10 14:21:53,193:INFO:             pycaret: 3.3.1
2024-07-10 14:21:53,193:INFO:             IPython: 8.20.0
2024-07-10 14:21:53,193:INFO:          ipywidgets: 7.6.5
2024-07-10 14:21:53,193:INFO:                tqdm: 4.65.0
2024-07-10 14:21:53,193:INFO:               numpy: 1.26.4
2024-07-10 14:21:53,193:INFO:              pandas: 2.1.4
2024-07-10 14:21:53,193:INFO:              jinja2: 3.1.3
2024-07-10 14:21:53,193:INFO:               scipy: 1.11.4
2024-07-10 14:21:53,193:INFO:              joblib: 1.2.0
2024-07-10 14:21:53,193:INFO:             sklearn: 1.4.2
2024-07-10 14:21:53,193:INFO:                pyod: 1.1.3
2024-07-10 14:21:53,193:INFO:            imblearn: 0.12.2
2024-07-10 14:21:53,193:INFO:   category_encoders: 2.6.3
2024-07-10 14:21:53,193:INFO:            lightgbm: 4.3.0
2024-07-10 14:21:53,193:INFO:               numba: 0.59.0
2024-07-10 14:21:53,193:INFO:            requests: 2.31.0
2024-07-10 14:21:53,193:INFO:          matplotlib: 3.8.0
2024-07-10 14:21:53,193:INFO:          scikitplot: 0.3.7
2024-07-10 14:21:53,193:INFO:         yellowbrick: 1.5
2024-07-10 14:21:53,193:INFO:              plotly: 5.22.0
2024-07-10 14:21:53,193:INFO:    plotly-resampler: Not installed
2024-07-10 14:21:53,194:INFO:             kaleido: 0.2.1
2024-07-10 14:21:53,194:INFO:           schemdraw: 0.15
2024-07-10 14:21:53,194:INFO:         statsmodels: 0.14.0
2024-07-10 14:21:53,194:INFO:              sktime: 0.26.0
2024-07-10 14:21:53,194:INFO:               tbats: 1.1.3
2024-07-10 14:21:53,194:INFO:            pmdarima: 2.0.4
2024-07-10 14:21:53,194:INFO:              psutil: 5.9.0
2024-07-10 14:21:53,194:INFO:          markupsafe: 2.1.3
2024-07-10 14:21:53,194:INFO:             pickle5: Not installed
2024-07-10 14:21:53,194:INFO:         cloudpickle: 2.2.1
2024-07-10 14:21:53,194:INFO:         deprecation: 2.1.0
2024-07-10 14:21:53,194:INFO:              xxhash: 3.4.1
2024-07-10 14:21:53,194:INFO:           wurlitzer: 3.0.2
2024-07-10 14:21:53,194:INFO:PyCaret optional dependencies:
2024-07-10 14:21:53,194:INFO:                shap: Not installed
2024-07-10 14:21:53,194:INFO:           interpret: Not installed
2024-07-10 14:21:53,194:INFO:                umap: 0.5.5
2024-07-10 14:21:53,194:INFO:     ydata_profiling: Not installed
2024-07-10 14:21:53,194:INFO:  explainerdashboard: Not installed
2024-07-10 14:21:53,194:INFO:             autoviz: Not installed
2024-07-10 14:21:53,194:INFO:           fairlearn: Not installed
2024-07-10 14:21:53,194:INFO:          deepchecks: Not installed
2024-07-10 14:21:53,194:INFO:             xgboost: Not installed
2024-07-10 14:21:53,194:INFO:            catboost: Not installed
2024-07-10 14:21:53,194:INFO:              kmodes: Not installed
2024-07-10 14:21:53,194:INFO:             mlxtend: Not installed
2024-07-10 14:21:53,194:INFO:       statsforecast: Not installed
2024-07-10 14:21:53,194:INFO:        tune_sklearn: Not installed
2024-07-10 14:21:53,194:INFO:                 ray: Not installed
2024-07-10 14:21:53,194:INFO:            hyperopt: Not installed
2024-07-10 14:21:53,194:INFO:              optuna: Not installed
2024-07-10 14:21:53,194:INFO:               skopt: Not installed
2024-07-10 14:21:53,194:INFO:              mlflow: 2.13.0
2024-07-10 14:21:53,194:INFO:              gradio: Not installed
2024-07-10 14:21:53,194:INFO:             fastapi: Not installed
2024-07-10 14:21:53,194:INFO:             uvicorn: Not installed
2024-07-10 14:21:53,195:INFO:              m2cgen: Not installed
2024-07-10 14:21:53,195:INFO:           evidently: Not installed
2024-07-10 14:21:53,195:INFO:               fugue: Not installed
2024-07-10 14:21:53,195:INFO:           streamlit: 1.30.0
2024-07-10 14:21:53,195:INFO:             prophet: Not installed
2024-07-10 14:21:53,195:INFO:None
2024-07-10 14:21:53,195:INFO:Set up data.
2024-07-10 14:21:53,206:INFO:Set up folding strategy.
2024-07-10 14:21:53,206:INFO:Set up train/test split.
2024-07-10 14:21:53,211:INFO:Set up index.
2024-07-10 14:21:53,211:INFO:Assigning column types.
2024-07-10 14:21:53,216:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 14:21:53,216:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,218:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,220:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,247:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,266:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,267:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,267:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,267:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,269:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,271:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,295:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,312:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,312:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,312:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,312:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 14:21:53,314:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,316:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,340:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,357:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,358:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,358:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,359:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,361:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,385:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,402:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,402:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,402:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,402:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 14:21:53,405:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,429:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,446:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,446:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,446:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,450:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,474:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,492:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,492:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,492:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,492:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 14:21:53,520:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,537:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,537:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,537:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,565:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,583:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,583:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,583:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,583:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 14:21:53,610:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,628:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,628:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,658:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 14:21:53,675:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,675:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,675:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 14:21:53,719:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,719:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,763:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,763:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:53,766:INFO:Preparing preprocessing pipeline...
2024-07-10 14:21:53,766:INFO:Set up simple imputation.
2024-07-10 14:21:53,768:INFO:Set up encoding of categorical features.
2024-07-10 14:21:53,768:INFO:Set up removing multicollinearity.
2024-07-10 14:21:53,768:INFO:Set up column transformation.
2024-07-10 14:21:53,768:INFO:Set up feature normalization.
2024-07-10 14:21:54,018:INFO:Finished creating preprocessing pipeline.
2024-07-10 14:21:54,022:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms',
                                             'is_exact_address_hidden',
                                             'is_floor_under', 'rent_price',
                                             'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitte...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 14:21:54,023:INFO:Creating final display dataframe.
2024-07-10 14:21:54,214:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape         (9406, 26)
4        Transformed data shape         (9406, 69)
5   Transformed train set shape         (6584, 69)
6    Transformed test set shape         (2822, 69)
7              Numeric features                 20
8          Categorical features                  5
9      Rows with missing values              51.3%
10                   Preprocess               True
11              Imputation type             simple
12           Numeric imputation               mean
13       Categorical imputation               mode
14     Maximum one-hot encoding                 25
15              Encoding method               None
16     Remove multicollinearity               True
17  Multicollinearity threshold                0.9
18               Transformation               True
19        Transformation method        yeo-johnson
20                    Normalize               True
21             Normalize method             zscore
22               Fold Generator              KFold
23                  Fold Number                 10
24                     CPU Jobs                 -1
25                      Use GPU              False
26               Log Experiment              False
27              Experiment Name   reg-default-name
28                          USI               7805
2024-07-10 14:21:54,264:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:54,264:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:54,310:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:54,310:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 14:21:54,310:INFO:setup() successfully completed in 1.13s...............
2024-07-10 14:21:55,073:INFO:Initializing compare_models()
2024-07-10 14:21:55,073:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 14:21:55,074:INFO:Checking exceptions
2024-07-10 14:21:55,079:INFO:Preparing display monitor
2024-07-10 14:21:55,095:INFO:Initializing Linear Regression
2024-07-10 14:21:55,095:INFO:Total runtime is 2.5312105814615887e-06 minutes
2024-07-10 14:21:55,098:INFO:SubProcess create_model() called ==================================
2024-07-10 14:21:55,099:INFO:Initializing create_model()
2024-07-10 14:21:55,099:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:21:55,099:INFO:Checking exceptions
2024-07-10 14:21:55,099:INFO:Importing libraries
2024-07-10 14:21:55,099:INFO:Copying training dataset
2024-07-10 14:21:55,108:INFO:Defining folds
2024-07-10 14:21:55,108:INFO:Declaring metric variables
2024-07-10 14:21:55,109:INFO:Importing untrained model
2024-07-10 14:21:55,111:INFO:Linear Regression Imported successfully
2024-07-10 14:21:55,113:INFO:Starting cross validation
2024-07-10 14:21:55,115:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:21:55,852:INFO:Calculating mean and std
2024-07-10 14:21:55,852:INFO:Creating metrics dataframe
2024-07-10 14:21:55,853:INFO:Uploading results into container
2024-07-10 14:21:55,853:INFO:Uploading model into container now
2024-07-10 14:21:55,853:INFO:_master_model_container: 1
2024-07-10 14:21:55,853:INFO:_display_container: 2
2024-07-10 14:21:55,853:INFO:LinearRegression(n_jobs=-1)
2024-07-10 14:21:55,853:INFO:create_model() successfully completed......................................
2024-07-10 14:21:56,047:INFO:SubProcess create_model() end ==================================
2024-07-10 14:21:56,047:INFO:Creating metrics dataframe
2024-07-10 14:21:56,049:INFO:Initializing Lasso Regression
2024-07-10 14:21:56,049:INFO:Total runtime is 0.015902582804361978 minutes
2024-07-10 14:21:56,050:INFO:SubProcess create_model() called ==================================
2024-07-10 14:21:56,051:INFO:Initializing create_model()
2024-07-10 14:21:56,051:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:21:56,051:INFO:Checking exceptions
2024-07-10 14:21:56,051:INFO:Importing libraries
2024-07-10 14:21:56,051:INFO:Copying training dataset
2024-07-10 14:21:56,055:INFO:Defining folds
2024-07-10 14:21:56,055:INFO:Declaring metric variables
2024-07-10 14:21:56,056:INFO:Importing untrained model
2024-07-10 14:21:56,057:INFO:Lasso Regression Imported successfully
2024-07-10 14:21:56,059:INFO:Starting cross validation
2024-07-10 14:21:56,060:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:21:56,802:INFO:Calculating mean and std
2024-07-10 14:21:56,803:INFO:Creating metrics dataframe
2024-07-10 14:21:56,804:INFO:Uploading results into container
2024-07-10 14:21:56,804:INFO:Uploading model into container now
2024-07-10 14:21:56,804:INFO:_master_model_container: 2
2024-07-10 14:21:56,804:INFO:_display_container: 2
2024-07-10 14:21:56,804:INFO:Lasso(random_state=9)
2024-07-10 14:21:56,804:INFO:create_model() successfully completed......................................
2024-07-10 14:21:56,991:INFO:SubProcess create_model() end ==================================
2024-07-10 14:21:56,991:INFO:Creating metrics dataframe
2024-07-10 14:21:56,993:INFO:Initializing Ridge Regression
2024-07-10 14:21:56,993:INFO:Total runtime is 0.03163739840189616 minutes
2024-07-10 14:21:56,994:INFO:SubProcess create_model() called ==================================
2024-07-10 14:21:56,994:INFO:Initializing create_model()
2024-07-10 14:21:56,995:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:21:56,995:INFO:Checking exceptions
2024-07-10 14:21:56,995:INFO:Importing libraries
2024-07-10 14:21:56,995:INFO:Copying training dataset
2024-07-10 14:21:56,999:INFO:Defining folds
2024-07-10 14:21:57,000:INFO:Declaring metric variables
2024-07-10 14:21:57,001:INFO:Importing untrained model
2024-07-10 14:21:57,002:INFO:Ridge Regression Imported successfully
2024-07-10 14:21:57,004:INFO:Starting cross validation
2024-07-10 14:21:57,005:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:21:57,686:INFO:Calculating mean and std
2024-07-10 14:21:57,687:INFO:Creating metrics dataframe
2024-07-10 14:21:57,688:INFO:Uploading results into container
2024-07-10 14:21:57,688:INFO:Uploading model into container now
2024-07-10 14:21:57,688:INFO:_master_model_container: 3
2024-07-10 14:21:57,688:INFO:_display_container: 2
2024-07-10 14:21:57,689:INFO:Ridge(random_state=9)
2024-07-10 14:21:57,689:INFO:create_model() successfully completed......................................
2024-07-10 14:21:57,868:INFO:SubProcess create_model() end ==================================
2024-07-10 14:21:57,868:INFO:Creating metrics dataframe
2024-07-10 14:21:57,871:INFO:Initializing Elastic Net
2024-07-10 14:21:57,871:INFO:Total runtime is 0.04626650015513102 minutes
2024-07-10 14:21:57,872:INFO:SubProcess create_model() called ==================================
2024-07-10 14:21:57,872:INFO:Initializing create_model()
2024-07-10 14:21:57,872:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:21:57,872:INFO:Checking exceptions
2024-07-10 14:21:57,873:INFO:Importing libraries
2024-07-10 14:21:57,873:INFO:Copying training dataset
2024-07-10 14:21:57,877:INFO:Defining folds
2024-07-10 14:21:57,877:INFO:Declaring metric variables
2024-07-10 14:21:57,878:INFO:Importing untrained model
2024-07-10 14:21:57,880:INFO:Elastic Net Imported successfully
2024-07-10 14:21:57,881:INFO:Starting cross validation
2024-07-10 14:21:57,883:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:21:58,563:INFO:Calculating mean and std
2024-07-10 14:21:58,564:INFO:Creating metrics dataframe
2024-07-10 14:21:58,565:INFO:Uploading results into container
2024-07-10 14:21:58,565:INFO:Uploading model into container now
2024-07-10 14:21:58,565:INFO:_master_model_container: 4
2024-07-10 14:21:58,565:INFO:_display_container: 2
2024-07-10 14:21:58,566:INFO:ElasticNet(random_state=9)
2024-07-10 14:21:58,566:INFO:create_model() successfully completed......................................
2024-07-10 14:21:58,745:INFO:SubProcess create_model() end ==================================
2024-07-10 14:21:58,745:INFO:Creating metrics dataframe
2024-07-10 14:21:58,748:INFO:Initializing Least Angle Regression
2024-07-10 14:21:58,748:INFO:Total runtime is 0.060883363087972 minutes
2024-07-10 14:21:58,749:INFO:SubProcess create_model() called ==================================
2024-07-10 14:21:58,749:INFO:Initializing create_model()
2024-07-10 14:21:58,749:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:21:58,749:INFO:Checking exceptions
2024-07-10 14:21:58,750:INFO:Importing libraries
2024-07-10 14:21:58,750:INFO:Copying training dataset
2024-07-10 14:21:58,755:INFO:Defining folds
2024-07-10 14:21:58,755:INFO:Declaring metric variables
2024-07-10 14:21:58,756:INFO:Importing untrained model
2024-07-10 14:21:58,757:INFO:Least Angle Regression Imported successfully
2024-07-10 14:21:58,759:INFO:Starting cross validation
2024-07-10 14:21:58,760:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:21:59,457:INFO:Calculating mean and std
2024-07-10 14:21:59,458:INFO:Creating metrics dataframe
2024-07-10 14:21:59,459:INFO:Uploading results into container
2024-07-10 14:21:59,459:INFO:Uploading model into container now
2024-07-10 14:21:59,459:INFO:_master_model_container: 5
2024-07-10 14:21:59,459:INFO:_display_container: 2
2024-07-10 14:21:59,459:INFO:Lars(random_state=9)
2024-07-10 14:21:59,459:INFO:create_model() successfully completed......................................
2024-07-10 14:21:59,639:INFO:SubProcess create_model() end ==================================
2024-07-10 14:21:59,639:INFO:Creating metrics dataframe
2024-07-10 14:21:59,642:INFO:Initializing Lasso Least Angle Regression
2024-07-10 14:21:59,642:INFO:Total runtime is 0.07578559716542561 minutes
2024-07-10 14:21:59,643:INFO:SubProcess create_model() called ==================================
2024-07-10 14:21:59,643:INFO:Initializing create_model()
2024-07-10 14:21:59,643:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:21:59,644:INFO:Checking exceptions
2024-07-10 14:21:59,644:INFO:Importing libraries
2024-07-10 14:21:59,644:INFO:Copying training dataset
2024-07-10 14:21:59,648:INFO:Defining folds
2024-07-10 14:21:59,648:INFO:Declaring metric variables
2024-07-10 14:21:59,649:INFO:Importing untrained model
2024-07-10 14:21:59,651:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 14:21:59,653:INFO:Starting cross validation
2024-07-10 14:21:59,654:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:00,341:INFO:Calculating mean and std
2024-07-10 14:22:00,342:INFO:Creating metrics dataframe
2024-07-10 14:22:00,342:INFO:Uploading results into container
2024-07-10 14:22:00,343:INFO:Uploading model into container now
2024-07-10 14:22:00,343:INFO:_master_model_container: 6
2024-07-10 14:22:00,343:INFO:_display_container: 2
2024-07-10 14:22:00,343:INFO:LassoLars(random_state=9)
2024-07-10 14:22:00,343:INFO:create_model() successfully completed......................................
2024-07-10 14:22:00,523:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:00,523:INFO:Creating metrics dataframe
2024-07-10 14:22:00,526:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 14:22:00,526:INFO:Total runtime is 0.09052437941233317 minutes
2024-07-10 14:22:00,528:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:00,528:INFO:Initializing create_model()
2024-07-10 14:22:00,528:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:00,528:INFO:Checking exceptions
2024-07-10 14:22:00,528:INFO:Importing libraries
2024-07-10 14:22:00,528:INFO:Copying training dataset
2024-07-10 14:22:00,533:INFO:Defining folds
2024-07-10 14:22:00,533:INFO:Declaring metric variables
2024-07-10 14:22:00,534:INFO:Importing untrained model
2024-07-10 14:22:00,535:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 14:22:00,537:INFO:Starting cross validation
2024-07-10 14:22:00,538:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:01,317:INFO:Calculating mean and std
2024-07-10 14:22:01,318:INFO:Creating metrics dataframe
2024-07-10 14:22:01,319:INFO:Uploading results into container
2024-07-10 14:22:01,319:INFO:Uploading model into container now
2024-07-10 14:22:01,319:INFO:_master_model_container: 7
2024-07-10 14:22:01,319:INFO:_display_container: 2
2024-07-10 14:22:01,319:INFO:OrthogonalMatchingPursuit()
2024-07-10 14:22:01,319:INFO:create_model() successfully completed......................................
2024-07-10 14:22:01,516:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:01,516:INFO:Creating metrics dataframe
2024-07-10 14:22:01,520:INFO:Initializing Bayesian Ridge
2024-07-10 14:22:01,520:INFO:Total runtime is 0.10707811514536539 minutes
2024-07-10 14:22:01,521:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:01,521:INFO:Initializing create_model()
2024-07-10 14:22:01,521:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:01,521:INFO:Checking exceptions
2024-07-10 14:22:01,521:INFO:Importing libraries
2024-07-10 14:22:01,521:INFO:Copying training dataset
2024-07-10 14:22:01,527:INFO:Defining folds
2024-07-10 14:22:01,527:INFO:Declaring metric variables
2024-07-10 14:22:01,528:INFO:Importing untrained model
2024-07-10 14:22:01,530:INFO:Bayesian Ridge Imported successfully
2024-07-10 14:22:01,533:INFO:Starting cross validation
2024-07-10 14:22:01,534:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:02,220:INFO:Calculating mean and std
2024-07-10 14:22:02,221:INFO:Creating metrics dataframe
2024-07-10 14:22:02,222:INFO:Uploading results into container
2024-07-10 14:22:02,222:INFO:Uploading model into container now
2024-07-10 14:22:02,222:INFO:_master_model_container: 8
2024-07-10 14:22:02,222:INFO:_display_container: 2
2024-07-10 14:22:02,222:INFO:BayesianRidge()
2024-07-10 14:22:02,222:INFO:create_model() successfully completed......................................
2024-07-10 14:22:02,405:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:02,405:INFO:Creating metrics dataframe
2024-07-10 14:22:02,408:INFO:Initializing Passive Aggressive Regressor
2024-07-10 14:22:02,408:INFO:Total runtime is 0.12188209692637125 minutes
2024-07-10 14:22:02,409:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:02,409:INFO:Initializing create_model()
2024-07-10 14:22:02,409:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:02,409:INFO:Checking exceptions
2024-07-10 14:22:02,409:INFO:Importing libraries
2024-07-10 14:22:02,409:INFO:Copying training dataset
2024-07-10 14:22:02,414:INFO:Defining folds
2024-07-10 14:22:02,414:INFO:Declaring metric variables
2024-07-10 14:22:02,415:INFO:Importing untrained model
2024-07-10 14:22:02,416:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 14:22:02,418:INFO:Starting cross validation
2024-07-10 14:22:02,420:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:03,114:INFO:Calculating mean and std
2024-07-10 14:22:03,114:INFO:Creating metrics dataframe
2024-07-10 14:22:03,115:INFO:Uploading results into container
2024-07-10 14:22:03,115:INFO:Uploading model into container now
2024-07-10 14:22:03,115:INFO:_master_model_container: 9
2024-07-10 14:22:03,115:INFO:_display_container: 2
2024-07-10 14:22:03,115:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 14:22:03,115:INFO:create_model() successfully completed......................................
2024-07-10 14:22:03,296:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:03,296:INFO:Creating metrics dataframe
2024-07-10 14:22:03,299:INFO:Initializing Huber Regressor
2024-07-10 14:22:03,300:INFO:Total runtime is 0.13674184878667195 minutes
2024-07-10 14:22:03,301:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:03,301:INFO:Initializing create_model()
2024-07-10 14:22:03,301:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:03,301:INFO:Checking exceptions
2024-07-10 14:22:03,301:INFO:Importing libraries
2024-07-10 14:22:03,301:INFO:Copying training dataset
2024-07-10 14:22:03,306:INFO:Defining folds
2024-07-10 14:22:03,306:INFO:Declaring metric variables
2024-07-10 14:22:03,307:INFO:Importing untrained model
2024-07-10 14:22:03,308:INFO:Huber Regressor Imported successfully
2024-07-10 14:22:03,310:INFO:Starting cross validation
2024-07-10 14:22:03,311:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:04,169:INFO:Calculating mean and std
2024-07-10 14:22:04,170:INFO:Creating metrics dataframe
2024-07-10 14:22:04,170:INFO:Uploading results into container
2024-07-10 14:22:04,171:INFO:Uploading model into container now
2024-07-10 14:22:04,171:INFO:_master_model_container: 10
2024-07-10 14:22:04,171:INFO:_display_container: 2
2024-07-10 14:22:04,171:INFO:HuberRegressor()
2024-07-10 14:22:04,171:INFO:create_model() successfully completed......................................
2024-07-10 14:22:04,351:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:04,351:INFO:Creating metrics dataframe
2024-07-10 14:22:04,354:INFO:Initializing K Neighbors Regressor
2024-07-10 14:22:04,354:INFO:Total runtime is 0.15432299772898356 minutes
2024-07-10 14:22:04,356:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:04,356:INFO:Initializing create_model()
2024-07-10 14:22:04,356:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:04,356:INFO:Checking exceptions
2024-07-10 14:22:04,356:INFO:Importing libraries
2024-07-10 14:22:04,356:INFO:Copying training dataset
2024-07-10 14:22:04,361:INFO:Defining folds
2024-07-10 14:22:04,361:INFO:Declaring metric variables
2024-07-10 14:22:04,362:INFO:Importing untrained model
2024-07-10 14:22:04,364:INFO:K Neighbors Regressor Imported successfully
2024-07-10 14:22:04,366:INFO:Starting cross validation
2024-07-10 14:22:04,368:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:05,073:INFO:Calculating mean and std
2024-07-10 14:22:05,074:INFO:Creating metrics dataframe
2024-07-10 14:22:05,075:INFO:Uploading results into container
2024-07-10 14:22:05,075:INFO:Uploading model into container now
2024-07-10 14:22:05,075:INFO:_master_model_container: 11
2024-07-10 14:22:05,075:INFO:_display_container: 2
2024-07-10 14:22:05,075:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 14:22:05,075:INFO:create_model() successfully completed......................................
2024-07-10 14:22:05,256:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:05,256:INFO:Creating metrics dataframe
2024-07-10 14:22:05,259:INFO:Initializing Decision Tree Regressor
2024-07-10 14:22:05,259:INFO:Total runtime is 0.16940463383992513 minutes
2024-07-10 14:22:05,260:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:05,261:INFO:Initializing create_model()
2024-07-10 14:22:05,261:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:05,261:INFO:Checking exceptions
2024-07-10 14:22:05,261:INFO:Importing libraries
2024-07-10 14:22:05,261:INFO:Copying training dataset
2024-07-10 14:22:05,265:INFO:Defining folds
2024-07-10 14:22:05,266:INFO:Declaring metric variables
2024-07-10 14:22:05,267:INFO:Importing untrained model
2024-07-10 14:22:05,268:INFO:Decision Tree Regressor Imported successfully
2024-07-10 14:22:05,270:INFO:Starting cross validation
2024-07-10 14:22:05,271:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:06,023:INFO:Calculating mean and std
2024-07-10 14:22:06,024:INFO:Creating metrics dataframe
2024-07-10 14:22:06,025:INFO:Uploading results into container
2024-07-10 14:22:06,025:INFO:Uploading model into container now
2024-07-10 14:22:06,025:INFO:_master_model_container: 12
2024-07-10 14:22:06,025:INFO:_display_container: 2
2024-07-10 14:22:06,025:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 14:22:06,025:INFO:create_model() successfully completed......................................
2024-07-10 14:22:06,206:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:06,206:INFO:Creating metrics dataframe
2024-07-10 14:22:06,210:INFO:Initializing Random Forest Regressor
2024-07-10 14:22:06,210:INFO:Total runtime is 0.18524608214696248 minutes
2024-07-10 14:22:06,211:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:06,211:INFO:Initializing create_model()
2024-07-10 14:22:06,211:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:06,211:INFO:Checking exceptions
2024-07-10 14:22:06,211:INFO:Importing libraries
2024-07-10 14:22:06,211:INFO:Copying training dataset
2024-07-10 14:22:06,216:INFO:Defining folds
2024-07-10 14:22:06,216:INFO:Declaring metric variables
2024-07-10 14:22:06,217:INFO:Importing untrained model
2024-07-10 14:22:06,218:INFO:Random Forest Regressor Imported successfully
2024-07-10 14:22:06,220:INFO:Starting cross validation
2024-07-10 14:22:06,221:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:11,186:INFO:Calculating mean and std
2024-07-10 14:22:11,186:INFO:Creating metrics dataframe
2024-07-10 14:22:11,187:INFO:Uploading results into container
2024-07-10 14:22:11,187:INFO:Uploading model into container now
2024-07-10 14:22:11,188:INFO:_master_model_container: 13
2024-07-10 14:22:11,188:INFO:_display_container: 2
2024-07-10 14:22:11,188:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:22:11,188:INFO:create_model() successfully completed......................................
2024-07-10 14:22:11,369:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:11,369:INFO:Creating metrics dataframe
2024-07-10 14:22:11,373:INFO:Initializing Extra Trees Regressor
2024-07-10 14:22:11,373:INFO:Total runtime is 0.2712934652964274 minutes
2024-07-10 14:22:11,374:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:11,374:INFO:Initializing create_model()
2024-07-10 14:22:11,374:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:11,374:INFO:Checking exceptions
2024-07-10 14:22:11,374:INFO:Importing libraries
2024-07-10 14:22:11,374:INFO:Copying training dataset
2024-07-10 14:22:11,379:INFO:Defining folds
2024-07-10 14:22:11,379:INFO:Declaring metric variables
2024-07-10 14:22:11,380:INFO:Importing untrained model
2024-07-10 14:22:11,381:INFO:Extra Trees Regressor Imported successfully
2024-07-10 14:22:11,383:INFO:Starting cross validation
2024-07-10 14:22:11,384:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:16,543:INFO:Calculating mean and std
2024-07-10 14:22:16,544:INFO:Creating metrics dataframe
2024-07-10 14:22:16,545:INFO:Uploading results into container
2024-07-10 14:22:16,545:INFO:Uploading model into container now
2024-07-10 14:22:16,545:INFO:_master_model_container: 14
2024-07-10 14:22:16,545:INFO:_display_container: 2
2024-07-10 14:22:16,545:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:22:16,545:INFO:create_model() successfully completed......................................
2024-07-10 14:22:16,727:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:16,727:INFO:Creating metrics dataframe
2024-07-10 14:22:16,731:INFO:Initializing AdaBoost Regressor
2024-07-10 14:22:16,731:INFO:Total runtime is 0.3605970978736877 minutes
2024-07-10 14:22:16,732:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:16,732:INFO:Initializing create_model()
2024-07-10 14:22:16,732:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:16,732:INFO:Checking exceptions
2024-07-10 14:22:16,732:INFO:Importing libraries
2024-07-10 14:22:16,732:INFO:Copying training dataset
2024-07-10 14:22:16,737:INFO:Defining folds
2024-07-10 14:22:16,737:INFO:Declaring metric variables
2024-07-10 14:22:16,738:INFO:Importing untrained model
2024-07-10 14:22:16,739:INFO:AdaBoost Regressor Imported successfully
2024-07-10 14:22:16,741:INFO:Starting cross validation
2024-07-10 14:22:16,742:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:18,403:INFO:Calculating mean and std
2024-07-10 14:22:18,404:INFO:Creating metrics dataframe
2024-07-10 14:22:18,405:INFO:Uploading results into container
2024-07-10 14:22:18,405:INFO:Uploading model into container now
2024-07-10 14:22:18,405:INFO:_master_model_container: 15
2024-07-10 14:22:18,405:INFO:_display_container: 2
2024-07-10 14:22:18,406:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 14:22:18,406:INFO:create_model() successfully completed......................................
2024-07-10 14:22:18,591:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:18,591:INFO:Creating metrics dataframe
2024-07-10 14:22:18,595:INFO:Initializing Gradient Boosting Regressor
2024-07-10 14:22:18,595:INFO:Total runtime is 0.3916627804438273 minutes
2024-07-10 14:22:18,596:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:18,596:INFO:Initializing create_model()
2024-07-10 14:22:18,596:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:18,596:INFO:Checking exceptions
2024-07-10 14:22:18,596:INFO:Importing libraries
2024-07-10 14:22:18,596:INFO:Copying training dataset
2024-07-10 14:22:18,601:INFO:Defining folds
2024-07-10 14:22:18,601:INFO:Declaring metric variables
2024-07-10 14:22:18,602:INFO:Importing untrained model
2024-07-10 14:22:18,603:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 14:22:18,605:INFO:Starting cross validation
2024-07-10 14:22:18,606:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:20,938:INFO:Calculating mean and std
2024-07-10 14:22:20,938:INFO:Creating metrics dataframe
2024-07-10 14:22:20,939:INFO:Uploading results into container
2024-07-10 14:22:20,939:INFO:Uploading model into container now
2024-07-10 14:22:20,939:INFO:_master_model_container: 16
2024-07-10 14:22:20,939:INFO:_display_container: 2
2024-07-10 14:22:20,940:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 14:22:20,940:INFO:create_model() successfully completed......................................
2024-07-10 14:22:21,121:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:21,122:INFO:Creating metrics dataframe
2024-07-10 14:22:21,126:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 14:22:21,126:INFO:Total runtime is 0.4338460326194763 minutes
2024-07-10 14:22:21,127:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:21,127:INFO:Initializing create_model()
2024-07-10 14:22:21,127:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:21,127:INFO:Checking exceptions
2024-07-10 14:22:21,127:INFO:Importing libraries
2024-07-10 14:22:21,127:INFO:Copying training dataset
2024-07-10 14:22:21,132:INFO:Defining folds
2024-07-10 14:22:21,132:INFO:Declaring metric variables
2024-07-10 14:22:21,133:INFO:Importing untrained model
2024-07-10 14:22:21,134:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 14:22:21,136:INFO:Starting cross validation
2024-07-10 14:22:21,138:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:24,290:INFO:Calculating mean and std
2024-07-10 14:22:24,291:INFO:Creating metrics dataframe
2024-07-10 14:22:24,292:INFO:Uploading results into container
2024-07-10 14:22:24,293:INFO:Uploading model into container now
2024-07-10 14:22:24,293:INFO:_master_model_container: 17
2024-07-10 14:22:24,293:INFO:_display_container: 2
2024-07-10 14:22:24,293:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:22:24,293:INFO:create_model() successfully completed......................................
2024-07-10 14:22:24,474:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:24,474:INFO:Creating metrics dataframe
2024-07-10 14:22:24,478:INFO:Initializing Dummy Regressor
2024-07-10 14:22:24,478:INFO:Total runtime is 0.4897173325220744 minutes
2024-07-10 14:22:24,479:INFO:SubProcess create_model() called ==================================
2024-07-10 14:22:24,479:INFO:Initializing create_model()
2024-07-10 14:22:24,479:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x35f497410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:24,479:INFO:Checking exceptions
2024-07-10 14:22:24,480:INFO:Importing libraries
2024-07-10 14:22:24,480:INFO:Copying training dataset
2024-07-10 14:22:24,484:INFO:Defining folds
2024-07-10 14:22:24,484:INFO:Declaring metric variables
2024-07-10 14:22:24,485:INFO:Importing untrained model
2024-07-10 14:22:24,486:INFO:Dummy Regressor Imported successfully
2024-07-10 14:22:24,488:INFO:Starting cross validation
2024-07-10 14:22:24,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 14:22:25,171:INFO:Calculating mean and std
2024-07-10 14:22:25,172:INFO:Creating metrics dataframe
2024-07-10 14:22:25,173:INFO:Uploading results into container
2024-07-10 14:22:25,173:INFO:Uploading model into container now
2024-07-10 14:22:25,173:INFO:_master_model_container: 18
2024-07-10 14:22:25,173:INFO:_display_container: 2
2024-07-10 14:22:25,173:INFO:DummyRegressor()
2024-07-10 14:22:25,173:INFO:create_model() successfully completed......................................
2024-07-10 14:22:25,357:INFO:SubProcess create_model() end ==================================
2024-07-10 14:22:25,357:INFO:Creating metrics dataframe
2024-07-10 14:22:25,364:INFO:Initializing create_model()
2024-07-10 14:22:25,364:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x363ea5210>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 14:22:25,365:INFO:Checking exceptions
2024-07-10 14:22:25,365:INFO:Importing libraries
2024-07-10 14:22:25,365:INFO:Copying training dataset
2024-07-10 14:22:25,370:INFO:Defining folds
2024-07-10 14:22:25,370:INFO:Declaring metric variables
2024-07-10 14:22:25,370:INFO:Importing untrained model
2024-07-10 14:22:25,370:INFO:Declaring custom model
2024-07-10 14:22:25,370:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 14:22:25,371:INFO:Cross validation set to False
2024-07-10 14:22:25,371:INFO:Fitting Model
2024-07-10 14:22:25,624:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-10 14:22:25,628:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001769 seconds.
2024-07-10 14:22:25,628:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-10 14:22:25,628:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-10 14:22:25,628:INFO:[LightGBM] [Info] Total Bins 967
2024-07-10 14:22:25,628:INFO:[LightGBM] [Info] Number of data points in the train set: 6584, number of used features: 66
2024-07-10 14:22:25,628:INFO:[LightGBM] [Info] Start training from score 3720.932260
2024-07-10 14:22:25,932:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:22:25,932:INFO:create_model() successfully completed......................................
2024-07-10 14:22:26,122:INFO:_master_model_container: 18
2024-07-10 14:22:26,122:INFO:_display_container: 2
2024-07-10 14:22:26,123:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 14:22:26,123:INFO:compare_models() successfully completed......................................
2024-07-10 15:11:48,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:11:48,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:11:48,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:11:48,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:11:55,135:INFO:PyCaret RegressionExperiment
2024-07-10 15:11:55,135:INFO:Logging name: reg-default-name
2024-07-10 15:11:55,135:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:11:55,135:INFO:version 3.3.1
2024-07-10 15:11:55,135:INFO:Initializing setup()
2024-07-10 15:11:55,135:INFO:self.USI: 18d9
2024-07-10 15:11:55,135:INFO:self._variable_keys: {'target_param', 'exp_id', 'n_jobs_param', 'logging_param', '_available_plots', '_ml_usecase', 'gpu_n_jobs_param', 'exp_name_log', 'seed', 'y_test', 'fold_groups_param', 'pipeline', 'X_train', 'gpu_param', 'y', 'X', 'memory', 'data', 'USI', 'fold_generator', 'transform_target_param', 'y_train', 'log_plots_param', 'idx', 'fold_shuffle_param', 'html_param', 'X_test'}
2024-07-10 15:11:55,135:INFO:Checking environment
2024-07-10 15:11:55,135:INFO:python_version: 3.11.8
2024-07-10 15:11:55,135:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:11:55,135:INFO:machine: arm64
2024-07-10 15:11:55,135:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:11:55,135:INFO:Memory: svmem(total=17179869184, available=5445140480, percent=68.3, used=6854459392, free=140738560, active=5318557696, inactive=5261819904, wired=1535901696)
2024-07-10 15:11:55,135:INFO:Physical Core: 8
2024-07-10 15:11:55,135:INFO:Logical Core: 8
2024-07-10 15:11:55,135:INFO:Checking libraries
2024-07-10 15:11:55,135:INFO:System:
2024-07-10 15:11:55,136:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:11:55,136:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:11:55,136:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:11:55,136:INFO:PyCaret required dependencies:
2024-07-10 15:11:55,402:INFO:                 pip: 23.3.1
2024-07-10 15:11:55,402:INFO:          setuptools: 68.2.2
2024-07-10 15:11:55,402:INFO:             pycaret: 3.3.1
2024-07-10 15:11:55,402:INFO:             IPython: 8.20.0
2024-07-10 15:11:55,402:INFO:          ipywidgets: 7.6.5
2024-07-10 15:11:55,402:INFO:                tqdm: 4.65.0
2024-07-10 15:11:55,402:INFO:               numpy: 1.26.4
2024-07-10 15:11:55,402:INFO:              pandas: 2.1.4
2024-07-10 15:11:55,402:INFO:              jinja2: 3.1.3
2024-07-10 15:11:55,402:INFO:               scipy: 1.11.4
2024-07-10 15:11:55,402:INFO:              joblib: 1.2.0
2024-07-10 15:11:55,402:INFO:             sklearn: 1.4.2
2024-07-10 15:11:55,402:INFO:                pyod: 1.1.3
2024-07-10 15:11:55,402:INFO:            imblearn: 0.12.2
2024-07-10 15:11:55,402:INFO:   category_encoders: 2.6.3
2024-07-10 15:11:55,402:INFO:            lightgbm: 4.3.0
2024-07-10 15:11:55,402:INFO:               numba: 0.59.0
2024-07-10 15:11:55,402:INFO:            requests: 2.31.0
2024-07-10 15:11:55,402:INFO:          matplotlib: 3.8.0
2024-07-10 15:11:55,402:INFO:          scikitplot: 0.3.7
2024-07-10 15:11:55,402:INFO:         yellowbrick: 1.5
2024-07-10 15:11:55,402:INFO:              plotly: 5.22.0
2024-07-10 15:11:55,402:INFO:    plotly-resampler: Not installed
2024-07-10 15:11:55,402:INFO:             kaleido: 0.2.1
2024-07-10 15:11:55,402:INFO:           schemdraw: 0.15
2024-07-10 15:11:55,402:INFO:         statsmodels: 0.14.0
2024-07-10 15:11:55,402:INFO:              sktime: 0.26.0
2024-07-10 15:11:55,402:INFO:               tbats: 1.1.3
2024-07-10 15:11:55,402:INFO:            pmdarima: 2.0.4
2024-07-10 15:11:55,402:INFO:              psutil: 5.9.0
2024-07-10 15:11:55,402:INFO:          markupsafe: 2.1.3
2024-07-10 15:11:55,402:INFO:             pickle5: Not installed
2024-07-10 15:11:55,402:INFO:         cloudpickle: 2.2.1
2024-07-10 15:11:55,402:INFO:         deprecation: 2.1.0
2024-07-10 15:11:55,402:INFO:              xxhash: 3.4.1
2024-07-10 15:11:55,402:INFO:           wurlitzer: 3.0.2
2024-07-10 15:11:55,402:INFO:PyCaret optional dependencies:
2024-07-10 15:11:55,407:INFO:                shap: Not installed
2024-07-10 15:11:55,407:INFO:           interpret: Not installed
2024-07-10 15:11:55,407:INFO:                umap: 0.5.5
2024-07-10 15:11:55,407:INFO:     ydata_profiling: Not installed
2024-07-10 15:11:55,407:INFO:  explainerdashboard: Not installed
2024-07-10 15:11:55,407:INFO:             autoviz: Not installed
2024-07-10 15:11:55,407:INFO:           fairlearn: Not installed
2024-07-10 15:11:55,407:INFO:          deepchecks: Not installed
2024-07-10 15:11:55,407:INFO:             xgboost: Not installed
2024-07-10 15:11:55,407:INFO:            catboost: Not installed
2024-07-10 15:11:55,407:INFO:              kmodes: Not installed
2024-07-10 15:11:55,407:INFO:             mlxtend: Not installed
2024-07-10 15:11:55,407:INFO:       statsforecast: Not installed
2024-07-10 15:11:55,407:INFO:        tune_sklearn: Not installed
2024-07-10 15:11:55,407:INFO:                 ray: Not installed
2024-07-10 15:11:55,407:INFO:            hyperopt: Not installed
2024-07-10 15:11:55,407:INFO:              optuna: Not installed
2024-07-10 15:11:55,407:INFO:               skopt: Not installed
2024-07-10 15:11:55,407:INFO:              mlflow: 2.13.0
2024-07-10 15:11:55,407:INFO:              gradio: Not installed
2024-07-10 15:11:55,407:INFO:             fastapi: Not installed
2024-07-10 15:11:55,407:INFO:             uvicorn: Not installed
2024-07-10 15:11:55,407:INFO:              m2cgen: Not installed
2024-07-10 15:11:55,407:INFO:           evidently: Not installed
2024-07-10 15:11:55,407:INFO:               fugue: Not installed
2024-07-10 15:11:55,407:INFO:           streamlit: 1.30.0
2024-07-10 15:11:55,407:INFO:             prophet: Not installed
2024-07-10 15:11:55,407:INFO:None
2024-07-10 15:11:55,407:INFO:Set up data.
2024-07-10 15:11:55,418:INFO:Set up folding strategy.
2024-07-10 15:11:55,418:INFO:Set up train/test split.
2024-07-10 15:11:55,425:INFO:Set up index.
2024-07-10 15:11:55,425:INFO:Assigning column types.
2024-07-10 15:11:55,430:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:11:55,430:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,431:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,433:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,459:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,476:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,476:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,476:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,478:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,480:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,505:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,522:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,522:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,522:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,522:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:11:55,524:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,526:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,551:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,568:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,568:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,568:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,570:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,572:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,597:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,614:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,614:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,614:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,614:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:11:55,618:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,644:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,661:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,661:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,661:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,665:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,690:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,707:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,707:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,707:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,707:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:11:55,736:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,753:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,753:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,753:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,781:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,799:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,799:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,799:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,799:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:11:55,827:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,845:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,845:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,873:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:11:55,891:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,891:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,891:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:11:55,937:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,937:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,984:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,984:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:55,985:INFO:Preparing preprocessing pipeline...
2024-07-10 15:11:55,985:INFO:Set up simple imputation.
2024-07-10 15:11:55,988:INFO:Set up encoding of categorical features.
2024-07-10 15:11:55,988:INFO:Set up removing multicollinearity.
2024-07-10 15:11:55,988:INFO:Set up column transformation.
2024-07-10 15:11:55,988:INFO:Set up feature normalization.
2024-07-10 15:11:56,462:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:11:56,467:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms',
                                             'is_exact_address_hidden',
                                             'is_floor_under', 'rent_price',
                                             'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitte...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:11:56,467:INFO:Creating final display dataframe.
2024-07-10 15:11:56,730:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (20429, 26)
4        Transformed data shape        (20429, 71)
5   Transformed train set shape        (14300, 71)
6    Transformed test set shape         (6129, 71)
7              Numeric features                 20
8          Categorical features                  5
9      Rows with missing values              65.0%
10                   Preprocess               True
11              Imputation type             simple
12           Numeric imputation               mean
13       Categorical imputation               mode
14     Maximum one-hot encoding                 25
15              Encoding method               None
16     Remove multicollinearity               True
17  Multicollinearity threshold                0.9
18               Transformation               True
19        Transformation method        yeo-johnson
20                    Normalize               True
21             Normalize method             zscore
22               Fold Generator              KFold
23                  Fold Number                 10
24                     CPU Jobs                 -1
25                      Use GPU              False
26               Log Experiment              False
27              Experiment Name   reg-default-name
28                          USI               18d9
2024-07-10 15:11:56,780:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:56,780:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:56,826:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:56,826:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:11:56,826:INFO:setup() successfully completed in 1.7s...............
2024-07-10 15:13:54,989:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:13:54,990:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:13:54,990:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:13:54,990:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:13:55,125:INFO:PyCaret RegressionExperiment
2024-07-10 15:13:55,125:INFO:Logging name: reg-default-name
2024-07-10 15:13:55,125:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:13:55,125:INFO:version 3.3.1
2024-07-10 15:13:55,125:INFO:Initializing setup()
2024-07-10 15:13:55,125:INFO:self.USI: 94d2
2024-07-10 15:13:55,126:INFO:self._variable_keys: {'gpu_n_jobs_param', 'data', 'gpu_param', 'target_param', 'n_jobs_param', 'transform_target_param', '_ml_usecase', 'fold_generator', 'exp_id', 'pipeline', 'X_train', '_available_plots', 'USI', 'fold_shuffle_param', 'log_plots_param', 'X_test', 'y_test', 'fold_groups_param', 'X', 'exp_name_log', 'y_train', 'logging_param', 'html_param', 'memory', 'seed', 'idx', 'y'}
2024-07-10 15:13:55,126:INFO:Checking environment
2024-07-10 15:13:55,126:INFO:python_version: 3.11.8
2024-07-10 15:13:55,126:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:13:55,126:INFO:machine: arm64
2024-07-10 15:13:55,126:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:13:55,126:INFO:Memory: svmem(total=17179869184, available=5380505600, percent=68.7, used=6977781760, free=60440576, active=5324455936, inactive=5187190784, wired=1653325824)
2024-07-10 15:13:55,126:INFO:Physical Core: 8
2024-07-10 15:13:55,126:INFO:Logical Core: 8
2024-07-10 15:13:55,126:INFO:Checking libraries
2024-07-10 15:13:55,126:INFO:System:
2024-07-10 15:13:55,126:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:13:55,126:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:13:55,126:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:13:55,126:INFO:PyCaret required dependencies:
2024-07-10 15:13:55,344:INFO:                 pip: 23.3.1
2024-07-10 15:13:55,344:INFO:          setuptools: 68.2.2
2024-07-10 15:13:55,344:INFO:             pycaret: 3.3.1
2024-07-10 15:13:55,344:INFO:             IPython: 8.20.0
2024-07-10 15:13:55,344:INFO:          ipywidgets: 7.6.5
2024-07-10 15:13:55,344:INFO:                tqdm: 4.65.0
2024-07-10 15:13:55,344:INFO:               numpy: 1.26.4
2024-07-10 15:13:55,344:INFO:              pandas: 2.1.4
2024-07-10 15:13:55,344:INFO:              jinja2: 3.1.3
2024-07-10 15:13:55,344:INFO:               scipy: 1.11.4
2024-07-10 15:13:55,345:INFO:              joblib: 1.2.0
2024-07-10 15:13:55,345:INFO:             sklearn: 1.4.2
2024-07-10 15:13:55,345:INFO:                pyod: 1.1.3
2024-07-10 15:13:55,345:INFO:            imblearn: 0.12.2
2024-07-10 15:13:55,345:INFO:   category_encoders: 2.6.3
2024-07-10 15:13:55,345:INFO:            lightgbm: 4.3.0
2024-07-10 15:13:55,345:INFO:               numba: 0.59.0
2024-07-10 15:13:55,345:INFO:            requests: 2.31.0
2024-07-10 15:13:55,345:INFO:          matplotlib: 3.8.0
2024-07-10 15:13:55,345:INFO:          scikitplot: 0.3.7
2024-07-10 15:13:55,345:INFO:         yellowbrick: 1.5
2024-07-10 15:13:55,345:INFO:              plotly: 5.22.0
2024-07-10 15:13:55,345:INFO:    plotly-resampler: Not installed
2024-07-10 15:13:55,345:INFO:             kaleido: 0.2.1
2024-07-10 15:13:55,345:INFO:           schemdraw: 0.15
2024-07-10 15:13:55,345:INFO:         statsmodels: 0.14.0
2024-07-10 15:13:55,345:INFO:              sktime: 0.26.0
2024-07-10 15:13:55,345:INFO:               tbats: 1.1.3
2024-07-10 15:13:55,345:INFO:            pmdarima: 2.0.4
2024-07-10 15:13:55,345:INFO:              psutil: 5.9.0
2024-07-10 15:13:55,345:INFO:          markupsafe: 2.1.3
2024-07-10 15:13:55,345:INFO:             pickle5: Not installed
2024-07-10 15:13:55,345:INFO:         cloudpickle: 2.2.1
2024-07-10 15:13:55,345:INFO:         deprecation: 2.1.0
2024-07-10 15:13:55,345:INFO:              xxhash: 3.4.1
2024-07-10 15:13:55,345:INFO:           wurlitzer: 3.0.2
2024-07-10 15:13:55,345:INFO:PyCaret optional dependencies:
2024-07-10 15:13:55,349:INFO:                shap: Not installed
2024-07-10 15:13:55,349:INFO:           interpret: Not installed
2024-07-10 15:13:55,349:INFO:                umap: 0.5.5
2024-07-10 15:13:55,349:INFO:     ydata_profiling: Not installed
2024-07-10 15:13:55,349:INFO:  explainerdashboard: Not installed
2024-07-10 15:13:55,349:INFO:             autoviz: Not installed
2024-07-10 15:13:55,350:INFO:           fairlearn: Not installed
2024-07-10 15:13:55,350:INFO:          deepchecks: Not installed
2024-07-10 15:13:55,350:INFO:             xgboost: Not installed
2024-07-10 15:13:55,350:INFO:            catboost: Not installed
2024-07-10 15:13:55,350:INFO:              kmodes: Not installed
2024-07-10 15:13:55,350:INFO:             mlxtend: Not installed
2024-07-10 15:13:55,350:INFO:       statsforecast: Not installed
2024-07-10 15:13:55,350:INFO:        tune_sklearn: Not installed
2024-07-10 15:13:55,350:INFO:                 ray: Not installed
2024-07-10 15:13:55,350:INFO:            hyperopt: Not installed
2024-07-10 15:13:55,350:INFO:              optuna: Not installed
2024-07-10 15:13:55,350:INFO:               skopt: Not installed
2024-07-10 15:13:55,350:INFO:              mlflow: 2.13.0
2024-07-10 15:13:55,350:INFO:              gradio: Not installed
2024-07-10 15:13:55,350:INFO:             fastapi: Not installed
2024-07-10 15:13:55,350:INFO:             uvicorn: Not installed
2024-07-10 15:13:55,350:INFO:              m2cgen: Not installed
2024-07-10 15:13:55,350:INFO:           evidently: Not installed
2024-07-10 15:13:55,350:INFO:               fugue: Not installed
2024-07-10 15:13:55,350:INFO:           streamlit: 1.30.0
2024-07-10 15:13:55,350:INFO:             prophet: Not installed
2024-07-10 15:13:55,350:INFO:None
2024-07-10 15:13:55,350:INFO:Set up data.
2024-07-10 15:13:55,359:INFO:Set up folding strategy.
2024-07-10 15:13:55,359:INFO:Set up train/test split.
2024-07-10 15:13:55,365:INFO:Set up index.
2024-07-10 15:13:55,366:INFO:Assigning column types.
2024-07-10 15:13:55,369:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:13:55,369:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,371:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,373:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,399:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,416:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,416:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,416:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,417:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,418:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,420:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,445:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,463:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,463:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,463:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,463:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:13:55,465:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,467:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,492:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,509:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,509:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,510:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,511:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,513:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,539:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,557:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,557:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,557:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:13:55,561:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,586:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,604:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,604:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,604:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,608:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,633:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,650:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,651:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,651:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,651:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:13:55,679:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,697:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,697:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,697:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,727:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,745:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,745:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,745:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,745:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:13:55,775:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,792:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,793:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,822:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:13:55,840:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,840:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,840:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:13:55,886:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,886:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,933:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,934:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:55,935:INFO:Preparing preprocessing pipeline...
2024-07-10 15:13:55,935:INFO:Set up simple imputation.
2024-07-10 15:13:55,938:INFO:Set up encoding of categorical features.
2024-07-10 15:13:55,938:INFO:Set up removing multicollinearity.
2024-07-10 15:13:55,938:INFO:Set up column transformation.
2024-07-10 15:13:55,938:INFO:Set up feature normalization.
2024-07-10 15:13:56,344:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:13:56,349:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms',
                                             'is_exact_address_hidden',
                                             'is_floor_under', 'rent_price',
                                             'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitte...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:13:56,349:INFO:Creating final display dataframe.
2024-07-10 15:13:56,598:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (16343, 26)
4        Transformed data shape        (16343, 70)
5   Transformed train set shape        (11440, 70)
6    Transformed test set shape         (4903, 70)
7              Numeric features                 20
8          Categorical features                  5
9      Rows with missing values              65.0%
10                   Preprocess               True
11              Imputation type             simple
12           Numeric imputation               mean
13       Categorical imputation               mode
14     Maximum one-hot encoding                 25
15              Encoding method               None
16     Remove multicollinearity               True
17  Multicollinearity threshold                0.9
18               Transformation               True
19        Transformation method        yeo-johnson
20                    Normalize               True
21             Normalize method             zscore
22               Fold Generator              KFold
23                  Fold Number                 10
24                     CPU Jobs                 -1
25                      Use GPU              False
26               Log Experiment              False
27              Experiment Name   reg-default-name
28                          USI               94d2
2024-07-10 15:13:56,649:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:56,649:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:56,696:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:56,696:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:13:56,696:INFO:setup() successfully completed in 1.57s...............
2024-07-10 15:13:56,702:INFO:Initializing compare_models()
2024-07-10 15:13:56,702:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x155207110>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 15:13:56,703:INFO:Checking exceptions
2024-07-10 15:13:56,705:INFO:Preparing display monitor
2024-07-10 15:13:56,733:INFO:Initializing Linear Regression
2024-07-10 15:13:56,733:INFO:Total runtime is 4.251797993977864e-06 minutes
2024-07-10 15:13:56,735:INFO:SubProcess create_model() called ==================================
2024-07-10 15:13:56,736:INFO:Initializing create_model()
2024-07-10 15:13:56,736:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:13:56,736:INFO:Checking exceptions
2024-07-10 15:13:56,736:INFO:Importing libraries
2024-07-10 15:13:56,736:INFO:Copying training dataset
2024-07-10 15:13:56,744:INFO:Defining folds
2024-07-10 15:13:56,744:INFO:Declaring metric variables
2024-07-10 15:13:56,745:INFO:Importing untrained model
2024-07-10 15:13:56,747:INFO:Linear Regression Imported successfully
2024-07-10 15:13:56,749:INFO:Starting cross validation
2024-07-10 15:13:56,754:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:00,486:INFO:Calculating mean and std
2024-07-10 15:14:00,488:INFO:Creating metrics dataframe
2024-07-10 15:14:00,489:INFO:Uploading results into container
2024-07-10 15:14:00,490:INFO:Uploading model into container now
2024-07-10 15:14:00,490:INFO:_master_model_container: 1
2024-07-10 15:14:00,490:INFO:_display_container: 2
2024-07-10 15:14:00,490:INFO:LinearRegression(n_jobs=-1)
2024-07-10 15:14:00,491:INFO:create_model() successfully completed......................................
2024-07-10 15:14:00,574:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:00,574:INFO:Creating metrics dataframe
2024-07-10 15:14:00,577:INFO:Initializing Lasso Regression
2024-07-10 15:14:00,577:INFO:Total runtime is 0.0640674869219462 minutes
2024-07-10 15:14:00,578:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:00,579:INFO:Initializing create_model()
2024-07-10 15:14:00,579:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:00,579:INFO:Checking exceptions
2024-07-10 15:14:00,579:INFO:Importing libraries
2024-07-10 15:14:00,579:INFO:Copying training dataset
2024-07-10 15:14:00,586:INFO:Defining folds
2024-07-10 15:14:00,586:INFO:Declaring metric variables
2024-07-10 15:14:00,587:INFO:Importing untrained model
2024-07-10 15:14:00,588:INFO:Lasso Regression Imported successfully
2024-07-10 15:14:00,591:INFO:Starting cross validation
2024-07-10 15:14:00,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:02,319:INFO:Calculating mean and std
2024-07-10 15:14:02,320:INFO:Creating metrics dataframe
2024-07-10 15:14:02,321:INFO:Uploading results into container
2024-07-10 15:14:02,321:INFO:Uploading model into container now
2024-07-10 15:14:02,321:INFO:_master_model_container: 2
2024-07-10 15:14:02,321:INFO:_display_container: 2
2024-07-10 15:14:02,321:INFO:Lasso(random_state=9)
2024-07-10 15:14:02,321:INFO:create_model() successfully completed......................................
2024-07-10 15:14:02,374:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:02,374:INFO:Creating metrics dataframe
2024-07-10 15:14:02,377:INFO:Initializing Ridge Regression
2024-07-10 15:14:02,377:INFO:Total runtime is 0.09406418402989705 minutes
2024-07-10 15:14:02,378:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:02,378:INFO:Initializing create_model()
2024-07-10 15:14:02,378:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:02,378:INFO:Checking exceptions
2024-07-10 15:14:02,379:INFO:Importing libraries
2024-07-10 15:14:02,379:INFO:Copying training dataset
2024-07-10 15:14:02,385:INFO:Defining folds
2024-07-10 15:14:02,385:INFO:Declaring metric variables
2024-07-10 15:14:02,386:INFO:Importing untrained model
2024-07-10 15:14:02,387:INFO:Ridge Regression Imported successfully
2024-07-10 15:14:02,389:INFO:Starting cross validation
2024-07-10 15:14:02,390:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:03,445:INFO:Calculating mean and std
2024-07-10 15:14:03,446:INFO:Creating metrics dataframe
2024-07-10 15:14:03,447:INFO:Uploading results into container
2024-07-10 15:14:03,447:INFO:Uploading model into container now
2024-07-10 15:14:03,448:INFO:_master_model_container: 3
2024-07-10 15:14:03,448:INFO:_display_container: 2
2024-07-10 15:14:03,448:INFO:Ridge(random_state=9)
2024-07-10 15:14:03,448:INFO:create_model() successfully completed......................................
2024-07-10 15:14:03,503:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:03,503:INFO:Creating metrics dataframe
2024-07-10 15:14:03,505:INFO:Initializing Elastic Net
2024-07-10 15:14:03,506:INFO:Total runtime is 0.11287346680959065 minutes
2024-07-10 15:14:03,507:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:03,507:INFO:Initializing create_model()
2024-07-10 15:14:03,507:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:03,507:INFO:Checking exceptions
2024-07-10 15:14:03,507:INFO:Importing libraries
2024-07-10 15:14:03,507:INFO:Copying training dataset
2024-07-10 15:14:03,515:INFO:Defining folds
2024-07-10 15:14:03,515:INFO:Declaring metric variables
2024-07-10 15:14:03,516:INFO:Importing untrained model
2024-07-10 15:14:03,518:INFO:Elastic Net Imported successfully
2024-07-10 15:14:03,520:INFO:Starting cross validation
2024-07-10 15:14:03,521:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:04,712:INFO:Calculating mean and std
2024-07-10 15:14:04,712:INFO:Creating metrics dataframe
2024-07-10 15:14:04,713:INFO:Uploading results into container
2024-07-10 15:14:04,713:INFO:Uploading model into container now
2024-07-10 15:14:04,714:INFO:_master_model_container: 4
2024-07-10 15:14:04,714:INFO:_display_container: 2
2024-07-10 15:14:04,714:INFO:ElasticNet(random_state=9)
2024-07-10 15:14:04,714:INFO:create_model() successfully completed......................................
2024-07-10 15:14:04,768:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:04,768:INFO:Creating metrics dataframe
2024-07-10 15:14:04,770:INFO:Initializing Least Angle Regression
2024-07-10 15:14:04,771:INFO:Total runtime is 0.13395617008209226 minutes
2024-07-10 15:14:04,772:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:04,772:INFO:Initializing create_model()
2024-07-10 15:14:04,772:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:04,772:INFO:Checking exceptions
2024-07-10 15:14:04,772:INFO:Importing libraries
2024-07-10 15:14:04,772:INFO:Copying training dataset
2024-07-10 15:14:04,778:INFO:Defining folds
2024-07-10 15:14:04,778:INFO:Declaring metric variables
2024-07-10 15:14:04,779:INFO:Importing untrained model
2024-07-10 15:14:04,781:INFO:Least Angle Regression Imported successfully
2024-07-10 15:14:04,783:INFO:Starting cross validation
2024-07-10 15:14:04,784:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:05,932:INFO:Calculating mean and std
2024-07-10 15:14:05,932:INFO:Creating metrics dataframe
2024-07-10 15:14:05,933:INFO:Uploading results into container
2024-07-10 15:14:05,933:INFO:Uploading model into container now
2024-07-10 15:14:05,934:INFO:_master_model_container: 5
2024-07-10 15:14:05,934:INFO:_display_container: 2
2024-07-10 15:14:05,934:INFO:Lars(random_state=9)
2024-07-10 15:14:05,934:INFO:create_model() successfully completed......................................
2024-07-10 15:14:05,987:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:05,987:INFO:Creating metrics dataframe
2024-07-10 15:14:05,990:INFO:Initializing Lasso Least Angle Regression
2024-07-10 15:14:05,990:INFO:Total runtime is 0.1542758822441101 minutes
2024-07-10 15:14:05,991:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:05,991:INFO:Initializing create_model()
2024-07-10 15:14:05,991:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:05,991:INFO:Checking exceptions
2024-07-10 15:14:05,992:INFO:Importing libraries
2024-07-10 15:14:05,992:INFO:Copying training dataset
2024-07-10 15:14:05,998:INFO:Defining folds
2024-07-10 15:14:05,998:INFO:Declaring metric variables
2024-07-10 15:14:05,999:INFO:Importing untrained model
2024-07-10 15:14:06,001:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 15:14:06,003:INFO:Starting cross validation
2024-07-10 15:14:06,004:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:07,157:INFO:Calculating mean and std
2024-07-10 15:14:07,158:INFO:Creating metrics dataframe
2024-07-10 15:14:07,158:INFO:Uploading results into container
2024-07-10 15:14:07,159:INFO:Uploading model into container now
2024-07-10 15:14:07,159:INFO:_master_model_container: 6
2024-07-10 15:14:07,159:INFO:_display_container: 2
2024-07-10 15:14:07,159:INFO:LassoLars(random_state=9)
2024-07-10 15:14:07,159:INFO:create_model() successfully completed......................................
2024-07-10 15:14:07,213:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:07,213:INFO:Creating metrics dataframe
2024-07-10 15:14:07,217:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 15:14:07,217:INFO:Total runtime is 0.17472413380940754 minutes
2024-07-10 15:14:07,218:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:07,218:INFO:Initializing create_model()
2024-07-10 15:14:07,218:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:07,218:INFO:Checking exceptions
2024-07-10 15:14:07,218:INFO:Importing libraries
2024-07-10 15:14:07,218:INFO:Copying training dataset
2024-07-10 15:14:07,225:INFO:Defining folds
2024-07-10 15:14:07,225:INFO:Declaring metric variables
2024-07-10 15:14:07,226:INFO:Importing untrained model
2024-07-10 15:14:07,227:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 15:14:07,230:INFO:Starting cross validation
2024-07-10 15:14:07,231:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:08,367:INFO:Calculating mean and std
2024-07-10 15:14:08,368:INFO:Creating metrics dataframe
2024-07-10 15:14:08,369:INFO:Uploading results into container
2024-07-10 15:14:08,369:INFO:Uploading model into container now
2024-07-10 15:14:08,369:INFO:_master_model_container: 7
2024-07-10 15:14:08,369:INFO:_display_container: 2
2024-07-10 15:14:08,369:INFO:OrthogonalMatchingPursuit()
2024-07-10 15:14:08,370:INFO:create_model() successfully completed......................................
2024-07-10 15:14:08,420:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:08,420:INFO:Creating metrics dataframe
2024-07-10 15:14:08,423:INFO:Initializing Bayesian Ridge
2024-07-10 15:14:08,423:INFO:Total runtime is 0.1948329170544942 minutes
2024-07-10 15:14:08,424:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:08,424:INFO:Initializing create_model()
2024-07-10 15:14:08,425:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:08,425:INFO:Checking exceptions
2024-07-10 15:14:08,425:INFO:Importing libraries
2024-07-10 15:14:08,425:INFO:Copying training dataset
2024-07-10 15:14:08,431:INFO:Defining folds
2024-07-10 15:14:08,431:INFO:Declaring metric variables
2024-07-10 15:14:08,432:INFO:Importing untrained model
2024-07-10 15:14:08,433:INFO:Bayesian Ridge Imported successfully
2024-07-10 15:14:08,435:INFO:Starting cross validation
2024-07-10 15:14:08,436:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:09,604:INFO:Calculating mean and std
2024-07-10 15:14:09,605:INFO:Creating metrics dataframe
2024-07-10 15:14:09,605:INFO:Uploading results into container
2024-07-10 15:14:09,606:INFO:Uploading model into container now
2024-07-10 15:14:09,606:INFO:_master_model_container: 8
2024-07-10 15:14:09,606:INFO:_display_container: 2
2024-07-10 15:14:09,606:INFO:BayesianRidge()
2024-07-10 15:14:09,606:INFO:create_model() successfully completed......................................
2024-07-10 15:14:09,657:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:09,657:INFO:Creating metrics dataframe
2024-07-10 15:14:09,660:INFO:Initializing Passive Aggressive Regressor
2024-07-10 15:14:09,660:INFO:Total runtime is 0.21544744968414306 minutes
2024-07-10 15:14:09,661:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:09,661:INFO:Initializing create_model()
2024-07-10 15:14:09,661:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:09,661:INFO:Checking exceptions
2024-07-10 15:14:09,662:INFO:Importing libraries
2024-07-10 15:14:09,662:INFO:Copying training dataset
2024-07-10 15:14:09,668:INFO:Defining folds
2024-07-10 15:14:09,668:INFO:Declaring metric variables
2024-07-10 15:14:09,669:INFO:Importing untrained model
2024-07-10 15:14:09,670:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 15:14:09,672:INFO:Starting cross validation
2024-07-10 15:14:09,673:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:10,876:INFO:Calculating mean and std
2024-07-10 15:14:10,877:INFO:Creating metrics dataframe
2024-07-10 15:14:10,878:INFO:Uploading results into container
2024-07-10 15:14:10,878:INFO:Uploading model into container now
2024-07-10 15:14:10,878:INFO:_master_model_container: 9
2024-07-10 15:14:10,878:INFO:_display_container: 2
2024-07-10 15:14:10,878:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 15:14:10,878:INFO:create_model() successfully completed......................................
2024-07-10 15:14:10,929:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:10,929:INFO:Creating metrics dataframe
2024-07-10 15:14:10,932:INFO:Initializing Huber Regressor
2024-07-10 15:14:10,932:INFO:Total runtime is 0.2366519331932068 minutes
2024-07-10 15:14:10,933:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:10,934:INFO:Initializing create_model()
2024-07-10 15:14:10,934:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:10,934:INFO:Checking exceptions
2024-07-10 15:14:10,934:INFO:Importing libraries
2024-07-10 15:14:10,934:INFO:Copying training dataset
2024-07-10 15:14:10,940:INFO:Defining folds
2024-07-10 15:14:10,940:INFO:Declaring metric variables
2024-07-10 15:14:10,941:INFO:Importing untrained model
2024-07-10 15:14:10,942:INFO:Huber Regressor Imported successfully
2024-07-10 15:14:10,944:INFO:Starting cross validation
2024-07-10 15:14:10,945:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:12,364:INFO:Calculating mean and std
2024-07-10 15:14:12,364:INFO:Creating metrics dataframe
2024-07-10 15:14:12,365:INFO:Uploading results into container
2024-07-10 15:14:12,365:INFO:Uploading model into container now
2024-07-10 15:14:12,365:INFO:_master_model_container: 10
2024-07-10 15:14:12,366:INFO:_display_container: 2
2024-07-10 15:14:12,366:INFO:HuberRegressor()
2024-07-10 15:14:12,366:INFO:create_model() successfully completed......................................
2024-07-10 15:14:12,416:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:12,417:INFO:Creating metrics dataframe
2024-07-10 15:14:12,420:INFO:Initializing K Neighbors Regressor
2024-07-10 15:14:12,420:INFO:Total runtime is 0.2614455501238505 minutes
2024-07-10 15:14:12,421:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:12,421:INFO:Initializing create_model()
2024-07-10 15:14:12,421:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:12,421:INFO:Checking exceptions
2024-07-10 15:14:12,421:INFO:Importing libraries
2024-07-10 15:14:12,422:INFO:Copying training dataset
2024-07-10 15:14:12,427:INFO:Defining folds
2024-07-10 15:14:12,428:INFO:Declaring metric variables
2024-07-10 15:14:12,428:INFO:Importing untrained model
2024-07-10 15:14:12,430:INFO:K Neighbors Regressor Imported successfully
2024-07-10 15:14:12,431:INFO:Starting cross validation
2024-07-10 15:14:12,433:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:13,617:INFO:Calculating mean and std
2024-07-10 15:14:13,618:INFO:Creating metrics dataframe
2024-07-10 15:14:13,619:INFO:Uploading results into container
2024-07-10 15:14:13,619:INFO:Uploading model into container now
2024-07-10 15:14:13,619:INFO:_master_model_container: 11
2024-07-10 15:14:13,619:INFO:_display_container: 2
2024-07-10 15:14:13,620:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 15:14:13,620:INFO:create_model() successfully completed......................................
2024-07-10 15:14:13,672:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:13,672:INFO:Creating metrics dataframe
2024-07-10 15:14:13,676:INFO:Initializing Decision Tree Regressor
2024-07-10 15:14:13,676:INFO:Total runtime is 0.2823739528656006 minutes
2024-07-10 15:14:13,677:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:13,677:INFO:Initializing create_model()
2024-07-10 15:14:13,677:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:13,677:INFO:Checking exceptions
2024-07-10 15:14:13,677:INFO:Importing libraries
2024-07-10 15:14:13,677:INFO:Copying training dataset
2024-07-10 15:14:13,683:INFO:Defining folds
2024-07-10 15:14:13,683:INFO:Declaring metric variables
2024-07-10 15:14:13,684:INFO:Importing untrained model
2024-07-10 15:14:13,685:INFO:Decision Tree Regressor Imported successfully
2024-07-10 15:14:13,688:INFO:Starting cross validation
2024-07-10 15:14:13,689:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:15,008:INFO:Calculating mean and std
2024-07-10 15:14:15,009:INFO:Creating metrics dataframe
2024-07-10 15:14:15,010:INFO:Uploading results into container
2024-07-10 15:14:15,010:INFO:Uploading model into container now
2024-07-10 15:14:15,010:INFO:_master_model_container: 12
2024-07-10 15:14:15,010:INFO:_display_container: 2
2024-07-10 15:14:15,010:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 15:14:15,010:INFO:create_model() successfully completed......................................
2024-07-10 15:14:15,061:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:15,062:INFO:Creating metrics dataframe
2024-07-10 15:14:15,065:INFO:Initializing Random Forest Regressor
2024-07-10 15:14:15,065:INFO:Total runtime is 0.3055320024490356 minutes
2024-07-10 15:14:15,066:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:15,066:INFO:Initializing create_model()
2024-07-10 15:14:15,066:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:15,067:INFO:Checking exceptions
2024-07-10 15:14:15,067:INFO:Importing libraries
2024-07-10 15:14:15,067:INFO:Copying training dataset
2024-07-10 15:14:15,073:INFO:Defining folds
2024-07-10 15:14:15,073:INFO:Declaring metric variables
2024-07-10 15:14:15,074:INFO:Importing untrained model
2024-07-10 15:14:15,075:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:14:15,077:INFO:Starting cross validation
2024-07-10 15:14:15,078:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:24,018:INFO:Calculating mean and std
2024-07-10 15:14:24,019:INFO:Creating metrics dataframe
2024-07-10 15:14:24,020:INFO:Uploading results into container
2024-07-10 15:14:24,020:INFO:Uploading model into container now
2024-07-10 15:14:24,020:INFO:_master_model_container: 13
2024-07-10 15:14:24,021:INFO:_display_container: 2
2024-07-10 15:14:24,021:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:14:24,021:INFO:create_model() successfully completed......................................
2024-07-10 15:14:24,076:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:24,076:INFO:Creating metrics dataframe
2024-07-10 15:14:24,080:INFO:Initializing Extra Trees Regressor
2024-07-10 15:14:24,080:INFO:Total runtime is 0.4557842373847961 minutes
2024-07-10 15:14:24,081:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:24,082:INFO:Initializing create_model()
2024-07-10 15:14:24,082:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:24,082:INFO:Checking exceptions
2024-07-10 15:14:24,082:INFO:Importing libraries
2024-07-10 15:14:24,082:INFO:Copying training dataset
2024-07-10 15:14:24,089:INFO:Defining folds
2024-07-10 15:14:24,089:INFO:Declaring metric variables
2024-07-10 15:14:24,090:INFO:Importing untrained model
2024-07-10 15:14:24,091:INFO:Extra Trees Regressor Imported successfully
2024-07-10 15:14:24,093:INFO:Starting cross validation
2024-07-10 15:14:24,094:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:33,574:INFO:Calculating mean and std
2024-07-10 15:14:33,575:INFO:Creating metrics dataframe
2024-07-10 15:14:33,576:INFO:Uploading results into container
2024-07-10 15:14:33,576:INFO:Uploading model into container now
2024-07-10 15:14:33,577:INFO:_master_model_container: 14
2024-07-10 15:14:33,577:INFO:_display_container: 2
2024-07-10 15:14:33,577:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:14:33,577:INFO:create_model() successfully completed......................................
2024-07-10 15:14:33,632:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:33,632:INFO:Creating metrics dataframe
2024-07-10 15:14:33,636:INFO:Initializing AdaBoost Regressor
2024-07-10 15:14:33,636:INFO:Total runtime is 0.6150426030158996 minutes
2024-07-10 15:14:33,637:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:33,637:INFO:Initializing create_model()
2024-07-10 15:14:33,637:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:33,637:INFO:Checking exceptions
2024-07-10 15:14:33,637:INFO:Importing libraries
2024-07-10 15:14:33,637:INFO:Copying training dataset
2024-07-10 15:14:33,643:INFO:Defining folds
2024-07-10 15:14:33,644:INFO:Declaring metric variables
2024-07-10 15:14:33,644:INFO:Importing untrained model
2024-07-10 15:14:33,646:INFO:AdaBoost Regressor Imported successfully
2024-07-10 15:14:33,648:INFO:Starting cross validation
2024-07-10 15:14:33,649:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:36,633:INFO:Calculating mean and std
2024-07-10 15:14:36,633:INFO:Creating metrics dataframe
2024-07-10 15:14:36,634:INFO:Uploading results into container
2024-07-10 15:14:36,634:INFO:Uploading model into container now
2024-07-10 15:14:36,634:INFO:_master_model_container: 15
2024-07-10 15:14:36,634:INFO:_display_container: 2
2024-07-10 15:14:36,635:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 15:14:36,635:INFO:create_model() successfully completed......................................
2024-07-10 15:14:36,688:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:36,688:INFO:Creating metrics dataframe
2024-07-10 15:14:36,692:INFO:Initializing Gradient Boosting Regressor
2024-07-10 15:14:36,692:INFO:Total runtime is 0.6659786502520243 minutes
2024-07-10 15:14:36,693:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:36,693:INFO:Initializing create_model()
2024-07-10 15:14:36,693:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:36,693:INFO:Checking exceptions
2024-07-10 15:14:36,693:INFO:Importing libraries
2024-07-10 15:14:36,694:INFO:Copying training dataset
2024-07-10 15:14:36,700:INFO:Defining folds
2024-07-10 15:14:36,700:INFO:Declaring metric variables
2024-07-10 15:14:36,701:INFO:Importing untrained model
2024-07-10 15:14:36,702:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 15:14:36,705:INFO:Starting cross validation
2024-07-10 15:14:36,706:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:40,725:INFO:Calculating mean and std
2024-07-10 15:14:40,726:INFO:Creating metrics dataframe
2024-07-10 15:14:40,726:INFO:Uploading results into container
2024-07-10 15:14:40,727:INFO:Uploading model into container now
2024-07-10 15:14:40,727:INFO:_master_model_container: 16
2024-07-10 15:14:40,727:INFO:_display_container: 2
2024-07-10 15:14:40,727:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 15:14:40,727:INFO:create_model() successfully completed......................................
2024-07-10 15:14:40,780:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:40,780:INFO:Creating metrics dataframe
2024-07-10 15:14:40,785:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 15:14:40,785:INFO:Total runtime is 0.7341901183128356 minutes
2024-07-10 15:14:40,786:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:40,786:INFO:Initializing create_model()
2024-07-10 15:14:40,786:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:40,786:INFO:Checking exceptions
2024-07-10 15:14:40,786:INFO:Importing libraries
2024-07-10 15:14:40,786:INFO:Copying training dataset
2024-07-10 15:14:40,792:INFO:Defining folds
2024-07-10 15:14:40,793:INFO:Declaring metric variables
2024-07-10 15:14:40,794:INFO:Importing untrained model
2024-07-10 15:14:40,795:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:14:40,797:INFO:Starting cross validation
2024-07-10 15:14:40,799:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:44,460:INFO:Calculating mean and std
2024-07-10 15:14:44,460:INFO:Creating metrics dataframe
2024-07-10 15:14:44,461:INFO:Uploading results into container
2024-07-10 15:14:44,461:INFO:Uploading model into container now
2024-07-10 15:14:44,462:INFO:_master_model_container: 17
2024-07-10 15:14:44,462:INFO:_display_container: 2
2024-07-10 15:14:44,462:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:14:44,462:INFO:create_model() successfully completed......................................
2024-07-10 15:14:44,513:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:44,513:INFO:Creating metrics dataframe
2024-07-10 15:14:44,517:INFO:Initializing Dummy Regressor
2024-07-10 15:14:44,517:INFO:Total runtime is 0.7963948011398315 minutes
2024-07-10 15:14:44,518:INFO:SubProcess create_model() called ==================================
2024-07-10 15:14:44,518:INFO:Initializing create_model()
2024-07-10 15:14:44,518:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x157528bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:44,518:INFO:Checking exceptions
2024-07-10 15:14:44,518:INFO:Importing libraries
2024-07-10 15:14:44,518:INFO:Copying training dataset
2024-07-10 15:14:44,525:INFO:Defining folds
2024-07-10 15:14:44,525:INFO:Declaring metric variables
2024-07-10 15:14:44,526:INFO:Importing untrained model
2024-07-10 15:14:44,527:INFO:Dummy Regressor Imported successfully
2024-07-10 15:14:44,529:INFO:Starting cross validation
2024-07-10 15:14:44,530:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:14:45,597:INFO:Calculating mean and std
2024-07-10 15:14:45,597:INFO:Creating metrics dataframe
2024-07-10 15:14:45,598:INFO:Uploading results into container
2024-07-10 15:14:45,598:INFO:Uploading model into container now
2024-07-10 15:14:45,598:INFO:_master_model_container: 18
2024-07-10 15:14:45,598:INFO:_display_container: 2
2024-07-10 15:14:45,599:INFO:DummyRegressor()
2024-07-10 15:14:45,599:INFO:create_model() successfully completed......................................
2024-07-10 15:14:45,649:INFO:SubProcess create_model() end ==================================
2024-07-10 15:14:45,649:INFO:Creating metrics dataframe
2024-07-10 15:14:45,656:INFO:Initializing create_model()
2024-07-10 15:14:45,656:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:14:45,656:INFO:Checking exceptions
2024-07-10 15:14:45,657:INFO:Importing libraries
2024-07-10 15:14:45,657:INFO:Copying training dataset
2024-07-10 15:14:45,663:INFO:Defining folds
2024-07-10 15:14:45,663:INFO:Declaring metric variables
2024-07-10 15:14:45,663:INFO:Importing untrained model
2024-07-10 15:14:45,663:INFO:Declaring custom model
2024-07-10 15:14:45,663:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:14:45,664:INFO:Cross validation set to False
2024-07-10 15:14:45,664:INFO:Fitting Model
2024-07-10 15:14:46,051:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-10 15:14:46,056:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002200 seconds.
2024-07-10 15:14:46,056:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-10 15:14:46,056:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-10 15:14:46,056:INFO:[LightGBM] [Info] Total Bins 968
2024-07-10 15:14:46,056:INFO:[LightGBM] [Info] Number of data points in the train set: 11440, number of used features: 67
2024-07-10 15:14:46,056:INFO:[LightGBM] [Info] Start training from score 3747.781469
2024-07-10 15:14:46,369:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:14:46,369:INFO:create_model() successfully completed......................................
2024-07-10 15:14:46,429:INFO:_master_model_container: 18
2024-07-10 15:14:46,430:INFO:_display_container: 2
2024-07-10 15:14:46,430:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:14:46,430:INFO:compare_models() successfully completed......................................
2024-07-10 15:14:46,432:INFO:Initializing predict_model()
2024-07-10 15:14:46,433:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x1572fb880>)
2024-07-10 15:14:46,433:INFO:Checking exceptions
2024-07-10 15:14:46,433:INFO:Preloading libraries
2024-07-10 15:14:46,434:INFO:Set up data.
2024-07-10 15:14:46,438:INFO:Set up index.
2024-07-10 15:21:17,136:INFO:Initializing predict_model()
2024-07-10 15:21:17,137:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x1518f0860>)
2024-07-10 15:21:17,137:INFO:Checking exceptions
2024-07-10 15:21:17,137:INFO:Preloading libraries
2024-07-10 15:21:17,141:INFO:Set up data.
2024-07-10 15:21:17,153:INFO:Set up index.
2024-07-10 15:23:57,055:INFO:Initializing plot_model()
2024-07-10 15:23:57,055:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:23:57,056:INFO:Checking exceptions
2024-07-10 15:23:57,066:INFO:Preloading libraries
2024-07-10 15:23:57,071:INFO:Copying training dataset
2024-07-10 15:23:57,071:INFO:Plot type: feature
2024-07-10 15:23:57,071:WARNING:No coef_ found. Trying feature_importances_
2024-07-10 15:23:57,236:INFO:Visual Rendered Successfully
2024-07-10 15:23:57,445:INFO:plot_model() successfully completed......................................
2024-07-10 15:24:01,152:INFO:Initializing plot_model()
2024-07-10 15:24:01,153:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=residuals, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:24:01,153:INFO:Checking exceptions
2024-07-10 15:24:01,167:INFO:Preloading libraries
2024-07-10 15:24:01,172:INFO:Copying training dataset
2024-07-10 15:24:01,172:INFO:Plot type: residuals
2024-07-10 15:24:01,453:INFO:Fitting Model
2024-07-10 15:24:01,494:INFO:Scoring test/hold-out set
2024-07-10 15:24:01,697:INFO:Visual Rendered Successfully
2024-07-10 15:24:01,865:INFO:plot_model() successfully completed......................................
2024-07-10 15:24:04,080:INFO:Initializing plot_model()
2024-07-10 15:24:04,081:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x155207110>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:24:04,081:INFO:Checking exceptions
2024-07-10 15:24:04,092:INFO:Preloading libraries
2024-07-10 15:24:04,098:INFO:Copying training dataset
2024-07-10 15:24:04,099:INFO:Plot type: error
2024-07-10 15:24:04,366:INFO:Fitting Model
2024-07-10 15:24:04,366:INFO:Scoring test/hold-out set
2024-07-10 15:24:04,468:INFO:Visual Rendered Successfully
2024-07-10 15:24:04,628:INFO:plot_model() successfully completed......................................
2024-07-10 15:26:21,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:26:21,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:26:21,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:26:21,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:26:21,626:INFO:PyCaret RegressionExperiment
2024-07-10 15:26:21,626:INFO:Logging name: reg-default-name
2024-07-10 15:26:21,626:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:26:21,626:INFO:version 3.3.1
2024-07-10 15:26:21,626:INFO:Initializing setup()
2024-07-10 15:26:21,626:INFO:self.USI: d9bd
2024-07-10 15:26:21,626:INFO:self._variable_keys: {'pipeline', 'X_test', '_available_plots', 'logging_param', 'y', 'exp_id', '_ml_usecase', 'idx', 'n_jobs_param', 'html_param', 'memory', 'transform_target_param', 'fold_shuffle_param', 'fold_groups_param', 'seed', 'X', 'USI', 'X_train', 'fold_generator', 'target_param', 'gpu_param', 'gpu_n_jobs_param', 'data', 'log_plots_param', 'y_test', 'exp_name_log', 'y_train'}
2024-07-10 15:26:21,626:INFO:Checking environment
2024-07-10 15:26:21,626:INFO:python_version: 3.11.8
2024-07-10 15:26:21,626:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:26:21,626:INFO:machine: arm64
2024-07-10 15:26:21,626:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:26:21,626:INFO:Memory: svmem(total=17179869184, available=5232050176, percent=69.5, used=6698450944, free=127074304, active=5113331712, inactive=4943527936, wired=1585119232)
2024-07-10 15:26:21,626:INFO:Physical Core: 8
2024-07-10 15:26:21,626:INFO:Logical Core: 8
2024-07-10 15:26:21,626:INFO:Checking libraries
2024-07-10 15:26:21,626:INFO:System:
2024-07-10 15:26:21,626:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:26:21,626:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:26:21,626:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:26:21,626:INFO:PyCaret required dependencies:
2024-07-10 15:26:21,862:INFO:                 pip: 23.3.1
2024-07-10 15:26:21,862:INFO:          setuptools: 68.2.2
2024-07-10 15:26:21,862:INFO:             pycaret: 3.3.1
2024-07-10 15:26:21,862:INFO:             IPython: 8.20.0
2024-07-10 15:26:21,862:INFO:          ipywidgets: 7.6.5
2024-07-10 15:26:21,862:INFO:                tqdm: 4.65.0
2024-07-10 15:26:21,862:INFO:               numpy: 1.26.4
2024-07-10 15:26:21,862:INFO:              pandas: 2.1.4
2024-07-10 15:26:21,862:INFO:              jinja2: 3.1.3
2024-07-10 15:26:21,862:INFO:               scipy: 1.11.4
2024-07-10 15:26:21,862:INFO:              joblib: 1.2.0
2024-07-10 15:26:21,862:INFO:             sklearn: 1.4.2
2024-07-10 15:26:21,862:INFO:                pyod: 1.1.3
2024-07-10 15:26:21,862:INFO:            imblearn: 0.12.2
2024-07-10 15:26:21,862:INFO:   category_encoders: 2.6.3
2024-07-10 15:26:21,862:INFO:            lightgbm: 4.3.0
2024-07-10 15:26:21,862:INFO:               numba: 0.59.0
2024-07-10 15:26:21,862:INFO:            requests: 2.31.0
2024-07-10 15:26:21,862:INFO:          matplotlib: 3.8.0
2024-07-10 15:26:21,862:INFO:          scikitplot: 0.3.7
2024-07-10 15:26:21,862:INFO:         yellowbrick: 1.5
2024-07-10 15:26:21,862:INFO:              plotly: 5.22.0
2024-07-10 15:26:21,862:INFO:    plotly-resampler: Not installed
2024-07-10 15:26:21,862:INFO:             kaleido: 0.2.1
2024-07-10 15:26:21,862:INFO:           schemdraw: 0.15
2024-07-10 15:26:21,862:INFO:         statsmodels: 0.14.0
2024-07-10 15:26:21,862:INFO:              sktime: 0.26.0
2024-07-10 15:26:21,862:INFO:               tbats: 1.1.3
2024-07-10 15:26:21,862:INFO:            pmdarima: 2.0.4
2024-07-10 15:26:21,862:INFO:              psutil: 5.9.0
2024-07-10 15:26:21,862:INFO:          markupsafe: 2.1.3
2024-07-10 15:26:21,862:INFO:             pickle5: Not installed
2024-07-10 15:26:21,862:INFO:         cloudpickle: 2.2.1
2024-07-10 15:26:21,862:INFO:         deprecation: 2.1.0
2024-07-10 15:26:21,862:INFO:              xxhash: 3.4.1
2024-07-10 15:26:21,862:INFO:           wurlitzer: 3.0.2
2024-07-10 15:26:21,862:INFO:PyCaret optional dependencies:
2024-07-10 15:26:21,867:INFO:                shap: Not installed
2024-07-10 15:26:21,867:INFO:           interpret: Not installed
2024-07-10 15:26:21,867:INFO:                umap: 0.5.5
2024-07-10 15:26:21,867:INFO:     ydata_profiling: Not installed
2024-07-10 15:26:21,867:INFO:  explainerdashboard: Not installed
2024-07-10 15:26:21,867:INFO:             autoviz: Not installed
2024-07-10 15:26:21,867:INFO:           fairlearn: Not installed
2024-07-10 15:26:21,867:INFO:          deepchecks: Not installed
2024-07-10 15:26:21,867:INFO:             xgboost: Not installed
2024-07-10 15:26:21,867:INFO:            catboost: Not installed
2024-07-10 15:26:21,867:INFO:              kmodes: Not installed
2024-07-10 15:26:21,867:INFO:             mlxtend: Not installed
2024-07-10 15:26:21,867:INFO:       statsforecast: Not installed
2024-07-10 15:26:21,867:INFO:        tune_sklearn: Not installed
2024-07-10 15:26:21,867:INFO:                 ray: Not installed
2024-07-10 15:26:21,867:INFO:            hyperopt: Not installed
2024-07-10 15:26:21,867:INFO:              optuna: Not installed
2024-07-10 15:26:21,867:INFO:               skopt: Not installed
2024-07-10 15:26:21,867:INFO:              mlflow: 2.13.0
2024-07-10 15:26:21,867:INFO:              gradio: Not installed
2024-07-10 15:26:21,867:INFO:             fastapi: Not installed
2024-07-10 15:26:21,867:INFO:             uvicorn: Not installed
2024-07-10 15:26:21,867:INFO:              m2cgen: Not installed
2024-07-10 15:26:21,867:INFO:           evidently: Not installed
2024-07-10 15:26:21,867:INFO:               fugue: Not installed
2024-07-10 15:26:21,867:INFO:           streamlit: 1.30.0
2024-07-10 15:26:21,867:INFO:             prophet: Not installed
2024-07-10 15:26:21,867:INFO:None
2024-07-10 15:26:21,867:INFO:Set up data.
2024-07-10 15:26:21,876:INFO:Set up folding strategy.
2024-07-10 15:26:21,876:INFO:Set up train/test split.
2024-07-10 15:26:21,882:INFO:Set up index.
2024-07-10 15:26:21,883:INFO:Assigning column types.
2024-07-10 15:26:21,885:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:26:21,885:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,887:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,889:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,914:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,932:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,932:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:21,932:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:21,932:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,934:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,936:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,960:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,978:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,978:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:21,978:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:21,978:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:26:21,980:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:26:21,982:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,006:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,024:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,024:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,026:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,028:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,053:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,071:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,071:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,071:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,071:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:26:22,075:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,100:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,118:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,118:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,118:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,122:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,146:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,164:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,164:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,164:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,164:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:26:22,193:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,211:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,211:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,211:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,239:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,257:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,257:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,257:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,258:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:26:22,286:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,304:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,304:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,332:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:26:22,350:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,351:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:26:22,397:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,397:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,443:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,443:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:22,445:INFO:Preparing preprocessing pipeline...
2024-07-10 15:26:22,445:INFO:Set up simple imputation.
2024-07-10 15:26:22,447:INFO:Set up encoding of categorical features.
2024-07-10 15:26:22,447:INFO:Set up removing multicollinearity.
2024-07-10 15:26:22,447:INFO:Set up column transformation.
2024-07-10 15:26:22,447:INFO:Set up feature normalization.
2024-07-10 15:26:22,765:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:26:22,769:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms', 'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitted_wardrobes', 'has_lift',
                                             'is_exterior', 'built_year'],
                                    tran...
                                                                    'energy_certificate',
                                                                    'district',
                                                                    'house_type'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:26:22,769:INFO:Creating final display dataframe.
2024-07-10 15:26:22,952:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (16343, 26)
4        Transformed data shape        (16343, 62)
5   Transformed train set shape        (11440, 62)
6    Transformed test set shape         (4903, 62)
7               Ignore features                  9
8              Numeric features                 12
9          Categorical features                  4
10     Rows with missing values              65.0%
11                   Preprocess               True
12              Imputation type             simple
13           Numeric imputation               mean
14       Categorical imputation               mode
15     Maximum one-hot encoding                 25
16              Encoding method               None
17     Remove multicollinearity               True
18  Multicollinearity threshold                0.9
19               Transformation               True
20        Transformation method        yeo-johnson
21                    Normalize               True
22             Normalize method             zscore
23               Fold Generator              KFold
24                  Fold Number                 10
25                     CPU Jobs                 -1
26                      Use GPU              False
27               Log Experiment              False
28              Experiment Name   reg-default-name
29                          USI               d9bd
2024-07-10 15:26:23,001:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:23,001:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:23,047:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:23,047:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:26:23,047:INFO:setup() successfully completed in 1.43s...............
2024-07-10 15:26:23,053:INFO:Initializing compare_models()
2024-07-10 15:26:23,053:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 15:26:23,053:INFO:Checking exceptions
2024-07-10 15:26:23,056:INFO:Preparing display monitor
2024-07-10 15:26:23,083:INFO:Initializing Linear Regression
2024-07-10 15:26:23,083:INFO:Total runtime is 5.014737447102865e-06 minutes
2024-07-10 15:26:23,085:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:23,085:INFO:Initializing create_model()
2024-07-10 15:26:23,085:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:23,085:INFO:Checking exceptions
2024-07-10 15:26:23,085:INFO:Importing libraries
2024-07-10 15:26:23,085:INFO:Copying training dataset
2024-07-10 15:26:23,092:INFO:Defining folds
2024-07-10 15:26:23,092:INFO:Declaring metric variables
2024-07-10 15:26:23,093:INFO:Importing untrained model
2024-07-10 15:26:23,095:INFO:Linear Regression Imported successfully
2024-07-10 15:26:23,097:INFO:Starting cross validation
2024-07-10 15:26:23,103:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:26,209:INFO:Calculating mean and std
2024-07-10 15:26:26,210:INFO:Creating metrics dataframe
2024-07-10 15:26:26,212:INFO:Uploading results into container
2024-07-10 15:26:26,212:INFO:Uploading model into container now
2024-07-10 15:26:26,213:INFO:_master_model_container: 1
2024-07-10 15:26:26,213:INFO:_display_container: 2
2024-07-10 15:26:26,213:INFO:LinearRegression(n_jobs=-1)
2024-07-10 15:26:26,213:INFO:create_model() successfully completed......................................
2024-07-10 15:26:26,287:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:26,287:INFO:Creating metrics dataframe
2024-07-10 15:26:26,289:INFO:Initializing Lasso Regression
2024-07-10 15:26:26,290:INFO:Total runtime is 0.05344603459040324 minutes
2024-07-10 15:26:26,291:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:26,291:INFO:Initializing create_model()
2024-07-10 15:26:26,291:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:26,291:INFO:Checking exceptions
2024-07-10 15:26:26,291:INFO:Importing libraries
2024-07-10 15:26:26,291:INFO:Copying training dataset
2024-07-10 15:26:26,296:INFO:Defining folds
2024-07-10 15:26:26,296:INFO:Declaring metric variables
2024-07-10 15:26:26,297:INFO:Importing untrained model
2024-07-10 15:26:26,298:INFO:Lasso Regression Imported successfully
2024-07-10 15:26:26,300:INFO:Starting cross validation
2024-07-10 15:26:26,301:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:28,035:INFO:Calculating mean and std
2024-07-10 15:26:28,035:INFO:Creating metrics dataframe
2024-07-10 15:26:28,036:INFO:Uploading results into container
2024-07-10 15:26:28,036:INFO:Uploading model into container now
2024-07-10 15:26:28,036:INFO:_master_model_container: 2
2024-07-10 15:26:28,037:INFO:_display_container: 2
2024-07-10 15:26:28,037:INFO:Lasso(random_state=9)
2024-07-10 15:26:28,037:INFO:create_model() successfully completed......................................
2024-07-10 15:26:28,087:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:28,087:INFO:Creating metrics dataframe
2024-07-10 15:26:28,090:INFO:Initializing Ridge Regression
2024-07-10 15:26:28,090:INFO:Total runtime is 0.08345131476720175 minutes
2024-07-10 15:26:28,091:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:28,091:INFO:Initializing create_model()
2024-07-10 15:26:28,091:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:28,091:INFO:Checking exceptions
2024-07-10 15:26:28,091:INFO:Importing libraries
2024-07-10 15:26:28,091:INFO:Copying training dataset
2024-07-10 15:26:28,095:INFO:Defining folds
2024-07-10 15:26:28,096:INFO:Declaring metric variables
2024-07-10 15:26:28,096:INFO:Importing untrained model
2024-07-10 15:26:28,098:INFO:Ridge Regression Imported successfully
2024-07-10 15:26:28,100:INFO:Starting cross validation
2024-07-10 15:26:28,101:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:28,919:INFO:Calculating mean and std
2024-07-10 15:26:28,919:INFO:Creating metrics dataframe
2024-07-10 15:26:28,920:INFO:Uploading results into container
2024-07-10 15:26:28,920:INFO:Uploading model into container now
2024-07-10 15:26:28,921:INFO:_master_model_container: 3
2024-07-10 15:26:28,921:INFO:_display_container: 2
2024-07-10 15:26:28,921:INFO:Ridge(random_state=9)
2024-07-10 15:26:28,921:INFO:create_model() successfully completed......................................
2024-07-10 15:26:28,971:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:28,971:INFO:Creating metrics dataframe
2024-07-10 15:26:28,974:INFO:Initializing Elastic Net
2024-07-10 15:26:28,974:INFO:Total runtime is 0.09818746646245322 minutes
2024-07-10 15:26:28,975:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:28,975:INFO:Initializing create_model()
2024-07-10 15:26:28,975:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:28,975:INFO:Checking exceptions
2024-07-10 15:26:28,975:INFO:Importing libraries
2024-07-10 15:26:28,976:INFO:Copying training dataset
2024-07-10 15:26:28,980:INFO:Defining folds
2024-07-10 15:26:28,980:INFO:Declaring metric variables
2024-07-10 15:26:28,981:INFO:Importing untrained model
2024-07-10 15:26:28,982:INFO:Elastic Net Imported successfully
2024-07-10 15:26:28,984:INFO:Starting cross validation
2024-07-10 15:26:28,985:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:29,864:INFO:Calculating mean and std
2024-07-10 15:26:29,864:INFO:Creating metrics dataframe
2024-07-10 15:26:29,865:INFO:Uploading results into container
2024-07-10 15:26:29,865:INFO:Uploading model into container now
2024-07-10 15:26:29,866:INFO:_master_model_container: 4
2024-07-10 15:26:29,866:INFO:_display_container: 2
2024-07-10 15:26:29,866:INFO:ElasticNet(random_state=9)
2024-07-10 15:26:29,866:INFO:create_model() successfully completed......................................
2024-07-10 15:26:29,916:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:29,916:INFO:Creating metrics dataframe
2024-07-10 15:26:29,919:INFO:Initializing Least Angle Regression
2024-07-10 15:26:29,919:INFO:Total runtime is 0.11393803358078004 minutes
2024-07-10 15:26:29,920:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:29,920:INFO:Initializing create_model()
2024-07-10 15:26:29,920:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:29,920:INFO:Checking exceptions
2024-07-10 15:26:29,921:INFO:Importing libraries
2024-07-10 15:26:29,921:INFO:Copying training dataset
2024-07-10 15:26:29,925:INFO:Defining folds
2024-07-10 15:26:29,925:INFO:Declaring metric variables
2024-07-10 15:26:29,926:INFO:Importing untrained model
2024-07-10 15:26:29,927:INFO:Least Angle Regression Imported successfully
2024-07-10 15:26:29,929:INFO:Starting cross validation
2024-07-10 15:26:29,930:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:30,771:INFO:Calculating mean and std
2024-07-10 15:26:30,771:INFO:Creating metrics dataframe
2024-07-10 15:26:30,772:INFO:Uploading results into container
2024-07-10 15:26:30,772:INFO:Uploading model into container now
2024-07-10 15:26:30,773:INFO:_master_model_container: 5
2024-07-10 15:26:30,773:INFO:_display_container: 2
2024-07-10 15:26:30,773:INFO:Lars(random_state=9)
2024-07-10 15:26:30,773:INFO:create_model() successfully completed......................................
2024-07-10 15:26:30,823:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:30,823:INFO:Creating metrics dataframe
2024-07-10 15:26:30,826:INFO:Initializing Lasso Least Angle Regression
2024-07-10 15:26:30,826:INFO:Total runtime is 0.12905896504720055 minutes
2024-07-10 15:26:30,827:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:30,828:INFO:Initializing create_model()
2024-07-10 15:26:30,828:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:30,828:INFO:Checking exceptions
2024-07-10 15:26:30,828:INFO:Importing libraries
2024-07-10 15:26:30,828:INFO:Copying training dataset
2024-07-10 15:26:30,832:INFO:Defining folds
2024-07-10 15:26:30,832:INFO:Declaring metric variables
2024-07-10 15:26:30,833:INFO:Importing untrained model
2024-07-10 15:26:30,834:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 15:26:30,836:INFO:Starting cross validation
2024-07-10 15:26:30,837:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:31,678:INFO:Calculating mean and std
2024-07-10 15:26:31,679:INFO:Creating metrics dataframe
2024-07-10 15:26:31,680:INFO:Uploading results into container
2024-07-10 15:26:31,680:INFO:Uploading model into container now
2024-07-10 15:26:31,680:INFO:_master_model_container: 6
2024-07-10 15:26:31,680:INFO:_display_container: 2
2024-07-10 15:26:31,680:INFO:LassoLars(random_state=9)
2024-07-10 15:26:31,680:INFO:create_model() successfully completed......................................
2024-07-10 15:26:31,731:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:31,731:INFO:Creating metrics dataframe
2024-07-10 15:26:31,734:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 15:26:31,734:INFO:Total runtime is 0.14419106642405194 minutes
2024-07-10 15:26:31,735:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:31,736:INFO:Initializing create_model()
2024-07-10 15:26:31,736:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:31,736:INFO:Checking exceptions
2024-07-10 15:26:31,736:INFO:Importing libraries
2024-07-10 15:26:31,736:INFO:Copying training dataset
2024-07-10 15:26:31,740:INFO:Defining folds
2024-07-10 15:26:31,740:INFO:Declaring metric variables
2024-07-10 15:26:31,741:INFO:Importing untrained model
2024-07-10 15:26:31,742:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 15:26:31,744:INFO:Starting cross validation
2024-07-10 15:26:31,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:32,578:INFO:Calculating mean and std
2024-07-10 15:26:32,578:INFO:Creating metrics dataframe
2024-07-10 15:26:32,579:INFO:Uploading results into container
2024-07-10 15:26:32,579:INFO:Uploading model into container now
2024-07-10 15:26:32,579:INFO:_master_model_container: 7
2024-07-10 15:26:32,579:INFO:_display_container: 2
2024-07-10 15:26:32,580:INFO:OrthogonalMatchingPursuit()
2024-07-10 15:26:32,580:INFO:create_model() successfully completed......................................
2024-07-10 15:26:32,630:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:32,630:INFO:Creating metrics dataframe
2024-07-10 15:26:32,633:INFO:Initializing Bayesian Ridge
2024-07-10 15:26:32,633:INFO:Total runtime is 0.15916818380355838 minutes
2024-07-10 15:26:32,634:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:32,634:INFO:Initializing create_model()
2024-07-10 15:26:32,634:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:32,634:INFO:Checking exceptions
2024-07-10 15:26:32,634:INFO:Importing libraries
2024-07-10 15:26:32,635:INFO:Copying training dataset
2024-07-10 15:26:32,639:INFO:Defining folds
2024-07-10 15:26:32,639:INFO:Declaring metric variables
2024-07-10 15:26:32,640:INFO:Importing untrained model
2024-07-10 15:26:32,641:INFO:Bayesian Ridge Imported successfully
2024-07-10 15:26:32,642:INFO:Starting cross validation
2024-07-10 15:26:32,644:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:33,559:INFO:Calculating mean and std
2024-07-10 15:26:33,560:INFO:Creating metrics dataframe
2024-07-10 15:26:33,560:INFO:Uploading results into container
2024-07-10 15:26:33,561:INFO:Uploading model into container now
2024-07-10 15:26:33,561:INFO:_master_model_container: 8
2024-07-10 15:26:33,561:INFO:_display_container: 2
2024-07-10 15:26:33,561:INFO:BayesianRidge()
2024-07-10 15:26:33,561:INFO:create_model() successfully completed......................................
2024-07-10 15:26:33,613:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:33,613:INFO:Creating metrics dataframe
2024-07-10 15:26:33,617:INFO:Initializing Passive Aggressive Regressor
2024-07-10 15:26:33,617:INFO:Total runtime is 0.1755653500556946 minutes
2024-07-10 15:26:33,618:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:33,618:INFO:Initializing create_model()
2024-07-10 15:26:33,618:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:33,618:INFO:Checking exceptions
2024-07-10 15:26:33,618:INFO:Importing libraries
2024-07-10 15:26:33,618:INFO:Copying training dataset
2024-07-10 15:26:33,622:INFO:Defining folds
2024-07-10 15:26:33,622:INFO:Declaring metric variables
2024-07-10 15:26:33,623:INFO:Importing untrained model
2024-07-10 15:26:33,624:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 15:26:33,626:INFO:Starting cross validation
2024-07-10 15:26:33,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:34,504:INFO:Calculating mean and std
2024-07-10 15:26:34,504:INFO:Creating metrics dataframe
2024-07-10 15:26:34,506:INFO:Uploading results into container
2024-07-10 15:26:34,506:INFO:Uploading model into container now
2024-07-10 15:26:34,506:INFO:_master_model_container: 9
2024-07-10 15:26:34,506:INFO:_display_container: 2
2024-07-10 15:26:34,506:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 15:26:34,506:INFO:create_model() successfully completed......................................
2024-07-10 15:26:34,558:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:34,558:INFO:Creating metrics dataframe
2024-07-10 15:26:34,561:INFO:Initializing Huber Regressor
2024-07-10 15:26:34,561:INFO:Total runtime is 0.19130275249481202 minutes
2024-07-10 15:26:34,562:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:34,562:INFO:Initializing create_model()
2024-07-10 15:26:34,562:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:34,562:INFO:Checking exceptions
2024-07-10 15:26:34,562:INFO:Importing libraries
2024-07-10 15:26:34,562:INFO:Copying training dataset
2024-07-10 15:26:34,567:INFO:Defining folds
2024-07-10 15:26:34,567:INFO:Declaring metric variables
2024-07-10 15:26:34,568:INFO:Importing untrained model
2024-07-10 15:26:34,569:INFO:Huber Regressor Imported successfully
2024-07-10 15:26:34,571:INFO:Starting cross validation
2024-07-10 15:26:34,572:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:35,682:INFO:Calculating mean and std
2024-07-10 15:26:35,683:INFO:Creating metrics dataframe
2024-07-10 15:26:35,683:INFO:Uploading results into container
2024-07-10 15:26:35,684:INFO:Uploading model into container now
2024-07-10 15:26:35,684:INFO:_master_model_container: 10
2024-07-10 15:26:35,684:INFO:_display_container: 2
2024-07-10 15:26:35,684:INFO:HuberRegressor()
2024-07-10 15:26:35,684:INFO:create_model() successfully completed......................................
2024-07-10 15:26:35,735:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:35,735:INFO:Creating metrics dataframe
2024-07-10 15:26:35,739:INFO:Initializing K Neighbors Regressor
2024-07-10 15:26:35,739:INFO:Total runtime is 0.2109346310297648 minutes
2024-07-10 15:26:35,740:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:35,740:INFO:Initializing create_model()
2024-07-10 15:26:35,740:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:35,740:INFO:Checking exceptions
2024-07-10 15:26:35,740:INFO:Importing libraries
2024-07-10 15:26:35,740:INFO:Copying training dataset
2024-07-10 15:26:35,744:INFO:Defining folds
2024-07-10 15:26:35,744:INFO:Declaring metric variables
2024-07-10 15:26:35,745:INFO:Importing untrained model
2024-07-10 15:26:35,746:INFO:K Neighbors Regressor Imported successfully
2024-07-10 15:26:35,748:INFO:Starting cross validation
2024-07-10 15:26:35,749:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:36,715:INFO:Calculating mean and std
2024-07-10 15:26:36,715:INFO:Creating metrics dataframe
2024-07-10 15:26:36,716:INFO:Uploading results into container
2024-07-10 15:26:36,716:INFO:Uploading model into container now
2024-07-10 15:26:36,716:INFO:_master_model_container: 11
2024-07-10 15:26:36,716:INFO:_display_container: 2
2024-07-10 15:26:36,716:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 15:26:36,716:INFO:create_model() successfully completed......................................
2024-07-10 15:26:36,768:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:36,768:INFO:Creating metrics dataframe
2024-07-10 15:26:36,771:INFO:Initializing Decision Tree Regressor
2024-07-10 15:26:36,771:INFO:Total runtime is 0.2281400005022685 minutes
2024-07-10 15:26:36,772:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:36,772:INFO:Initializing create_model()
2024-07-10 15:26:36,772:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:36,772:INFO:Checking exceptions
2024-07-10 15:26:36,773:INFO:Importing libraries
2024-07-10 15:26:36,773:INFO:Copying training dataset
2024-07-10 15:26:36,777:INFO:Defining folds
2024-07-10 15:26:36,777:INFO:Declaring metric variables
2024-07-10 15:26:36,778:INFO:Importing untrained model
2024-07-10 15:26:36,779:INFO:Decision Tree Regressor Imported successfully
2024-07-10 15:26:36,781:INFO:Starting cross validation
2024-07-10 15:26:36,782:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:37,802:INFO:Calculating mean and std
2024-07-10 15:26:37,803:INFO:Creating metrics dataframe
2024-07-10 15:26:37,804:INFO:Uploading results into container
2024-07-10 15:26:37,804:INFO:Uploading model into container now
2024-07-10 15:26:37,804:INFO:_master_model_container: 12
2024-07-10 15:26:37,804:INFO:_display_container: 2
2024-07-10 15:26:37,804:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 15:26:37,804:INFO:create_model() successfully completed......................................
2024-07-10 15:26:37,867:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:37,867:INFO:Creating metrics dataframe
2024-07-10 15:26:37,871:INFO:Initializing Random Forest Regressor
2024-07-10 15:26:37,871:INFO:Total runtime is 0.24647083282470705 minutes
2024-07-10 15:26:37,872:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:37,872:INFO:Initializing create_model()
2024-07-10 15:26:37,872:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:37,872:INFO:Checking exceptions
2024-07-10 15:26:37,872:INFO:Importing libraries
2024-07-10 15:26:37,872:INFO:Copying training dataset
2024-07-10 15:26:37,877:INFO:Defining folds
2024-07-10 15:26:37,877:INFO:Declaring metric variables
2024-07-10 15:26:37,878:INFO:Importing untrained model
2024-07-10 15:26:37,880:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:26:37,882:INFO:Starting cross validation
2024-07-10 15:26:37,883:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:45,743:INFO:Calculating mean and std
2024-07-10 15:26:45,744:INFO:Creating metrics dataframe
2024-07-10 15:26:45,745:INFO:Uploading results into container
2024-07-10 15:26:45,745:INFO:Uploading model into container now
2024-07-10 15:26:45,746:INFO:_master_model_container: 13
2024-07-10 15:26:45,746:INFO:_display_container: 2
2024-07-10 15:26:45,746:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:26:45,746:INFO:create_model() successfully completed......................................
2024-07-10 15:26:45,803:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:45,803:INFO:Creating metrics dataframe
2024-07-10 15:26:45,806:INFO:Initializing Extra Trees Regressor
2024-07-10 15:26:45,807:INFO:Total runtime is 0.3787302335103353 minutes
2024-07-10 15:26:45,808:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:45,808:INFO:Initializing create_model()
2024-07-10 15:26:45,808:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:45,808:INFO:Checking exceptions
2024-07-10 15:26:45,808:INFO:Importing libraries
2024-07-10 15:26:45,808:INFO:Copying training dataset
2024-07-10 15:26:45,813:INFO:Defining folds
2024-07-10 15:26:45,813:INFO:Declaring metric variables
2024-07-10 15:26:45,814:INFO:Importing untrained model
2024-07-10 15:26:45,815:INFO:Extra Trees Regressor Imported successfully
2024-07-10 15:26:45,817:INFO:Starting cross validation
2024-07-10 15:26:45,818:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:53,841:INFO:Calculating mean and std
2024-07-10 15:26:53,842:INFO:Creating metrics dataframe
2024-07-10 15:26:53,843:INFO:Uploading results into container
2024-07-10 15:26:53,843:INFO:Uploading model into container now
2024-07-10 15:26:53,843:INFO:_master_model_container: 14
2024-07-10 15:26:53,844:INFO:_display_container: 2
2024-07-10 15:26:53,844:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:26:53,844:INFO:create_model() successfully completed......................................
2024-07-10 15:26:53,896:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:53,896:INFO:Creating metrics dataframe
2024-07-10 15:26:53,899:INFO:Initializing AdaBoost Regressor
2024-07-10 15:26:53,899:INFO:Total runtime is 0.5136098663012187 minutes
2024-07-10 15:26:53,901:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:53,901:INFO:Initializing create_model()
2024-07-10 15:26:53,901:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:53,901:INFO:Checking exceptions
2024-07-10 15:26:53,901:INFO:Importing libraries
2024-07-10 15:26:53,901:INFO:Copying training dataset
2024-07-10 15:26:53,905:INFO:Defining folds
2024-07-10 15:26:53,905:INFO:Declaring metric variables
2024-07-10 15:26:53,906:INFO:Importing untrained model
2024-07-10 15:26:53,907:INFO:AdaBoost Regressor Imported successfully
2024-07-10 15:26:53,909:INFO:Starting cross validation
2024-07-10 15:26:53,910:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:55,592:INFO:Calculating mean and std
2024-07-10 15:26:55,593:INFO:Creating metrics dataframe
2024-07-10 15:26:55,594:INFO:Uploading results into container
2024-07-10 15:26:55,594:INFO:Uploading model into container now
2024-07-10 15:26:55,594:INFO:_master_model_container: 15
2024-07-10 15:26:55,594:INFO:_display_container: 2
2024-07-10 15:26:55,594:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 15:26:55,594:INFO:create_model() successfully completed......................................
2024-07-10 15:26:55,648:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:55,648:INFO:Creating metrics dataframe
2024-07-10 15:26:55,652:INFO:Initializing Gradient Boosting Regressor
2024-07-10 15:26:55,652:INFO:Total runtime is 0.5428218960762025 minutes
2024-07-10 15:26:55,653:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:55,653:INFO:Initializing create_model()
2024-07-10 15:26:55,653:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:55,653:INFO:Checking exceptions
2024-07-10 15:26:55,654:INFO:Importing libraries
2024-07-10 15:26:55,654:INFO:Copying training dataset
2024-07-10 15:26:55,658:INFO:Defining folds
2024-07-10 15:26:55,658:INFO:Declaring metric variables
2024-07-10 15:26:55,659:INFO:Importing untrained model
2024-07-10 15:26:55,660:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 15:26:55,662:INFO:Starting cross validation
2024-07-10 15:26:55,663:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:26:58,377:INFO:Calculating mean and std
2024-07-10 15:26:58,377:INFO:Creating metrics dataframe
2024-07-10 15:26:58,378:INFO:Uploading results into container
2024-07-10 15:26:58,378:INFO:Uploading model into container now
2024-07-10 15:26:58,378:INFO:_master_model_container: 16
2024-07-10 15:26:58,378:INFO:_display_container: 2
2024-07-10 15:26:58,379:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 15:26:58,379:INFO:create_model() successfully completed......................................
2024-07-10 15:26:58,431:INFO:SubProcess create_model() end ==================================
2024-07-10 15:26:58,431:INFO:Creating metrics dataframe
2024-07-10 15:26:58,434:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 15:26:58,435:INFO:Total runtime is 0.5891966342926026 minutes
2024-07-10 15:26:58,436:INFO:SubProcess create_model() called ==================================
2024-07-10 15:26:58,436:INFO:Initializing create_model()
2024-07-10 15:26:58,436:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:26:58,436:INFO:Checking exceptions
2024-07-10 15:26:58,436:INFO:Importing libraries
2024-07-10 15:26:58,436:INFO:Copying training dataset
2024-07-10 15:26:58,440:INFO:Defining folds
2024-07-10 15:26:58,440:INFO:Declaring metric variables
2024-07-10 15:26:58,441:INFO:Importing untrained model
2024-07-10 15:26:58,442:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:26:58,444:INFO:Starting cross validation
2024-07-10 15:26:58,445:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:27:01,425:INFO:Calculating mean and std
2024-07-10 15:27:01,425:INFO:Creating metrics dataframe
2024-07-10 15:27:01,426:INFO:Uploading results into container
2024-07-10 15:27:01,426:INFO:Uploading model into container now
2024-07-10 15:27:01,426:INFO:_master_model_container: 17
2024-07-10 15:27:01,426:INFO:_display_container: 2
2024-07-10 15:27:01,427:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:27:01,427:INFO:create_model() successfully completed......................................
2024-07-10 15:27:01,477:INFO:SubProcess create_model() end ==================================
2024-07-10 15:27:01,477:INFO:Creating metrics dataframe
2024-07-10 15:27:01,481:INFO:Initializing Dummy Regressor
2024-07-10 15:27:01,481:INFO:Total runtime is 0.6399745663007101 minutes
2024-07-10 15:27:01,482:INFO:SubProcess create_model() called ==================================
2024-07-10 15:27:01,483:INFO:Initializing create_model()
2024-07-10 15:27:01,483:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x12cc7e450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:27:01,483:INFO:Checking exceptions
2024-07-10 15:27:01,483:INFO:Importing libraries
2024-07-10 15:27:01,483:INFO:Copying training dataset
2024-07-10 15:27:01,487:INFO:Defining folds
2024-07-10 15:27:01,487:INFO:Declaring metric variables
2024-07-10 15:27:01,488:INFO:Importing untrained model
2024-07-10 15:27:01,489:INFO:Dummy Regressor Imported successfully
2024-07-10 15:27:01,491:INFO:Starting cross validation
2024-07-10 15:27:01,492:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:27:02,343:INFO:Calculating mean and std
2024-07-10 15:27:02,343:INFO:Creating metrics dataframe
2024-07-10 15:27:02,344:INFO:Uploading results into container
2024-07-10 15:27:02,344:INFO:Uploading model into container now
2024-07-10 15:27:02,344:INFO:_master_model_container: 18
2024-07-10 15:27:02,344:INFO:_display_container: 2
2024-07-10 15:27:02,345:INFO:DummyRegressor()
2024-07-10 15:27:02,345:INFO:create_model() successfully completed......................................
2024-07-10 15:27:02,395:INFO:SubProcess create_model() end ==================================
2024-07-10 15:27:02,395:INFO:Creating metrics dataframe
2024-07-10 15:27:02,402:INFO:Initializing create_model()
2024-07-10 15:27:02,402:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:27:02,402:INFO:Checking exceptions
2024-07-10 15:27:02,403:INFO:Importing libraries
2024-07-10 15:27:02,403:INFO:Copying training dataset
2024-07-10 15:27:02,407:INFO:Defining folds
2024-07-10 15:27:02,407:INFO:Declaring metric variables
2024-07-10 15:27:02,407:INFO:Importing untrained model
2024-07-10 15:27:02,407:INFO:Declaring custom model
2024-07-10 15:27:02,407:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:27:02,408:INFO:Cross validation set to False
2024-07-10 15:27:02,408:INFO:Fitting Model
2024-07-10 15:27:03,418:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:27:03,418:INFO:create_model() successfully completed......................................
2024-07-10 15:27:03,477:INFO:_master_model_container: 18
2024-07-10 15:27:03,477:INFO:_display_container: 2
2024-07-10 15:27:03,478:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:27:03,478:INFO:compare_models() successfully completed......................................
2024-07-10 15:27:03,480:INFO:Initializing predict_model()
2024-07-10 15:27:03,480:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x129b332e0>)
2024-07-10 15:27:03,480:INFO:Checking exceptions
2024-07-10 15:27:03,480:INFO:Preloading libraries
2024-07-10 15:27:03,481:INFO:Set up data.
2024-07-10 15:27:03,486:INFO:Set up index.
2024-07-10 15:27:03,617:INFO:Initializing plot_model()
2024-07-10 15:27:03,617:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), plot=residuals, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:27:03,617:INFO:Checking exceptions
2024-07-10 15:27:03,626:INFO:Preloading libraries
2024-07-10 15:27:03,670:INFO:Copying training dataset
2024-07-10 15:27:03,670:INFO:Plot type: residuals
2024-07-10 15:27:03,880:INFO:Fitting Model
2024-07-10 15:27:03,958:INFO:Scoring test/hold-out set
2024-07-10 15:27:04,288:INFO:Visual Rendered Successfully
2024-07-10 15:27:04,343:INFO:plot_model() successfully completed......................................
2024-07-10 15:27:04,350:INFO:Initializing plot_model()
2024-07-10 15:27:04,351:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:27:04,351:INFO:Checking exceptions
2024-07-10 15:27:04,359:INFO:Preloading libraries
2024-07-10 15:27:04,405:INFO:Copying training dataset
2024-07-10 15:27:04,405:INFO:Plot type: error
2024-07-10 15:27:04,590:INFO:Fitting Model
2024-07-10 15:27:04,590:INFO:Scoring test/hold-out set
2024-07-10 15:27:04,723:INFO:Visual Rendered Successfully
2024-07-10 15:27:04,779:INFO:plot_model() successfully completed......................................
2024-07-10 15:27:04,793:INFO:Initializing plot_model()
2024-07-10 15:27:04,793:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x12797b510>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:27:04,793:INFO:Checking exceptions
2024-07-10 15:27:04,803:INFO:Preloading libraries
2024-07-10 15:27:04,841:INFO:Copying training dataset
2024-07-10 15:27:04,842:INFO:Plot type: feature
2024-07-10 15:27:04,842:WARNING:No coef_ found. Trying feature_importances_
2024-07-10 15:27:04,959:INFO:Visual Rendered Successfully
2024-07-10 15:27:05,014:INFO:plot_model() successfully completed......................................
2024-07-10 15:28:44,221:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:28:44,221:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:28:44,222:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:28:44,222:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:28:44,371:INFO:PyCaret RegressionExperiment
2024-07-10 15:28:44,371:INFO:Logging name: reg-default-name
2024-07-10 15:28:44,371:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:28:44,371:INFO:version 3.3.1
2024-07-10 15:28:44,371:INFO:Initializing setup()
2024-07-10 15:28:44,371:INFO:self.USI: 0cce
2024-07-10 15:28:44,371:INFO:self._variable_keys: {'idx', 'y_train', 'log_plots_param', 'seed', 'logging_param', 'gpu_param', 'exp_id', 'fold_groups_param', 'n_jobs_param', 'pipeline', 'y', 'transform_target_param', 'fold_generator', '_available_plots', 'y_test', 'X', 'X_train', 'fold_shuffle_param', 'memory', 'USI', 'gpu_n_jobs_param', 'target_param', 'exp_name_log', 'html_param', 'X_test', 'data', '_ml_usecase'}
2024-07-10 15:28:44,371:INFO:Checking environment
2024-07-10 15:28:44,371:INFO:python_version: 3.11.8
2024-07-10 15:28:44,371:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:28:44,371:INFO:machine: arm64
2024-07-10 15:28:44,371:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:28:44,371:INFO:Memory: svmem(total=17179869184, available=6117785600, percent=64.4, used=4886249472, free=2235990016, active=3290349568, inactive=3762307072, wired=1595899904)
2024-07-10 15:28:44,371:INFO:Physical Core: 8
2024-07-10 15:28:44,371:INFO:Logical Core: 8
2024-07-10 15:28:44,371:INFO:Checking libraries
2024-07-10 15:28:44,371:INFO:System:
2024-07-10 15:28:44,372:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:28:44,372:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:28:44,372:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:28:44,372:INFO:PyCaret required dependencies:
2024-07-10 15:28:44,616:INFO:                 pip: 23.3.1
2024-07-10 15:28:44,616:INFO:          setuptools: 68.2.2
2024-07-10 15:28:44,616:INFO:             pycaret: 3.3.1
2024-07-10 15:28:44,616:INFO:             IPython: 8.20.0
2024-07-10 15:28:44,616:INFO:          ipywidgets: 7.6.5
2024-07-10 15:28:44,616:INFO:                tqdm: 4.65.0
2024-07-10 15:28:44,616:INFO:               numpy: 1.26.4
2024-07-10 15:28:44,616:INFO:              pandas: 2.1.4
2024-07-10 15:28:44,616:INFO:              jinja2: 3.1.3
2024-07-10 15:28:44,616:INFO:               scipy: 1.11.4
2024-07-10 15:28:44,616:INFO:              joblib: 1.2.0
2024-07-10 15:28:44,616:INFO:             sklearn: 1.4.2
2024-07-10 15:28:44,616:INFO:                pyod: 1.1.3
2024-07-10 15:28:44,616:INFO:            imblearn: 0.12.2
2024-07-10 15:28:44,616:INFO:   category_encoders: 2.6.3
2024-07-10 15:28:44,616:INFO:            lightgbm: 4.3.0
2024-07-10 15:28:44,616:INFO:               numba: 0.59.0
2024-07-10 15:28:44,616:INFO:            requests: 2.31.0
2024-07-10 15:28:44,616:INFO:          matplotlib: 3.8.0
2024-07-10 15:28:44,616:INFO:          scikitplot: 0.3.7
2024-07-10 15:28:44,616:INFO:         yellowbrick: 1.5
2024-07-10 15:28:44,616:INFO:              plotly: 5.22.0
2024-07-10 15:28:44,616:INFO:    plotly-resampler: Not installed
2024-07-10 15:28:44,616:INFO:             kaleido: 0.2.1
2024-07-10 15:28:44,616:INFO:           schemdraw: 0.15
2024-07-10 15:28:44,616:INFO:         statsmodels: 0.14.0
2024-07-10 15:28:44,616:INFO:              sktime: 0.26.0
2024-07-10 15:28:44,616:INFO:               tbats: 1.1.3
2024-07-10 15:28:44,616:INFO:            pmdarima: 2.0.4
2024-07-10 15:28:44,616:INFO:              psutil: 5.9.0
2024-07-10 15:28:44,616:INFO:          markupsafe: 2.1.3
2024-07-10 15:28:44,616:INFO:             pickle5: Not installed
2024-07-10 15:28:44,616:INFO:         cloudpickle: 2.2.1
2024-07-10 15:28:44,616:INFO:         deprecation: 2.1.0
2024-07-10 15:28:44,616:INFO:              xxhash: 3.4.1
2024-07-10 15:28:44,616:INFO:           wurlitzer: 3.0.2
2024-07-10 15:28:44,616:INFO:PyCaret optional dependencies:
2024-07-10 15:28:44,621:INFO:                shap: Not installed
2024-07-10 15:28:44,621:INFO:           interpret: Not installed
2024-07-10 15:28:44,621:INFO:                umap: 0.5.5
2024-07-10 15:28:44,621:INFO:     ydata_profiling: Not installed
2024-07-10 15:28:44,621:INFO:  explainerdashboard: Not installed
2024-07-10 15:28:44,621:INFO:             autoviz: Not installed
2024-07-10 15:28:44,621:INFO:           fairlearn: Not installed
2024-07-10 15:28:44,621:INFO:          deepchecks: Not installed
2024-07-10 15:28:44,621:INFO:             xgboost: Not installed
2024-07-10 15:28:44,621:INFO:            catboost: Not installed
2024-07-10 15:28:44,621:INFO:              kmodes: Not installed
2024-07-10 15:28:44,621:INFO:             mlxtend: Not installed
2024-07-10 15:28:44,621:INFO:       statsforecast: Not installed
2024-07-10 15:28:44,621:INFO:        tune_sklearn: Not installed
2024-07-10 15:28:44,621:INFO:                 ray: Not installed
2024-07-10 15:28:44,621:INFO:            hyperopt: Not installed
2024-07-10 15:28:44,621:INFO:              optuna: Not installed
2024-07-10 15:28:44,621:INFO:               skopt: Not installed
2024-07-10 15:28:44,621:INFO:              mlflow: 2.13.0
2024-07-10 15:28:44,621:INFO:              gradio: Not installed
2024-07-10 15:28:44,621:INFO:             fastapi: Not installed
2024-07-10 15:28:44,621:INFO:             uvicorn: Not installed
2024-07-10 15:28:44,621:INFO:              m2cgen: Not installed
2024-07-10 15:28:44,621:INFO:           evidently: Not installed
2024-07-10 15:28:44,622:INFO:               fugue: Not installed
2024-07-10 15:28:44,622:INFO:           streamlit: 1.30.0
2024-07-10 15:28:44,622:INFO:             prophet: Not installed
2024-07-10 15:28:44,622:INFO:None
2024-07-10 15:28:44,622:INFO:Set up data.
2024-07-10 15:28:44,631:INFO:Set up folding strategy.
2024-07-10 15:28:44,631:INFO:Set up train/test split.
2024-07-10 15:28:44,638:INFO:Set up index.
2024-07-10 15:28:44,638:INFO:Assigning column types.
2024-07-10 15:28:44,640:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:28:44,641:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,642:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,644:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,669:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,687:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,687:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,687:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,687:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,689:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,691:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,715:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,733:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,733:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,733:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,734:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:28:44,736:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,737:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,762:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,779:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,780:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,780:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,782:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,783:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,808:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,826:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,826:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,826:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,826:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:28:44,830:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,855:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,873:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,873:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,873:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,876:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,901:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,919:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,919:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,919:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,919:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:28:44,947:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,964:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:44,965:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,965:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:44,993:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:45,011:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:28:45,011:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,012:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,012:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:28:45,040:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:45,058:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,058:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,087:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:28:45,104:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,104:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,105:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:28:45,150:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,150:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,195:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,196:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,197:INFO:Preparing preprocessing pipeline...
2024-07-10 15:28:45,197:INFO:Set up simple imputation.
2024-07-10 15:28:45,199:INFO:Set up encoding of categorical features.
2024-07-10 15:28:45,199:INFO:Set up removing multicollinearity.
2024-07-10 15:28:45,199:INFO:Set up column transformation.
2024-07-10 15:28:45,199:INFO:Set up feature normalization.
2024-07-10 15:28:45,547:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:28:45,552:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms', 'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitted_wardrobes', 'has_lift',
                                             'is_exterior', 'built_year'],
                                    tran...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:28:45,552:INFO:Creating final display dataframe.
2024-07-10 15:28:45,777:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (16343, 26)
4        Transformed data shape        (16343, 63)
5   Transformed train set shape        (11440, 63)
6    Transformed test set shape         (4903, 63)
7               Ignore features                  8
8              Numeric features                 12
9          Categorical features                  5
10     Rows with missing values              65.0%
11                   Preprocess               True
12              Imputation type             simple
13           Numeric imputation               mean
14       Categorical imputation               mode
15     Maximum one-hot encoding                 25
16              Encoding method               None
17     Remove multicollinearity               True
18  Multicollinearity threshold                0.9
19               Transformation               True
20        Transformation method        yeo-johnson
21                    Normalize               True
22             Normalize method             zscore
23               Fold Generator              KFold
24                  Fold Number                 10
25                     CPU Jobs                 -1
26                      Use GPU              False
27               Log Experiment              False
28              Experiment Name   reg-default-name
29                          USI               0cce
2024-07-10 15:28:45,826:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,827:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,873:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,873:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:28:45,874:INFO:setup() successfully completed in 1.51s...............
2024-07-10 15:28:45,880:INFO:Initializing compare_models()
2024-07-10 15:28:45,880:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 15:28:45,880:INFO:Checking exceptions
2024-07-10 15:28:45,882:INFO:Preparing display monitor
2024-07-10 15:28:45,911:INFO:Initializing Linear Regression
2024-07-10 15:28:45,911:INFO:Total runtime is 3.234545389811198e-06 minutes
2024-07-10 15:28:45,912:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:45,913:INFO:Initializing create_model()
2024-07-10 15:28:45,913:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:45,913:INFO:Checking exceptions
2024-07-10 15:28:45,913:INFO:Importing libraries
2024-07-10 15:28:45,913:INFO:Copying training dataset
2024-07-10 15:28:45,919:INFO:Defining folds
2024-07-10 15:28:45,919:INFO:Declaring metric variables
2024-07-10 15:28:45,920:INFO:Importing untrained model
2024-07-10 15:28:45,921:INFO:Linear Regression Imported successfully
2024-07-10 15:28:45,924:INFO:Starting cross validation
2024-07-10 15:28:45,928:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:49,288:INFO:Calculating mean and std
2024-07-10 15:28:49,289:INFO:Creating metrics dataframe
2024-07-10 15:28:49,290:INFO:Uploading results into container
2024-07-10 15:28:49,290:INFO:Uploading model into container now
2024-07-10 15:28:49,291:INFO:_master_model_container: 1
2024-07-10 15:28:49,291:INFO:_display_container: 2
2024-07-10 15:28:49,291:INFO:LinearRegression(n_jobs=-1)
2024-07-10 15:28:49,291:INFO:create_model() successfully completed......................................
2024-07-10 15:28:49,355:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:49,355:INFO:Creating metrics dataframe
2024-07-10 15:28:49,358:INFO:Initializing Lasso Regression
2024-07-10 15:28:49,358:INFO:Total runtime is 0.05745673576990764 minutes
2024-07-10 15:28:49,359:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:49,359:INFO:Initializing create_model()
2024-07-10 15:28:49,359:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:49,360:INFO:Checking exceptions
2024-07-10 15:28:49,360:INFO:Importing libraries
2024-07-10 15:28:49,360:INFO:Copying training dataset
2024-07-10 15:28:49,365:INFO:Defining folds
2024-07-10 15:28:49,365:INFO:Declaring metric variables
2024-07-10 15:28:49,366:INFO:Importing untrained model
2024-07-10 15:28:49,367:INFO:Lasso Regression Imported successfully
2024-07-10 15:28:49,369:INFO:Starting cross validation
2024-07-10 15:28:49,371:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:51,285:INFO:Calculating mean and std
2024-07-10 15:28:51,286:INFO:Creating metrics dataframe
2024-07-10 15:28:51,287:INFO:Uploading results into container
2024-07-10 15:28:51,287:INFO:Uploading model into container now
2024-07-10 15:28:51,288:INFO:_master_model_container: 2
2024-07-10 15:28:51,288:INFO:_display_container: 2
2024-07-10 15:28:51,288:INFO:Lasso(random_state=9)
2024-07-10 15:28:51,288:INFO:create_model() successfully completed......................................
2024-07-10 15:28:51,348:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:51,348:INFO:Creating metrics dataframe
2024-07-10 15:28:51,351:INFO:Initializing Ridge Regression
2024-07-10 15:28:51,351:INFO:Total runtime is 0.09066857099533082 minutes
2024-07-10 15:28:51,352:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:51,352:INFO:Initializing create_model()
2024-07-10 15:28:51,352:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:51,352:INFO:Checking exceptions
2024-07-10 15:28:51,352:INFO:Importing libraries
2024-07-10 15:28:51,352:INFO:Copying training dataset
2024-07-10 15:28:51,357:INFO:Defining folds
2024-07-10 15:28:51,357:INFO:Declaring metric variables
2024-07-10 15:28:51,359:INFO:Importing untrained model
2024-07-10 15:28:51,360:INFO:Ridge Regression Imported successfully
2024-07-10 15:28:51,362:INFO:Starting cross validation
2024-07-10 15:28:51,363:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:52,264:INFO:Calculating mean and std
2024-07-10 15:28:52,265:INFO:Creating metrics dataframe
2024-07-10 15:28:52,266:INFO:Uploading results into container
2024-07-10 15:28:52,266:INFO:Uploading model into container now
2024-07-10 15:28:52,266:INFO:_master_model_container: 3
2024-07-10 15:28:52,266:INFO:_display_container: 2
2024-07-10 15:28:52,266:INFO:Ridge(random_state=9)
2024-07-10 15:28:52,266:INFO:create_model() successfully completed......................................
2024-07-10 15:28:52,316:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:52,317:INFO:Creating metrics dataframe
2024-07-10 15:28:52,319:INFO:Initializing Elastic Net
2024-07-10 15:28:52,320:INFO:Total runtime is 0.1068163832028707 minutes
2024-07-10 15:28:52,321:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:52,321:INFO:Initializing create_model()
2024-07-10 15:28:52,321:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:52,321:INFO:Checking exceptions
2024-07-10 15:28:52,321:INFO:Importing libraries
2024-07-10 15:28:52,321:INFO:Copying training dataset
2024-07-10 15:28:52,326:INFO:Defining folds
2024-07-10 15:28:52,326:INFO:Declaring metric variables
2024-07-10 15:28:52,327:INFO:Importing untrained model
2024-07-10 15:28:52,328:INFO:Elastic Net Imported successfully
2024-07-10 15:28:52,330:INFO:Starting cross validation
2024-07-10 15:28:52,331:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:53,251:INFO:Calculating mean and std
2024-07-10 15:28:53,252:INFO:Creating metrics dataframe
2024-07-10 15:28:53,253:INFO:Uploading results into container
2024-07-10 15:28:53,253:INFO:Uploading model into container now
2024-07-10 15:28:53,253:INFO:_master_model_container: 4
2024-07-10 15:28:53,253:INFO:_display_container: 2
2024-07-10 15:28:53,253:INFO:ElasticNet(random_state=9)
2024-07-10 15:28:53,253:INFO:create_model() successfully completed......................................
2024-07-10 15:28:53,304:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:53,304:INFO:Creating metrics dataframe
2024-07-10 15:28:53,307:INFO:Initializing Least Angle Regression
2024-07-10 15:28:53,307:INFO:Total runtime is 0.12326976855595907 minutes
2024-07-10 15:28:53,308:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:53,308:INFO:Initializing create_model()
2024-07-10 15:28:53,308:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:53,308:INFO:Checking exceptions
2024-07-10 15:28:53,308:INFO:Importing libraries
2024-07-10 15:28:53,308:INFO:Copying training dataset
2024-07-10 15:28:53,313:INFO:Defining folds
2024-07-10 15:28:53,313:INFO:Declaring metric variables
2024-07-10 15:28:53,314:INFO:Importing untrained model
2024-07-10 15:28:53,315:INFO:Least Angle Regression Imported successfully
2024-07-10 15:28:53,317:INFO:Starting cross validation
2024-07-10 15:28:53,318:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:53,949:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.488e+00, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:28:53,949:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.124e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:28:53,949:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.970e+00, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:28:53,949:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=4.633e+00, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:28:54,206:INFO:Calculating mean and std
2024-07-10 15:28:54,206:INFO:Creating metrics dataframe
2024-07-10 15:28:54,207:INFO:Uploading results into container
2024-07-10 15:28:54,207:INFO:Uploading model into container now
2024-07-10 15:28:54,208:INFO:_master_model_container: 5
2024-07-10 15:28:54,208:INFO:_display_container: 2
2024-07-10 15:28:54,208:INFO:Lars(random_state=9)
2024-07-10 15:28:54,208:INFO:create_model() successfully completed......................................
2024-07-10 15:28:54,259:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:54,259:INFO:Creating metrics dataframe
2024-07-10 15:28:54,261:INFO:Initializing Lasso Least Angle Regression
2024-07-10 15:28:54,261:INFO:Total runtime is 0.13918135166168213 minutes
2024-07-10 15:28:54,263:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:54,263:INFO:Initializing create_model()
2024-07-10 15:28:54,263:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:54,263:INFO:Checking exceptions
2024-07-10 15:28:54,263:INFO:Importing libraries
2024-07-10 15:28:54,263:INFO:Copying training dataset
2024-07-10 15:28:54,267:INFO:Defining folds
2024-07-10 15:28:54,267:INFO:Declaring metric variables
2024-07-10 15:28:54,268:INFO:Importing untrained model
2024-07-10 15:28:54,269:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 15:28:54,271:INFO:Starting cross validation
2024-07-10 15:28:54,273:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:55,154:INFO:Calculating mean and std
2024-07-10 15:28:55,154:INFO:Creating metrics dataframe
2024-07-10 15:28:55,155:INFO:Uploading results into container
2024-07-10 15:28:55,155:INFO:Uploading model into container now
2024-07-10 15:28:55,155:INFO:_master_model_container: 6
2024-07-10 15:28:55,155:INFO:_display_container: 2
2024-07-10 15:28:55,156:INFO:LassoLars(random_state=9)
2024-07-10 15:28:55,156:INFO:create_model() successfully completed......................................
2024-07-10 15:28:55,206:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:55,206:INFO:Creating metrics dataframe
2024-07-10 15:28:55,209:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 15:28:55,209:INFO:Total runtime is 0.1549780011177063 minutes
2024-07-10 15:28:55,210:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:55,211:INFO:Initializing create_model()
2024-07-10 15:28:55,211:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:55,211:INFO:Checking exceptions
2024-07-10 15:28:55,211:INFO:Importing libraries
2024-07-10 15:28:55,211:INFO:Copying training dataset
2024-07-10 15:28:55,215:INFO:Defining folds
2024-07-10 15:28:55,215:INFO:Declaring metric variables
2024-07-10 15:28:55,216:INFO:Importing untrained model
2024-07-10 15:28:55,217:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 15:28:55,219:INFO:Starting cross validation
2024-07-10 15:28:55,220:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:56,091:INFO:Calculating mean and std
2024-07-10 15:28:56,091:INFO:Creating metrics dataframe
2024-07-10 15:28:56,092:INFO:Uploading results into container
2024-07-10 15:28:56,092:INFO:Uploading model into container now
2024-07-10 15:28:56,093:INFO:_master_model_container: 7
2024-07-10 15:28:56,093:INFO:_display_container: 2
2024-07-10 15:28:56,093:INFO:OrthogonalMatchingPursuit()
2024-07-10 15:28:56,093:INFO:create_model() successfully completed......................................
2024-07-10 15:28:56,143:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:56,144:INFO:Creating metrics dataframe
2024-07-10 15:28:56,147:INFO:Initializing Bayesian Ridge
2024-07-10 15:28:56,147:INFO:Total runtime is 0.1706016182899475 minutes
2024-07-10 15:28:56,148:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:56,148:INFO:Initializing create_model()
2024-07-10 15:28:56,148:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:56,148:INFO:Checking exceptions
2024-07-10 15:28:56,148:INFO:Importing libraries
2024-07-10 15:28:56,148:INFO:Copying training dataset
2024-07-10 15:28:56,152:INFO:Defining folds
2024-07-10 15:28:56,152:INFO:Declaring metric variables
2024-07-10 15:28:56,153:INFO:Importing untrained model
2024-07-10 15:28:56,155:INFO:Bayesian Ridge Imported successfully
2024-07-10 15:28:56,156:INFO:Starting cross validation
2024-07-10 15:28:56,158:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:57,124:INFO:Calculating mean and std
2024-07-10 15:28:57,124:INFO:Creating metrics dataframe
2024-07-10 15:28:57,125:INFO:Uploading results into container
2024-07-10 15:28:57,125:INFO:Uploading model into container now
2024-07-10 15:28:57,125:INFO:_master_model_container: 8
2024-07-10 15:28:57,125:INFO:_display_container: 2
2024-07-10 15:28:57,125:INFO:BayesianRidge()
2024-07-10 15:28:57,126:INFO:create_model() successfully completed......................................
2024-07-10 15:28:57,176:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:57,176:INFO:Creating metrics dataframe
2024-07-10 15:28:57,179:INFO:Initializing Passive Aggressive Regressor
2024-07-10 15:28:57,179:INFO:Total runtime is 0.1878144860267639 minutes
2024-07-10 15:28:57,181:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:57,181:INFO:Initializing create_model()
2024-07-10 15:28:57,181:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:57,181:INFO:Checking exceptions
2024-07-10 15:28:57,181:INFO:Importing libraries
2024-07-10 15:28:57,181:INFO:Copying training dataset
2024-07-10 15:28:57,185:INFO:Defining folds
2024-07-10 15:28:57,185:INFO:Declaring metric variables
2024-07-10 15:28:57,186:INFO:Importing untrained model
2024-07-10 15:28:57,187:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 15:28:57,189:INFO:Starting cross validation
2024-07-10 15:28:57,190:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:58,091:INFO:Calculating mean and std
2024-07-10 15:28:58,092:INFO:Creating metrics dataframe
2024-07-10 15:28:58,092:INFO:Uploading results into container
2024-07-10 15:28:58,093:INFO:Uploading model into container now
2024-07-10 15:28:58,093:INFO:_master_model_container: 9
2024-07-10 15:28:58,093:INFO:_display_container: 2
2024-07-10 15:28:58,093:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 15:28:58,093:INFO:create_model() successfully completed......................................
2024-07-10 15:28:58,144:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:58,144:INFO:Creating metrics dataframe
2024-07-10 15:28:58,147:INFO:Initializing Huber Regressor
2024-07-10 15:28:58,147:INFO:Total runtime is 0.20394165515899657 minutes
2024-07-10 15:28:58,148:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:58,148:INFO:Initializing create_model()
2024-07-10 15:28:58,148:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:58,148:INFO:Checking exceptions
2024-07-10 15:28:58,149:INFO:Importing libraries
2024-07-10 15:28:58,149:INFO:Copying training dataset
2024-07-10 15:28:58,153:INFO:Defining folds
2024-07-10 15:28:58,153:INFO:Declaring metric variables
2024-07-10 15:28:58,154:INFO:Importing untrained model
2024-07-10 15:28:58,155:INFO:Huber Regressor Imported successfully
2024-07-10 15:28:58,157:INFO:Starting cross validation
2024-07-10 15:28:58,158:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:28:59,387:INFO:Calculating mean and std
2024-07-10 15:28:59,388:INFO:Creating metrics dataframe
2024-07-10 15:28:59,389:INFO:Uploading results into container
2024-07-10 15:28:59,389:INFO:Uploading model into container now
2024-07-10 15:28:59,389:INFO:_master_model_container: 10
2024-07-10 15:28:59,389:INFO:_display_container: 2
2024-07-10 15:28:59,389:INFO:HuberRegressor()
2024-07-10 15:28:59,389:INFO:create_model() successfully completed......................................
2024-07-10 15:28:59,443:INFO:SubProcess create_model() end ==================================
2024-07-10 15:28:59,443:INFO:Creating metrics dataframe
2024-07-10 15:28:59,446:INFO:Initializing K Neighbors Regressor
2024-07-10 15:28:59,446:INFO:Total runtime is 0.22559435367584227 minutes
2024-07-10 15:28:59,447:INFO:SubProcess create_model() called ==================================
2024-07-10 15:28:59,447:INFO:Initializing create_model()
2024-07-10 15:28:59,448:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:28:59,448:INFO:Checking exceptions
2024-07-10 15:28:59,448:INFO:Importing libraries
2024-07-10 15:28:59,448:INFO:Copying training dataset
2024-07-10 15:28:59,452:INFO:Defining folds
2024-07-10 15:28:59,452:INFO:Declaring metric variables
2024-07-10 15:28:59,453:INFO:Importing untrained model
2024-07-10 15:28:59,454:INFO:K Neighbors Regressor Imported successfully
2024-07-10 15:28:59,456:INFO:Starting cross validation
2024-07-10 15:28:59,457:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:00,574:INFO:Calculating mean and std
2024-07-10 15:29:00,575:INFO:Creating metrics dataframe
2024-07-10 15:29:00,575:INFO:Uploading results into container
2024-07-10 15:29:00,576:INFO:Uploading model into container now
2024-07-10 15:29:00,576:INFO:_master_model_container: 11
2024-07-10 15:29:00,576:INFO:_display_container: 2
2024-07-10 15:29:00,576:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 15:29:00,576:INFO:create_model() successfully completed......................................
2024-07-10 15:29:00,627:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:00,627:INFO:Creating metrics dataframe
2024-07-10 15:29:00,631:INFO:Initializing Decision Tree Regressor
2024-07-10 15:29:00,631:INFO:Total runtime is 0.2453342517217 minutes
2024-07-10 15:29:00,632:INFO:SubProcess create_model() called ==================================
2024-07-10 15:29:00,632:INFO:Initializing create_model()
2024-07-10 15:29:00,632:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:00,632:INFO:Checking exceptions
2024-07-10 15:29:00,632:INFO:Importing libraries
2024-07-10 15:29:00,632:INFO:Copying training dataset
2024-07-10 15:29:00,636:INFO:Defining folds
2024-07-10 15:29:00,636:INFO:Declaring metric variables
2024-07-10 15:29:00,637:INFO:Importing untrained model
2024-07-10 15:29:00,638:INFO:Decision Tree Regressor Imported successfully
2024-07-10 15:29:00,640:INFO:Starting cross validation
2024-07-10 15:29:00,641:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:01,653:INFO:Calculating mean and std
2024-07-10 15:29:01,654:INFO:Creating metrics dataframe
2024-07-10 15:29:01,655:INFO:Uploading results into container
2024-07-10 15:29:01,655:INFO:Uploading model into container now
2024-07-10 15:29:01,655:INFO:_master_model_container: 12
2024-07-10 15:29:01,655:INFO:_display_container: 2
2024-07-10 15:29:01,655:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 15:29:01,655:INFO:create_model() successfully completed......................................
2024-07-10 15:29:01,706:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:01,706:INFO:Creating metrics dataframe
2024-07-10 15:29:01,710:INFO:Initializing Random Forest Regressor
2024-07-10 15:29:01,710:INFO:Total runtime is 0.26332363684972127 minutes
2024-07-10 15:29:01,711:INFO:SubProcess create_model() called ==================================
2024-07-10 15:29:01,711:INFO:Initializing create_model()
2024-07-10 15:29:01,711:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:01,711:INFO:Checking exceptions
2024-07-10 15:29:01,711:INFO:Importing libraries
2024-07-10 15:29:01,712:INFO:Copying training dataset
2024-07-10 15:29:01,716:INFO:Defining folds
2024-07-10 15:29:01,716:INFO:Declaring metric variables
2024-07-10 15:29:01,717:INFO:Importing untrained model
2024-07-10 15:29:01,718:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:29:01,720:INFO:Starting cross validation
2024-07-10 15:29:01,721:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:08,961:INFO:Calculating mean and std
2024-07-10 15:29:08,962:INFO:Creating metrics dataframe
2024-07-10 15:29:08,963:INFO:Uploading results into container
2024-07-10 15:29:08,963:INFO:Uploading model into container now
2024-07-10 15:29:08,963:INFO:_master_model_container: 13
2024-07-10 15:29:08,963:INFO:_display_container: 2
2024-07-10 15:29:08,963:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:29:08,963:INFO:create_model() successfully completed......................................
2024-07-10 15:29:09,015:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:09,015:INFO:Creating metrics dataframe
2024-07-10 15:29:09,019:INFO:Initializing Extra Trees Regressor
2024-07-10 15:29:09,019:INFO:Total runtime is 0.38514153560002645 minutes
2024-07-10 15:29:09,020:INFO:SubProcess create_model() called ==================================
2024-07-10 15:29:09,020:INFO:Initializing create_model()
2024-07-10 15:29:09,020:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:09,021:INFO:Checking exceptions
2024-07-10 15:29:09,021:INFO:Importing libraries
2024-07-10 15:29:09,021:INFO:Copying training dataset
2024-07-10 15:29:09,025:INFO:Defining folds
2024-07-10 15:29:09,025:INFO:Declaring metric variables
2024-07-10 15:29:09,026:INFO:Importing untrained model
2024-07-10 15:29:09,027:INFO:Extra Trees Regressor Imported successfully
2024-07-10 15:29:09,029:INFO:Starting cross validation
2024-07-10 15:29:09,030:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:16,366:INFO:Calculating mean and std
2024-07-10 15:29:16,367:INFO:Creating metrics dataframe
2024-07-10 15:29:16,368:INFO:Uploading results into container
2024-07-10 15:29:16,368:INFO:Uploading model into container now
2024-07-10 15:29:16,368:INFO:_master_model_container: 14
2024-07-10 15:29:16,368:INFO:_display_container: 2
2024-07-10 15:29:16,368:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:29:16,368:INFO:create_model() successfully completed......................................
2024-07-10 15:29:16,424:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:16,424:INFO:Creating metrics dataframe
2024-07-10 15:29:16,428:INFO:Initializing AdaBoost Regressor
2024-07-10 15:29:16,428:INFO:Total runtime is 0.5086241563161215 minutes
2024-07-10 15:29:16,429:INFO:SubProcess create_model() called ==================================
2024-07-10 15:29:16,429:INFO:Initializing create_model()
2024-07-10 15:29:16,429:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:16,429:INFO:Checking exceptions
2024-07-10 15:29:16,430:INFO:Importing libraries
2024-07-10 15:29:16,430:INFO:Copying training dataset
2024-07-10 15:29:16,434:INFO:Defining folds
2024-07-10 15:29:16,434:INFO:Declaring metric variables
2024-07-10 15:29:16,435:INFO:Importing untrained model
2024-07-10 15:29:16,436:INFO:AdaBoost Regressor Imported successfully
2024-07-10 15:29:16,438:INFO:Starting cross validation
2024-07-10 15:29:16,439:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:18,756:INFO:Calculating mean and std
2024-07-10 15:29:18,757:INFO:Creating metrics dataframe
2024-07-10 15:29:18,757:INFO:Uploading results into container
2024-07-10 15:29:18,758:INFO:Uploading model into container now
2024-07-10 15:29:18,758:INFO:_master_model_container: 15
2024-07-10 15:29:18,758:INFO:_display_container: 2
2024-07-10 15:29:18,758:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 15:29:18,758:INFO:create_model() successfully completed......................................
2024-07-10 15:29:18,809:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:18,810:INFO:Creating metrics dataframe
2024-07-10 15:29:18,813:INFO:Initializing Gradient Boosting Regressor
2024-07-10 15:29:18,813:INFO:Total runtime is 0.5483800212542216 minutes
2024-07-10 15:29:18,815:INFO:SubProcess create_model() called ==================================
2024-07-10 15:29:18,815:INFO:Initializing create_model()
2024-07-10 15:29:18,815:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:18,815:INFO:Checking exceptions
2024-07-10 15:29:18,815:INFO:Importing libraries
2024-07-10 15:29:18,815:INFO:Copying training dataset
2024-07-10 15:29:18,819:INFO:Defining folds
2024-07-10 15:29:18,819:INFO:Declaring metric variables
2024-07-10 15:29:18,820:INFO:Importing untrained model
2024-07-10 15:29:18,821:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 15:29:18,823:INFO:Starting cross validation
2024-07-10 15:29:18,825:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:21,816:INFO:Calculating mean and std
2024-07-10 15:29:21,817:INFO:Creating metrics dataframe
2024-07-10 15:29:21,817:INFO:Uploading results into container
2024-07-10 15:29:21,818:INFO:Uploading model into container now
2024-07-10 15:29:21,818:INFO:_master_model_container: 16
2024-07-10 15:29:21,818:INFO:_display_container: 2
2024-07-10 15:29:21,818:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 15:29:21,818:INFO:create_model() successfully completed......................................
2024-07-10 15:29:21,869:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:21,869:INFO:Creating metrics dataframe
2024-07-10 15:29:21,873:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 15:29:21,873:INFO:Total runtime is 0.5993719538052876 minutes
2024-07-10 15:29:21,874:INFO:SubProcess create_model() called ==================================
2024-07-10 15:29:21,874:INFO:Initializing create_model()
2024-07-10 15:29:21,874:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:21,874:INFO:Checking exceptions
2024-07-10 15:29:21,874:INFO:Importing libraries
2024-07-10 15:29:21,874:INFO:Copying training dataset
2024-07-10 15:29:21,879:INFO:Defining folds
2024-07-10 15:29:21,879:INFO:Declaring metric variables
2024-07-10 15:29:21,880:INFO:Importing untrained model
2024-07-10 15:29:21,881:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:29:21,883:INFO:Starting cross validation
2024-07-10 15:29:21,884:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:25,325:INFO:Calculating mean and std
2024-07-10 15:29:25,326:INFO:Creating metrics dataframe
2024-07-10 15:29:25,327:INFO:Uploading results into container
2024-07-10 15:29:25,327:INFO:Uploading model into container now
2024-07-10 15:29:25,327:INFO:_master_model_container: 17
2024-07-10 15:29:25,327:INFO:_display_container: 2
2024-07-10 15:29:25,328:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:29:25,328:INFO:create_model() successfully completed......................................
2024-07-10 15:29:25,378:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:25,378:INFO:Creating metrics dataframe
2024-07-10 15:29:25,382:INFO:Initializing Dummy Regressor
2024-07-10 15:29:25,382:INFO:Total runtime is 0.6578598896662393 minutes
2024-07-10 15:29:25,383:INFO:SubProcess create_model() called ==================================
2024-07-10 15:29:25,383:INFO:Initializing create_model()
2024-07-10 15:29:25,383:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x104642190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:25,383:INFO:Checking exceptions
2024-07-10 15:29:25,383:INFO:Importing libraries
2024-07-10 15:29:25,384:INFO:Copying training dataset
2024-07-10 15:29:25,388:INFO:Defining folds
2024-07-10 15:29:25,388:INFO:Declaring metric variables
2024-07-10 15:29:25,389:INFO:Importing untrained model
2024-07-10 15:29:25,390:INFO:Dummy Regressor Imported successfully
2024-07-10 15:29:25,392:INFO:Starting cross validation
2024-07-10 15:29:25,393:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:29:26,259:INFO:Calculating mean and std
2024-07-10 15:29:26,260:INFO:Creating metrics dataframe
2024-07-10 15:29:26,261:INFO:Uploading results into container
2024-07-10 15:29:26,261:INFO:Uploading model into container now
2024-07-10 15:29:26,261:INFO:_master_model_container: 18
2024-07-10 15:29:26,261:INFO:_display_container: 2
2024-07-10 15:29:26,261:INFO:DummyRegressor()
2024-07-10 15:29:26,261:INFO:create_model() successfully completed......................................
2024-07-10 15:29:26,312:INFO:SubProcess create_model() end ==================================
2024-07-10 15:29:26,312:INFO:Creating metrics dataframe
2024-07-10 15:29:26,319:INFO:Initializing create_model()
2024-07-10 15:29:26,319:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:29:26,319:INFO:Checking exceptions
2024-07-10 15:29:26,320:INFO:Importing libraries
2024-07-10 15:29:26,320:INFO:Copying training dataset
2024-07-10 15:29:26,324:INFO:Defining folds
2024-07-10 15:29:26,324:INFO:Declaring metric variables
2024-07-10 15:29:26,324:INFO:Importing untrained model
2024-07-10 15:29:26,324:INFO:Declaring custom model
2024-07-10 15:29:26,324:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:29:26,325:INFO:Cross validation set to False
2024-07-10 15:29:26,325:INFO:Fitting Model
2024-07-10 15:29:27,383:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:29:27,383:INFO:create_model() successfully completed......................................
2024-07-10 15:29:27,443:INFO:_master_model_container: 18
2024-07-10 15:29:27,443:INFO:_display_container: 2
2024-07-10 15:29:27,443:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:29:27,443:INFO:compare_models() successfully completed......................................
2024-07-10 15:29:27,446:INFO:Initializing predict_model()
2024-07-10 15:29:27,446:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x134137a60>)
2024-07-10 15:29:27,446:INFO:Checking exceptions
2024-07-10 15:29:27,446:INFO:Preloading libraries
2024-07-10 15:29:27,447:INFO:Set up data.
2024-07-10 15:29:27,452:INFO:Set up index.
2024-07-10 15:29:27,586:INFO:Initializing plot_model()
2024-07-10 15:29:27,586:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), plot=residuals, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:29:27,586:INFO:Checking exceptions
2024-07-10 15:29:27,596:INFO:Preloading libraries
2024-07-10 15:29:27,642:INFO:Copying training dataset
2024-07-10 15:29:27,642:INFO:Plot type: residuals
2024-07-10 15:29:27,884:INFO:Fitting Model
2024-07-10 15:29:27,963:INFO:Scoring test/hold-out set
2024-07-10 15:29:28,280:INFO:Visual Rendered Successfully
2024-07-10 15:29:28,337:INFO:plot_model() successfully completed......................................
2024-07-10 15:29:28,347:INFO:Initializing plot_model()
2024-07-10 15:29:28,348:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:29:28,348:INFO:Checking exceptions
2024-07-10 15:29:28,355:INFO:Preloading libraries
2024-07-10 15:29:28,394:INFO:Copying training dataset
2024-07-10 15:29:28,394:INFO:Plot type: error
2024-07-10 15:29:28,619:INFO:Fitting Model
2024-07-10 15:29:28,619:INFO:Scoring test/hold-out set
2024-07-10 15:29:28,756:INFO:Visual Rendered Successfully
2024-07-10 15:29:28,816:INFO:plot_model() successfully completed......................................
2024-07-10 15:29:28,822:INFO:Initializing plot_model()
2024-07-10 15:29:28,822:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x13257be50>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:29:28,822:INFO:Checking exceptions
2024-07-10 15:29:28,837:INFO:Preloading libraries
2024-07-10 15:29:28,878:INFO:Copying training dataset
2024-07-10 15:29:28,878:INFO:Plot type: feature
2024-07-10 15:29:28,878:WARNING:No coef_ found. Trying feature_importances_
2024-07-10 15:29:29,006:INFO:Visual Rendered Successfully
2024-07-10 15:29:29,061:INFO:plot_model() successfully completed......................................
2024-07-10 15:29:59,585:INFO:PyCaret RegressionExperiment
2024-07-10 15:29:59,585:INFO:Logging name: reg-default-name
2024-07-10 15:29:59,585:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:29:59,586:INFO:version 3.3.1
2024-07-10 15:29:59,586:INFO:Initializing setup()
2024-07-10 15:29:59,586:INFO:self.USI: 2622
2024-07-10 15:29:59,586:INFO:self._variable_keys: {'idx', 'y_train', 'log_plots_param', 'seed', 'logging_param', 'gpu_param', 'exp_id', 'fold_groups_param', 'n_jobs_param', 'pipeline', 'y', 'transform_target_param', 'fold_generator', '_available_plots', 'y_test', 'X', 'X_train', 'fold_shuffle_param', 'memory', 'USI', 'gpu_n_jobs_param', 'target_param', 'exp_name_log', 'html_param', 'X_test', 'data', '_ml_usecase'}
2024-07-10 15:29:59,586:INFO:Checking environment
2024-07-10 15:29:59,586:INFO:python_version: 3.11.8
2024-07-10 15:29:59,586:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:29:59,586:INFO:machine: arm64
2024-07-10 15:29:59,586:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:29:59,586:INFO:Memory: svmem(total=17179869184, available=4319313920, percent=74.9, used=6023626752, free=47611904, active=4286709760, inactive=4270571520, wired=1736916992)
2024-07-10 15:29:59,586:INFO:Physical Core: 8
2024-07-10 15:29:59,586:INFO:Logical Core: 8
2024-07-10 15:29:59,586:INFO:Checking libraries
2024-07-10 15:29:59,586:INFO:System:
2024-07-10 15:29:59,586:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:29:59,586:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:29:59,586:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:29:59,586:INFO:PyCaret required dependencies:
2024-07-10 15:29:59,586:INFO:                 pip: 23.3.1
2024-07-10 15:29:59,586:INFO:          setuptools: 68.2.2
2024-07-10 15:29:59,586:INFO:             pycaret: 3.3.1
2024-07-10 15:29:59,586:INFO:             IPython: 8.20.0
2024-07-10 15:29:59,586:INFO:          ipywidgets: 7.6.5
2024-07-10 15:29:59,586:INFO:                tqdm: 4.65.0
2024-07-10 15:29:59,586:INFO:               numpy: 1.26.4
2024-07-10 15:29:59,586:INFO:              pandas: 2.1.4
2024-07-10 15:29:59,586:INFO:              jinja2: 3.1.3
2024-07-10 15:29:59,587:INFO:               scipy: 1.11.4
2024-07-10 15:29:59,587:INFO:              joblib: 1.2.0
2024-07-10 15:29:59,587:INFO:             sklearn: 1.4.2
2024-07-10 15:29:59,587:INFO:                pyod: 1.1.3
2024-07-10 15:29:59,587:INFO:            imblearn: 0.12.2
2024-07-10 15:29:59,587:INFO:   category_encoders: 2.6.3
2024-07-10 15:29:59,587:INFO:            lightgbm: 4.3.0
2024-07-10 15:29:59,587:INFO:               numba: 0.59.0
2024-07-10 15:29:59,587:INFO:            requests: 2.31.0
2024-07-10 15:29:59,587:INFO:          matplotlib: 3.8.0
2024-07-10 15:29:59,587:INFO:          scikitplot: 0.3.7
2024-07-10 15:29:59,587:INFO:         yellowbrick: 1.5
2024-07-10 15:29:59,587:INFO:              plotly: 5.22.0
2024-07-10 15:29:59,587:INFO:    plotly-resampler: Not installed
2024-07-10 15:29:59,587:INFO:             kaleido: 0.2.1
2024-07-10 15:29:59,587:INFO:           schemdraw: 0.15
2024-07-10 15:29:59,587:INFO:         statsmodels: 0.14.0
2024-07-10 15:29:59,587:INFO:              sktime: 0.26.0
2024-07-10 15:29:59,587:INFO:               tbats: 1.1.3
2024-07-10 15:29:59,587:INFO:            pmdarima: 2.0.4
2024-07-10 15:29:59,587:INFO:              psutil: 5.9.0
2024-07-10 15:29:59,587:INFO:          markupsafe: 2.1.3
2024-07-10 15:29:59,587:INFO:             pickle5: Not installed
2024-07-10 15:29:59,587:INFO:         cloudpickle: 2.2.1
2024-07-10 15:29:59,587:INFO:         deprecation: 2.1.0
2024-07-10 15:29:59,587:INFO:              xxhash: 3.4.1
2024-07-10 15:29:59,587:INFO:           wurlitzer: 3.0.2
2024-07-10 15:29:59,587:INFO:PyCaret optional dependencies:
2024-07-10 15:29:59,587:INFO:                shap: Not installed
2024-07-10 15:29:59,587:INFO:           interpret: Not installed
2024-07-10 15:29:59,588:INFO:                umap: 0.5.5
2024-07-10 15:29:59,588:INFO:     ydata_profiling: Not installed
2024-07-10 15:29:59,588:INFO:  explainerdashboard: Not installed
2024-07-10 15:29:59,588:INFO:             autoviz: Not installed
2024-07-10 15:29:59,588:INFO:           fairlearn: Not installed
2024-07-10 15:29:59,588:INFO:          deepchecks: Not installed
2024-07-10 15:29:59,588:INFO:             xgboost: Not installed
2024-07-10 15:29:59,588:INFO:            catboost: Not installed
2024-07-10 15:29:59,588:INFO:              kmodes: Not installed
2024-07-10 15:29:59,588:INFO:             mlxtend: Not installed
2024-07-10 15:29:59,588:INFO:       statsforecast: Not installed
2024-07-10 15:29:59,588:INFO:        tune_sklearn: Not installed
2024-07-10 15:29:59,588:INFO:                 ray: Not installed
2024-07-10 15:29:59,588:INFO:            hyperopt: Not installed
2024-07-10 15:29:59,588:INFO:              optuna: Not installed
2024-07-10 15:29:59,588:INFO:               skopt: Not installed
2024-07-10 15:29:59,588:INFO:              mlflow: 2.13.0
2024-07-10 15:29:59,588:INFO:              gradio: Not installed
2024-07-10 15:29:59,588:INFO:             fastapi: Not installed
2024-07-10 15:29:59,588:INFO:             uvicorn: Not installed
2024-07-10 15:29:59,588:INFO:              m2cgen: Not installed
2024-07-10 15:29:59,588:INFO:           evidently: Not installed
2024-07-10 15:29:59,588:INFO:               fugue: Not installed
2024-07-10 15:29:59,588:INFO:           streamlit: 1.30.0
2024-07-10 15:29:59,588:INFO:             prophet: Not installed
2024-07-10 15:29:59,588:INFO:None
2024-07-10 15:29:59,588:INFO:Set up data.
2024-07-10 15:29:59,605:INFO:Set up folding strategy.
2024-07-10 15:29:59,605:INFO:Set up train/test split.
2024-07-10 15:29:59,613:INFO:Set up index.
2024-07-10 15:29:59,614:INFO:Assigning column types.
2024-07-10 15:29:59,618:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:29:59,618:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,620:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,622:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,652:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,672:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,672:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,672:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,673:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,675:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,676:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,701:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,718:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,718:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,718:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,718:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:29:59,720:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,722:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,746:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,763:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,763:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,763:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,765:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,767:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,791:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,808:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,808:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,808:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,809:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:29:59,812:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,836:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,853:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,854:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,857:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,881:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,899:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,899:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,899:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,899:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:29:59,927:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,944:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,944:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,944:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,972:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,989:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:29:59,989:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,989:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:29:59,989:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:30:00,017:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:30:00,035:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,035:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,064:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:30:00,082:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,082:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,082:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:30:00,127:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,127:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,173:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,173:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,175:INFO:Preparing preprocessing pipeline...
2024-07-10 15:30:00,175:INFO:Set up simple imputation.
2024-07-10 15:30:00,177:INFO:Set up encoding of categorical features.
2024-07-10 15:30:00,177:INFO:Set up removing multicollinearity.
2024-07-10 15:30:00,177:INFO:Set up column transformation.
2024-07-10 15:30:00,177:INFO:Set up feature normalization.
2024-07-10 15:30:00,554:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:30:00,559:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms', 'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitted_wardrobes', 'has_lift',
                                             'is_exterior',
                                             'is_orientation_so...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:30:00,559:INFO:Creating final display dataframe.
2024-07-10 15:30:00,784:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (16343, 26)
4        Transformed data shape        (16343, 64)
5   Transformed train set shape        (11440, 64)
6    Transformed test set shape         (4903, 64)
7               Ignore features                  7
8              Numeric features                 13
9          Categorical features                  5
10     Rows with missing values              65.0%
11                   Preprocess               True
12              Imputation type             simple
13           Numeric imputation               mean
14       Categorical imputation               mode
15     Maximum one-hot encoding                 25
16              Encoding method               None
17     Remove multicollinearity               True
18  Multicollinearity threshold                0.9
19               Transformation               True
20        Transformation method        yeo-johnson
21                    Normalize               True
22             Normalize method             zscore
23               Fold Generator              KFold
24                  Fold Number                 10
25                     CPU Jobs                 -1
26                      Use GPU              False
27               Log Experiment              False
28              Experiment Name   reg-default-name
29                          USI               2622
2024-07-10 15:30:00,834:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,834:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,884:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,884:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:30:00,884:INFO:setup() successfully completed in 1.31s...............
2024-07-10 15:30:00,896:INFO:Initializing compare_models()
2024-07-10 15:30:00,896:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 15:30:00,896:INFO:Checking exceptions
2024-07-10 15:30:00,900:INFO:Preparing display monitor
2024-07-10 15:30:00,909:INFO:Initializing Linear Regression
2024-07-10 15:30:00,909:INFO:Total runtime is 3.234545389811198e-06 minutes
2024-07-10 15:30:00,910:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:00,911:INFO:Initializing create_model()
2024-07-10 15:30:00,911:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:00,911:INFO:Checking exceptions
2024-07-10 15:30:00,911:INFO:Importing libraries
2024-07-10 15:30:00,911:INFO:Copying training dataset
2024-07-10 15:30:00,918:INFO:Defining folds
2024-07-10 15:30:00,918:INFO:Declaring metric variables
2024-07-10 15:30:00,920:INFO:Importing untrained model
2024-07-10 15:30:00,921:INFO:Linear Regression Imported successfully
2024-07-10 15:30:00,923:INFO:Starting cross validation
2024-07-10 15:30:00,924:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:01,915:INFO:Calculating mean and std
2024-07-10 15:30:01,916:INFO:Creating metrics dataframe
2024-07-10 15:30:01,917:INFO:Uploading results into container
2024-07-10 15:30:01,917:INFO:Uploading model into container now
2024-07-10 15:30:01,917:INFO:_master_model_container: 1
2024-07-10 15:30:01,917:INFO:_display_container: 2
2024-07-10 15:30:01,917:INFO:LinearRegression(n_jobs=-1)
2024-07-10 15:30:01,917:INFO:create_model() successfully completed......................................
2024-07-10 15:30:01,972:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:01,972:INFO:Creating metrics dataframe
2024-07-10 15:30:01,974:INFO:Initializing Lasso Regression
2024-07-10 15:30:01,974:INFO:Total runtime is 0.017756954828898112 minutes
2024-07-10 15:30:01,975:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:01,975:INFO:Initializing create_model()
2024-07-10 15:30:01,975:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:01,975:INFO:Checking exceptions
2024-07-10 15:30:01,976:INFO:Importing libraries
2024-07-10 15:30:01,976:INFO:Copying training dataset
2024-07-10 15:30:01,980:INFO:Defining folds
2024-07-10 15:30:01,980:INFO:Declaring metric variables
2024-07-10 15:30:01,981:INFO:Importing untrained model
2024-07-10 15:30:01,982:INFO:Lasso Regression Imported successfully
2024-07-10 15:30:01,984:INFO:Starting cross validation
2024-07-10 15:30:01,985:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:03,790:INFO:Calculating mean and std
2024-07-10 15:30:03,791:INFO:Creating metrics dataframe
2024-07-10 15:30:03,792:INFO:Uploading results into container
2024-07-10 15:30:03,792:INFO:Uploading model into container now
2024-07-10 15:30:03,792:INFO:_master_model_container: 2
2024-07-10 15:30:03,793:INFO:_display_container: 2
2024-07-10 15:30:03,793:INFO:Lasso(random_state=9)
2024-07-10 15:30:03,793:INFO:create_model() successfully completed......................................
2024-07-10 15:30:03,853:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:03,853:INFO:Creating metrics dataframe
2024-07-10 15:30:03,855:INFO:Initializing Ridge Regression
2024-07-10 15:30:03,855:INFO:Total runtime is 0.04911096890767415 minutes
2024-07-10 15:30:03,856:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:03,857:INFO:Initializing create_model()
2024-07-10 15:30:03,857:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:03,857:INFO:Checking exceptions
2024-07-10 15:30:03,857:INFO:Importing libraries
2024-07-10 15:30:03,857:INFO:Copying training dataset
2024-07-10 15:30:03,861:INFO:Defining folds
2024-07-10 15:30:03,861:INFO:Declaring metric variables
2024-07-10 15:30:03,862:INFO:Importing untrained model
2024-07-10 15:30:03,864:INFO:Ridge Regression Imported successfully
2024-07-10 15:30:03,865:INFO:Starting cross validation
2024-07-10 15:30:03,867:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:04,780:INFO:Calculating mean and std
2024-07-10 15:30:04,780:INFO:Creating metrics dataframe
2024-07-10 15:30:04,781:INFO:Uploading results into container
2024-07-10 15:30:04,781:INFO:Uploading model into container now
2024-07-10 15:30:04,782:INFO:_master_model_container: 3
2024-07-10 15:30:04,782:INFO:_display_container: 2
2024-07-10 15:30:04,782:INFO:Ridge(random_state=9)
2024-07-10 15:30:04,782:INFO:create_model() successfully completed......................................
2024-07-10 15:30:04,833:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:04,833:INFO:Creating metrics dataframe
2024-07-10 15:30:04,836:INFO:Initializing Elastic Net
2024-07-10 15:30:04,836:INFO:Total runtime is 0.06545292139053344 minutes
2024-07-10 15:30:04,837:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:04,837:INFO:Initializing create_model()
2024-07-10 15:30:04,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:04,837:INFO:Checking exceptions
2024-07-10 15:30:04,837:INFO:Importing libraries
2024-07-10 15:30:04,837:INFO:Copying training dataset
2024-07-10 15:30:04,842:INFO:Defining folds
2024-07-10 15:30:04,842:INFO:Declaring metric variables
2024-07-10 15:30:04,843:INFO:Importing untrained model
2024-07-10 15:30:04,844:INFO:Elastic Net Imported successfully
2024-07-10 15:30:04,846:INFO:Starting cross validation
2024-07-10 15:30:04,847:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:05,772:INFO:Calculating mean and std
2024-07-10 15:30:05,772:INFO:Creating metrics dataframe
2024-07-10 15:30:05,773:INFO:Uploading results into container
2024-07-10 15:30:05,773:INFO:Uploading model into container now
2024-07-10 15:30:05,773:INFO:_master_model_container: 4
2024-07-10 15:30:05,773:INFO:_display_container: 2
2024-07-10 15:30:05,774:INFO:ElasticNet(random_state=9)
2024-07-10 15:30:05,774:INFO:create_model() successfully completed......................................
2024-07-10 15:30:05,825:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:05,825:INFO:Creating metrics dataframe
2024-07-10 15:30:05,828:INFO:Initializing Least Angle Regression
2024-07-10 15:30:05,828:INFO:Total runtime is 0.08199258645375568 minutes
2024-07-10 15:30:05,829:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:05,829:INFO:Initializing create_model()
2024-07-10 15:30:05,830:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:05,830:INFO:Checking exceptions
2024-07-10 15:30:05,830:INFO:Importing libraries
2024-07-10 15:30:05,830:INFO:Copying training dataset
2024-07-10 15:30:05,834:INFO:Defining folds
2024-07-10 15:30:05,834:INFO:Declaring metric variables
2024-07-10 15:30:05,835:INFO:Importing untrained model
2024-07-10 15:30:05,836:INFO:Least Angle Regression Imported successfully
2024-07-10 15:30:05,838:INFO:Starting cross validation
2024-07-10 15:30:05,840:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:06,743:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.669e+00, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:30:06,744:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=9.752e-01, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:30:06,744:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.167e+00, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:30:06,754:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:812: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2024-07-10 15:30:06,755:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:776: RuntimeWarning: overflow encountered in scalar divide
  gamma_ = min(g1, g2, C / AA)

2024-07-10 15:30:06,755:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:775: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2024-07-10 15:30:06,755:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:780: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2024-07-10 15:30:06,770:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:208: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2024-07-10 15:30:06,778:WARNING:/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/containers/metrics/regression.py", line 209, in root_mean_squared_log_error
    metrics.mean_squared_log_error(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 680, in mean_squared_log_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:30:06,779:WARNING:/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/containers/metrics/regression.py", line 233, in mean_absolute_percentage_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:30:06,780:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 207, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:30:06,780:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 497, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:30:06,780:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 572, in root_mean_squared_error
    mean_squared_error(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 497, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:30:06,780:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 1180, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:30:06,781:INFO:Calculating mean and std
2024-07-10 15:30:06,781:INFO:Creating metrics dataframe
2024-07-10 15:30:06,782:INFO:Uploading results into container
2024-07-10 15:30:06,782:INFO:Uploading model into container now
2024-07-10 15:30:06,782:INFO:_master_model_container: 5
2024-07-10 15:30:06,782:INFO:_display_container: 2
2024-07-10 15:30:06,782:INFO:Lars(random_state=9)
2024-07-10 15:30:06,783:INFO:create_model() successfully completed......................................
2024-07-10 15:30:06,834:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:06,834:INFO:Creating metrics dataframe
2024-07-10 15:30:06,837:INFO:Initializing Lasso Least Angle Regression
2024-07-10 15:30:06,837:INFO:Total runtime is 0.09880777200063068 minutes
2024-07-10 15:30:06,838:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:06,838:INFO:Initializing create_model()
2024-07-10 15:30:06,838:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:06,838:INFO:Checking exceptions
2024-07-10 15:30:06,838:INFO:Importing libraries
2024-07-10 15:30:06,839:INFO:Copying training dataset
2024-07-10 15:30:06,843:INFO:Defining folds
2024-07-10 15:30:06,843:INFO:Declaring metric variables
2024-07-10 15:30:06,844:INFO:Importing untrained model
2024-07-10 15:30:06,845:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 15:30:06,847:INFO:Starting cross validation
2024-07-10 15:30:06,848:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:07,762:INFO:Calculating mean and std
2024-07-10 15:30:07,763:INFO:Creating metrics dataframe
2024-07-10 15:30:07,764:INFO:Uploading results into container
2024-07-10 15:30:07,764:INFO:Uploading model into container now
2024-07-10 15:30:07,764:INFO:_master_model_container: 6
2024-07-10 15:30:07,764:INFO:_display_container: 2
2024-07-10 15:30:07,764:INFO:LassoLars(random_state=9)
2024-07-10 15:30:07,764:INFO:create_model() successfully completed......................................
2024-07-10 15:30:07,817:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:07,817:INFO:Creating metrics dataframe
2024-07-10 15:30:07,820:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 15:30:07,820:INFO:Total runtime is 0.11519208749135333 minutes
2024-07-10 15:30:07,821:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:07,821:INFO:Initializing create_model()
2024-07-10 15:30:07,821:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:07,821:INFO:Checking exceptions
2024-07-10 15:30:07,821:INFO:Importing libraries
2024-07-10 15:30:07,822:INFO:Copying training dataset
2024-07-10 15:30:07,827:INFO:Defining folds
2024-07-10 15:30:07,827:INFO:Declaring metric variables
2024-07-10 15:30:07,828:INFO:Importing untrained model
2024-07-10 15:30:07,828:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 15:30:07,830:INFO:Starting cross validation
2024-07-10 15:30:07,832:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:08,897:INFO:Calculating mean and std
2024-07-10 15:30:08,898:INFO:Creating metrics dataframe
2024-07-10 15:30:08,899:INFO:Uploading results into container
2024-07-10 15:30:08,899:INFO:Uploading model into container now
2024-07-10 15:30:08,900:INFO:_master_model_container: 7
2024-07-10 15:30:08,900:INFO:_display_container: 2
2024-07-10 15:30:08,900:INFO:OrthogonalMatchingPursuit()
2024-07-10 15:30:08,900:INFO:create_model() successfully completed......................................
2024-07-10 15:30:08,951:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:08,952:INFO:Creating metrics dataframe
2024-07-10 15:30:08,954:INFO:Initializing Bayesian Ridge
2024-07-10 15:30:08,955:INFO:Total runtime is 0.1340994874636332 minutes
2024-07-10 15:30:08,956:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:08,956:INFO:Initializing create_model()
2024-07-10 15:30:08,956:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:08,956:INFO:Checking exceptions
2024-07-10 15:30:08,956:INFO:Importing libraries
2024-07-10 15:30:08,956:INFO:Copying training dataset
2024-07-10 15:30:08,961:INFO:Defining folds
2024-07-10 15:30:08,961:INFO:Declaring metric variables
2024-07-10 15:30:08,962:INFO:Importing untrained model
2024-07-10 15:30:08,963:INFO:Bayesian Ridge Imported successfully
2024-07-10 15:30:08,964:INFO:Starting cross validation
2024-07-10 15:30:08,966:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:09,940:INFO:Calculating mean and std
2024-07-10 15:30:09,940:INFO:Creating metrics dataframe
2024-07-10 15:30:09,941:INFO:Uploading results into container
2024-07-10 15:30:09,941:INFO:Uploading model into container now
2024-07-10 15:30:09,941:INFO:_master_model_container: 8
2024-07-10 15:30:09,942:INFO:_display_container: 2
2024-07-10 15:30:09,942:INFO:BayesianRidge()
2024-07-10 15:30:09,942:INFO:create_model() successfully completed......................................
2024-07-10 15:30:09,994:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:09,994:INFO:Creating metrics dataframe
2024-07-10 15:30:09,997:INFO:Initializing Passive Aggressive Regressor
2024-07-10 15:30:09,997:INFO:Total runtime is 0.15147677262624104 minutes
2024-07-10 15:30:09,998:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:09,999:INFO:Initializing create_model()
2024-07-10 15:30:09,999:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:09,999:INFO:Checking exceptions
2024-07-10 15:30:09,999:INFO:Importing libraries
2024-07-10 15:30:09,999:INFO:Copying training dataset
2024-07-10 15:30:10,003:INFO:Defining folds
2024-07-10 15:30:10,003:INFO:Declaring metric variables
2024-07-10 15:30:10,004:INFO:Importing untrained model
2024-07-10 15:30:10,006:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 15:30:10,008:INFO:Starting cross validation
2024-07-10 15:30:10,009:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:10,956:INFO:Calculating mean and std
2024-07-10 15:30:10,956:INFO:Creating metrics dataframe
2024-07-10 15:30:10,957:INFO:Uploading results into container
2024-07-10 15:30:10,957:INFO:Uploading model into container now
2024-07-10 15:30:10,957:INFO:_master_model_container: 9
2024-07-10 15:30:10,957:INFO:_display_container: 2
2024-07-10 15:30:10,957:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 15:30:10,958:INFO:create_model() successfully completed......................................
2024-07-10 15:30:11,009:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:11,009:INFO:Creating metrics dataframe
2024-07-10 15:30:11,012:INFO:Initializing Huber Regressor
2024-07-10 15:30:11,012:INFO:Total runtime is 0.16839743455251058 minutes
2024-07-10 15:30:11,014:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:11,014:INFO:Initializing create_model()
2024-07-10 15:30:11,014:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:11,014:INFO:Checking exceptions
2024-07-10 15:30:11,014:INFO:Importing libraries
2024-07-10 15:30:11,014:INFO:Copying training dataset
2024-07-10 15:30:11,018:INFO:Defining folds
2024-07-10 15:30:11,019:INFO:Declaring metric variables
2024-07-10 15:30:11,019:INFO:Importing untrained model
2024-07-10 15:30:11,021:INFO:Huber Regressor Imported successfully
2024-07-10 15:30:11,022:INFO:Starting cross validation
2024-07-10 15:30:11,024:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:12,256:INFO:Calculating mean and std
2024-07-10 15:30:12,257:INFO:Creating metrics dataframe
2024-07-10 15:30:12,258:INFO:Uploading results into container
2024-07-10 15:30:12,258:INFO:Uploading model into container now
2024-07-10 15:30:12,258:INFO:_master_model_container: 10
2024-07-10 15:30:12,258:INFO:_display_container: 2
2024-07-10 15:30:12,258:INFO:HuberRegressor()
2024-07-10 15:30:12,258:INFO:create_model() successfully completed......................................
2024-07-10 15:30:12,310:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:12,310:INFO:Creating metrics dataframe
2024-07-10 15:30:12,313:INFO:Initializing K Neighbors Regressor
2024-07-10 15:30:12,313:INFO:Total runtime is 0.1900768518447876 minutes
2024-07-10 15:30:12,314:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:12,314:INFO:Initializing create_model()
2024-07-10 15:30:12,314:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:12,315:INFO:Checking exceptions
2024-07-10 15:30:12,315:INFO:Importing libraries
2024-07-10 15:30:12,315:INFO:Copying training dataset
2024-07-10 15:30:12,320:INFO:Defining folds
2024-07-10 15:30:12,320:INFO:Declaring metric variables
2024-07-10 15:30:12,321:INFO:Importing untrained model
2024-07-10 15:30:12,322:INFO:K Neighbors Regressor Imported successfully
2024-07-10 15:30:12,324:INFO:Starting cross validation
2024-07-10 15:30:12,325:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:13,368:INFO:Calculating mean and std
2024-07-10 15:30:13,368:INFO:Creating metrics dataframe
2024-07-10 15:30:13,369:INFO:Uploading results into container
2024-07-10 15:30:13,369:INFO:Uploading model into container now
2024-07-10 15:30:13,369:INFO:_master_model_container: 11
2024-07-10 15:30:13,369:INFO:_display_container: 2
2024-07-10 15:30:13,369:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 15:30:13,369:INFO:create_model() successfully completed......................................
2024-07-10 15:30:13,421:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:13,421:INFO:Creating metrics dataframe
2024-07-10 15:30:13,425:INFO:Initializing Decision Tree Regressor
2024-07-10 15:30:13,425:INFO:Total runtime is 0.20860668818155925 minutes
2024-07-10 15:30:13,426:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:13,426:INFO:Initializing create_model()
2024-07-10 15:30:13,426:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:13,426:INFO:Checking exceptions
2024-07-10 15:30:13,426:INFO:Importing libraries
2024-07-10 15:30:13,426:INFO:Copying training dataset
2024-07-10 15:30:13,431:INFO:Defining folds
2024-07-10 15:30:13,431:INFO:Declaring metric variables
2024-07-10 15:30:13,432:INFO:Importing untrained model
2024-07-10 15:30:13,433:INFO:Decision Tree Regressor Imported successfully
2024-07-10 15:30:13,435:INFO:Starting cross validation
2024-07-10 15:30:13,436:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:14,469:INFO:Calculating mean and std
2024-07-10 15:30:14,469:INFO:Creating metrics dataframe
2024-07-10 15:30:14,470:INFO:Uploading results into container
2024-07-10 15:30:14,470:INFO:Uploading model into container now
2024-07-10 15:30:14,470:INFO:_master_model_container: 12
2024-07-10 15:30:14,470:INFO:_display_container: 2
2024-07-10 15:30:14,470:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 15:30:14,471:INFO:create_model() successfully completed......................................
2024-07-10 15:30:14,522:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:14,522:INFO:Creating metrics dataframe
2024-07-10 15:30:14,526:INFO:Initializing Random Forest Regressor
2024-07-10 15:30:14,526:INFO:Total runtime is 0.22695206801096598 minutes
2024-07-10 15:30:14,527:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:14,527:INFO:Initializing create_model()
2024-07-10 15:30:14,527:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:14,527:INFO:Checking exceptions
2024-07-10 15:30:14,527:INFO:Importing libraries
2024-07-10 15:30:14,527:INFO:Copying training dataset
2024-07-10 15:30:14,532:INFO:Defining folds
2024-07-10 15:30:14,532:INFO:Declaring metric variables
2024-07-10 15:30:14,533:INFO:Importing untrained model
2024-07-10 15:30:14,534:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:30:14,536:INFO:Starting cross validation
2024-07-10 15:30:14,537:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:22,083:INFO:Calculating mean and std
2024-07-10 15:30:22,083:INFO:Creating metrics dataframe
2024-07-10 15:30:22,084:INFO:Uploading results into container
2024-07-10 15:30:22,085:INFO:Uploading model into container now
2024-07-10 15:30:22,085:INFO:_master_model_container: 13
2024-07-10 15:30:22,085:INFO:_display_container: 2
2024-07-10 15:30:22,085:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:30:22,085:INFO:create_model() successfully completed......................................
2024-07-10 15:30:22,138:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:22,138:INFO:Creating metrics dataframe
2024-07-10 15:30:22,142:INFO:Initializing Extra Trees Regressor
2024-07-10 15:30:22,142:INFO:Total runtime is 0.35389148791631064 minutes
2024-07-10 15:30:22,143:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:22,143:INFO:Initializing create_model()
2024-07-10 15:30:22,144:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:22,144:INFO:Checking exceptions
2024-07-10 15:30:22,144:INFO:Importing libraries
2024-07-10 15:30:22,144:INFO:Copying training dataset
2024-07-10 15:30:22,148:INFO:Defining folds
2024-07-10 15:30:22,148:INFO:Declaring metric variables
2024-07-10 15:30:22,149:INFO:Importing untrained model
2024-07-10 15:30:22,150:INFO:Extra Trees Regressor Imported successfully
2024-07-10 15:30:22,152:INFO:Starting cross validation
2024-07-10 15:30:22,153:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:29,855:INFO:Calculating mean and std
2024-07-10 15:30:29,857:INFO:Creating metrics dataframe
2024-07-10 15:30:29,858:INFO:Uploading results into container
2024-07-10 15:30:29,858:INFO:Uploading model into container now
2024-07-10 15:30:29,858:INFO:_master_model_container: 14
2024-07-10 15:30:29,858:INFO:_display_container: 2
2024-07-10 15:30:29,858:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:30:29,858:INFO:create_model() successfully completed......................................
2024-07-10 15:30:29,915:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:29,915:INFO:Creating metrics dataframe
2024-07-10 15:30:29,919:INFO:Initializing AdaBoost Regressor
2024-07-10 15:30:29,919:INFO:Total runtime is 0.4835010687510173 minutes
2024-07-10 15:30:29,920:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:29,920:INFO:Initializing create_model()
2024-07-10 15:30:29,920:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:29,920:INFO:Checking exceptions
2024-07-10 15:30:29,920:INFO:Importing libraries
2024-07-10 15:30:29,920:INFO:Copying training dataset
2024-07-10 15:30:29,925:INFO:Defining folds
2024-07-10 15:30:29,925:INFO:Declaring metric variables
2024-07-10 15:30:29,926:INFO:Importing untrained model
2024-07-10 15:30:29,927:INFO:AdaBoost Regressor Imported successfully
2024-07-10 15:30:29,929:INFO:Starting cross validation
2024-07-10 15:30:29,930:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:32,215:INFO:Calculating mean and std
2024-07-10 15:30:32,215:INFO:Creating metrics dataframe
2024-07-10 15:30:32,216:INFO:Uploading results into container
2024-07-10 15:30:32,216:INFO:Uploading model into container now
2024-07-10 15:30:32,216:INFO:_master_model_container: 15
2024-07-10 15:30:32,217:INFO:_display_container: 2
2024-07-10 15:30:32,217:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 15:30:32,217:INFO:create_model() successfully completed......................................
2024-07-10 15:30:32,269:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:32,269:INFO:Creating metrics dataframe
2024-07-10 15:30:32,273:INFO:Initializing Gradient Boosting Regressor
2024-07-10 15:30:32,273:INFO:Total runtime is 0.5227353851000468 minutes
2024-07-10 15:30:32,274:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:32,274:INFO:Initializing create_model()
2024-07-10 15:30:32,274:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:32,274:INFO:Checking exceptions
2024-07-10 15:30:32,274:INFO:Importing libraries
2024-07-10 15:30:32,274:INFO:Copying training dataset
2024-07-10 15:30:32,279:INFO:Defining folds
2024-07-10 15:30:32,279:INFO:Declaring metric variables
2024-07-10 15:30:32,280:INFO:Importing untrained model
2024-07-10 15:30:32,281:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 15:30:32,283:INFO:Starting cross validation
2024-07-10 15:30:32,284:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:35,443:INFO:Calculating mean and std
2024-07-10 15:30:35,444:INFO:Creating metrics dataframe
2024-07-10 15:30:35,444:INFO:Uploading results into container
2024-07-10 15:30:35,445:INFO:Uploading model into container now
2024-07-10 15:30:35,445:INFO:_master_model_container: 16
2024-07-10 15:30:35,445:INFO:_display_container: 2
2024-07-10 15:30:35,445:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 15:30:35,445:INFO:create_model() successfully completed......................................
2024-07-10 15:30:35,497:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:35,497:INFO:Creating metrics dataframe
2024-07-10 15:30:35,501:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 15:30:35,501:INFO:Total runtime is 0.5765467882156372 minutes
2024-07-10 15:30:35,503:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:35,503:INFO:Initializing create_model()
2024-07-10 15:30:35,503:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:35,503:INFO:Checking exceptions
2024-07-10 15:30:35,503:INFO:Importing libraries
2024-07-10 15:30:35,503:INFO:Copying training dataset
2024-07-10 15:30:35,508:INFO:Defining folds
2024-07-10 15:30:35,508:INFO:Declaring metric variables
2024-07-10 15:30:35,509:INFO:Importing untrained model
2024-07-10 15:30:35,510:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:30:35,512:INFO:Starting cross validation
2024-07-10 15:30:35,513:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:39,027:INFO:Calculating mean and std
2024-07-10 15:30:39,028:INFO:Creating metrics dataframe
2024-07-10 15:30:39,029:INFO:Uploading results into container
2024-07-10 15:30:39,029:INFO:Uploading model into container now
2024-07-10 15:30:39,029:INFO:_master_model_container: 17
2024-07-10 15:30:39,029:INFO:_display_container: 2
2024-07-10 15:30:39,030:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:30:39,030:INFO:create_model() successfully completed......................................
2024-07-10 15:30:39,082:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:39,082:INFO:Creating metrics dataframe
2024-07-10 15:30:39,086:INFO:Initializing Dummy Regressor
2024-07-10 15:30:39,086:INFO:Total runtime is 0.6362969875335693 minutes
2024-07-10 15:30:39,088:INFO:SubProcess create_model() called ==================================
2024-07-10 15:30:39,088:INFO:Initializing create_model()
2024-07-10 15:30:39,088:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x11893c190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:39,088:INFO:Checking exceptions
2024-07-10 15:30:39,088:INFO:Importing libraries
2024-07-10 15:30:39,088:INFO:Copying training dataset
2024-07-10 15:30:39,092:INFO:Defining folds
2024-07-10 15:30:39,092:INFO:Declaring metric variables
2024-07-10 15:30:39,093:INFO:Importing untrained model
2024-07-10 15:30:39,094:INFO:Dummy Regressor Imported successfully
2024-07-10 15:30:39,096:INFO:Starting cross validation
2024-07-10 15:30:39,097:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:30:40,048:INFO:Calculating mean and std
2024-07-10 15:30:40,049:INFO:Creating metrics dataframe
2024-07-10 15:30:40,050:INFO:Uploading results into container
2024-07-10 15:30:40,050:INFO:Uploading model into container now
2024-07-10 15:30:40,050:INFO:_master_model_container: 18
2024-07-10 15:30:40,050:INFO:_display_container: 2
2024-07-10 15:30:40,051:INFO:DummyRegressor()
2024-07-10 15:30:40,051:INFO:create_model() successfully completed......................................
2024-07-10 15:30:40,110:INFO:SubProcess create_model() end ==================================
2024-07-10 15:30:40,110:INFO:Creating metrics dataframe
2024-07-10 15:30:40,116:INFO:Initializing create_model()
2024-07-10 15:30:40,116:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121fc5ed0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:30:40,116:INFO:Checking exceptions
2024-07-10 15:30:40,117:INFO:Importing libraries
2024-07-10 15:30:40,117:INFO:Copying training dataset
2024-07-10 15:30:40,122:INFO:Defining folds
2024-07-10 15:30:40,122:INFO:Declaring metric variables
2024-07-10 15:30:40,122:INFO:Importing untrained model
2024-07-10 15:30:40,122:INFO:Declaring custom model
2024-07-10 15:30:40,122:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:30:40,123:INFO:Cross validation set to False
2024-07-10 15:30:40,123:INFO:Fitting Model
2024-07-10 15:30:41,246:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:30:41,246:INFO:create_model() successfully completed......................................
2024-07-10 15:30:41,308:INFO:_master_model_container: 18
2024-07-10 15:30:41,308:INFO:_display_container: 2
2024-07-10 15:30:41,308:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:30:41,308:INFO:compare_models() successfully completed......................................
2024-07-10 15:31:05,463:INFO:PyCaret RegressionExperiment
2024-07-10 15:31:05,463:INFO:Logging name: reg-default-name
2024-07-10 15:31:05,463:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:31:05,463:INFO:version 3.3.1
2024-07-10 15:31:05,463:INFO:Initializing setup()
2024-07-10 15:31:05,463:INFO:self.USI: 8db6
2024-07-10 15:31:05,463:INFO:self._variable_keys: {'idx', 'y_train', 'log_plots_param', 'seed', 'logging_param', 'gpu_param', 'exp_id', 'fold_groups_param', 'n_jobs_param', 'pipeline', 'y', 'transform_target_param', 'fold_generator', '_available_plots', 'y_test', 'X', 'X_train', 'fold_shuffle_param', 'memory', 'USI', 'gpu_n_jobs_param', 'target_param', 'exp_name_log', 'html_param', 'X_test', 'data', '_ml_usecase'}
2024-07-10 15:31:05,463:INFO:Checking environment
2024-07-10 15:31:05,463:INFO:python_version: 3.11.8
2024-07-10 15:31:05,463:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:31:05,463:INFO:machine: arm64
2024-07-10 15:31:05,463:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:31:05,463:INFO:Memory: svmem(total=17179869184, available=4384030720, percent=74.5, used=5847564288, free=155287552, active=4239507456, inactive=4228464640, wired=1608056832)
2024-07-10 15:31:05,464:INFO:Physical Core: 8
2024-07-10 15:31:05,464:INFO:Logical Core: 8
2024-07-10 15:31:05,464:INFO:Checking libraries
2024-07-10 15:31:05,464:INFO:System:
2024-07-10 15:31:05,464:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:31:05,464:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:31:05,464:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:31:05,464:INFO:PyCaret required dependencies:
2024-07-10 15:31:05,464:INFO:                 pip: 23.3.1
2024-07-10 15:31:05,464:INFO:          setuptools: 68.2.2
2024-07-10 15:31:05,464:INFO:             pycaret: 3.3.1
2024-07-10 15:31:05,464:INFO:             IPython: 8.20.0
2024-07-10 15:31:05,464:INFO:          ipywidgets: 7.6.5
2024-07-10 15:31:05,464:INFO:                tqdm: 4.65.0
2024-07-10 15:31:05,464:INFO:               numpy: 1.26.4
2024-07-10 15:31:05,464:INFO:              pandas: 2.1.4
2024-07-10 15:31:05,464:INFO:              jinja2: 3.1.3
2024-07-10 15:31:05,464:INFO:               scipy: 1.11.4
2024-07-10 15:31:05,464:INFO:              joblib: 1.2.0
2024-07-10 15:31:05,464:INFO:             sklearn: 1.4.2
2024-07-10 15:31:05,464:INFO:                pyod: 1.1.3
2024-07-10 15:31:05,464:INFO:            imblearn: 0.12.2
2024-07-10 15:31:05,464:INFO:   category_encoders: 2.6.3
2024-07-10 15:31:05,464:INFO:            lightgbm: 4.3.0
2024-07-10 15:31:05,464:INFO:               numba: 0.59.0
2024-07-10 15:31:05,464:INFO:            requests: 2.31.0
2024-07-10 15:31:05,464:INFO:          matplotlib: 3.8.0
2024-07-10 15:31:05,464:INFO:          scikitplot: 0.3.7
2024-07-10 15:31:05,464:INFO:         yellowbrick: 1.5
2024-07-10 15:31:05,464:INFO:              plotly: 5.22.0
2024-07-10 15:31:05,464:INFO:    plotly-resampler: Not installed
2024-07-10 15:31:05,464:INFO:             kaleido: 0.2.1
2024-07-10 15:31:05,465:INFO:           schemdraw: 0.15
2024-07-10 15:31:05,465:INFO:         statsmodels: 0.14.0
2024-07-10 15:31:05,465:INFO:              sktime: 0.26.0
2024-07-10 15:31:05,465:INFO:               tbats: 1.1.3
2024-07-10 15:31:05,465:INFO:            pmdarima: 2.0.4
2024-07-10 15:31:05,465:INFO:              psutil: 5.9.0
2024-07-10 15:31:05,465:INFO:          markupsafe: 2.1.3
2024-07-10 15:31:05,465:INFO:             pickle5: Not installed
2024-07-10 15:31:05,465:INFO:         cloudpickle: 2.2.1
2024-07-10 15:31:05,465:INFO:         deprecation: 2.1.0
2024-07-10 15:31:05,465:INFO:              xxhash: 3.4.1
2024-07-10 15:31:05,465:INFO:           wurlitzer: 3.0.2
2024-07-10 15:31:05,465:INFO:PyCaret optional dependencies:
2024-07-10 15:31:05,465:INFO:                shap: Not installed
2024-07-10 15:31:05,465:INFO:           interpret: Not installed
2024-07-10 15:31:05,465:INFO:                umap: 0.5.5
2024-07-10 15:31:05,465:INFO:     ydata_profiling: Not installed
2024-07-10 15:31:05,465:INFO:  explainerdashboard: Not installed
2024-07-10 15:31:05,465:INFO:             autoviz: Not installed
2024-07-10 15:31:05,465:INFO:           fairlearn: Not installed
2024-07-10 15:31:05,465:INFO:          deepchecks: Not installed
2024-07-10 15:31:05,465:INFO:             xgboost: Not installed
2024-07-10 15:31:05,465:INFO:            catboost: Not installed
2024-07-10 15:31:05,465:INFO:              kmodes: Not installed
2024-07-10 15:31:05,465:INFO:             mlxtend: Not installed
2024-07-10 15:31:05,465:INFO:       statsforecast: Not installed
2024-07-10 15:31:05,465:INFO:        tune_sklearn: Not installed
2024-07-10 15:31:05,465:INFO:                 ray: Not installed
2024-07-10 15:31:05,465:INFO:            hyperopt: Not installed
2024-07-10 15:31:05,466:INFO:              optuna: Not installed
2024-07-10 15:31:05,466:INFO:               skopt: Not installed
2024-07-10 15:31:05,466:INFO:              mlflow: 2.13.0
2024-07-10 15:31:05,466:INFO:              gradio: Not installed
2024-07-10 15:31:05,466:INFO:             fastapi: Not installed
2024-07-10 15:31:05,466:INFO:             uvicorn: Not installed
2024-07-10 15:31:05,466:INFO:              m2cgen: Not installed
2024-07-10 15:31:05,466:INFO:           evidently: Not installed
2024-07-10 15:31:05,466:INFO:               fugue: Not installed
2024-07-10 15:31:05,466:INFO:           streamlit: 1.30.0
2024-07-10 15:31:05,466:INFO:             prophet: Not installed
2024-07-10 15:31:05,466:INFO:None
2024-07-10 15:31:05,466:INFO:Set up data.
2024-07-10 15:31:05,488:INFO:Set up folding strategy.
2024-07-10 15:31:05,488:INFO:Set up train/test split.
2024-07-10 15:31:05,498:INFO:Set up index.
2024-07-10 15:31:05,498:INFO:Assigning column types.
2024-07-10 15:31:05,503:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:31:05,503:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,506:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,511:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,548:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,570:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,570:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,570:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,570:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,572:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,575:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,603:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,621:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,621:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,621:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:31:05,623:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,625:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,650:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,668:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,668:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,670:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,672:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,697:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,714:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,715:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,715:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,715:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:31:05,718:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,743:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,761:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,761:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,761:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,765:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,790:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,807:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,807:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,807:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,808:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:31:05,836:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,853:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,853:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,881:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,899:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,899:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,899:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,899:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:31:05,927:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,945:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,945:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,974:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:31:05,991:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,991:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:05,992:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:31:06,037:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,037:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,084:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,084:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,086:INFO:Preparing preprocessing pipeline...
2024-07-10 15:31:06,086:INFO:Set up simple imputation.
2024-07-10 15:31:06,089:INFO:Set up encoding of categorical features.
2024-07-10 15:31:06,089:INFO:Set up removing multicollinearity.
2024-07-10 15:31:06,089:INFO:Set up column transformation.
2024-07-10 15:31:06,089:INFO:Set up feature normalization.
2024-07-10 15:31:06,460:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:31:06,464:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms', 'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitted_wardrobes', 'has_lift',
                                             'is_exterior', 'has_parking',
                                             'is_...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:31:06,464:INFO:Creating final display dataframe.
2024-07-10 15:31:06,685:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (16343, 26)
4        Transformed data shape        (16343, 65)
5   Transformed train set shape        (11440, 65)
6    Transformed test set shape         (4903, 65)
7               Ignore features                  6
8              Numeric features                 14
9          Categorical features                  5
10     Rows with missing values              65.0%
11                   Preprocess               True
12              Imputation type             simple
13           Numeric imputation               mean
14       Categorical imputation               mode
15     Maximum one-hot encoding                 25
16              Encoding method               None
17     Remove multicollinearity               True
18  Multicollinearity threshold                0.9
19               Transformation               True
20        Transformation method        yeo-johnson
21                    Normalize               True
22             Normalize method             zscore
23               Fold Generator              KFold
24                  Fold Number                 10
25                     CPU Jobs                 -1
26                      Use GPU              False
27               Log Experiment              False
28              Experiment Name   reg-default-name
29                          USI               8db6
2024-07-10 15:31:06,734:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,734:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,781:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,781:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:31:06,782:INFO:setup() successfully completed in 1.33s...............
2024-07-10 15:31:06,792:INFO:Initializing compare_models()
2024-07-10 15:31:06,792:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 15:31:06,792:INFO:Checking exceptions
2024-07-10 15:31:06,795:INFO:Preparing display monitor
2024-07-10 15:31:06,804:INFO:Initializing Linear Regression
2024-07-10 15:31:06,804:INFO:Total runtime is 2.7298927307128905e-06 minutes
2024-07-10 15:31:06,805:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:06,805:INFO:Initializing create_model()
2024-07-10 15:31:06,805:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:06,805:INFO:Checking exceptions
2024-07-10 15:31:06,805:INFO:Importing libraries
2024-07-10 15:31:06,806:INFO:Copying training dataset
2024-07-10 15:31:06,811:INFO:Defining folds
2024-07-10 15:31:06,811:INFO:Declaring metric variables
2024-07-10 15:31:06,812:INFO:Importing untrained model
2024-07-10 15:31:06,813:INFO:Linear Regression Imported successfully
2024-07-10 15:31:06,815:INFO:Starting cross validation
2024-07-10 15:31:06,817:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:07,830:INFO:Calculating mean and std
2024-07-10 15:31:07,830:INFO:Creating metrics dataframe
2024-07-10 15:31:07,831:INFO:Uploading results into container
2024-07-10 15:31:07,831:INFO:Uploading model into container now
2024-07-10 15:31:07,831:INFO:_master_model_container: 1
2024-07-10 15:31:07,832:INFO:_display_container: 2
2024-07-10 15:31:07,832:INFO:LinearRegression(n_jobs=-1)
2024-07-10 15:31:07,832:INFO:create_model() successfully completed......................................
2024-07-10 15:31:07,885:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:07,885:INFO:Creating metrics dataframe
2024-07-10 15:31:07,887:INFO:Initializing Lasso Regression
2024-07-10 15:31:07,887:INFO:Total runtime is 0.01805419921875 minutes
2024-07-10 15:31:07,888:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:07,888:INFO:Initializing create_model()
2024-07-10 15:31:07,888:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:07,888:INFO:Checking exceptions
2024-07-10 15:31:07,889:INFO:Importing libraries
2024-07-10 15:31:07,889:INFO:Copying training dataset
2024-07-10 15:31:07,893:INFO:Defining folds
2024-07-10 15:31:07,893:INFO:Declaring metric variables
2024-07-10 15:31:07,894:INFO:Importing untrained model
2024-07-10 15:31:07,895:INFO:Lasso Regression Imported successfully
2024-07-10 15:31:07,897:INFO:Starting cross validation
2024-07-10 15:31:07,898:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:09,623:INFO:Calculating mean and std
2024-07-10 15:31:09,624:INFO:Creating metrics dataframe
2024-07-10 15:31:09,624:INFO:Uploading results into container
2024-07-10 15:31:09,625:INFO:Uploading model into container now
2024-07-10 15:31:09,625:INFO:_master_model_container: 2
2024-07-10 15:31:09,625:INFO:_display_container: 2
2024-07-10 15:31:09,625:INFO:Lasso(random_state=9)
2024-07-10 15:31:09,625:INFO:create_model() successfully completed......................................
2024-07-10 15:31:09,678:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:09,678:INFO:Creating metrics dataframe
2024-07-10 15:31:09,680:INFO:Initializing Ridge Regression
2024-07-10 15:31:09,680:INFO:Total runtime is 0.04794078270594279 minutes
2024-07-10 15:31:09,681:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:09,682:INFO:Initializing create_model()
2024-07-10 15:31:09,682:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:09,682:INFO:Checking exceptions
2024-07-10 15:31:09,682:INFO:Importing libraries
2024-07-10 15:31:09,682:INFO:Copying training dataset
2024-07-10 15:31:09,686:INFO:Defining folds
2024-07-10 15:31:09,686:INFO:Declaring metric variables
2024-07-10 15:31:09,687:INFO:Importing untrained model
2024-07-10 15:31:09,689:INFO:Ridge Regression Imported successfully
2024-07-10 15:31:09,690:INFO:Starting cross validation
2024-07-10 15:31:09,692:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:10,674:INFO:Calculating mean and std
2024-07-10 15:31:10,675:INFO:Creating metrics dataframe
2024-07-10 15:31:10,675:INFO:Uploading results into container
2024-07-10 15:31:10,676:INFO:Uploading model into container now
2024-07-10 15:31:10,676:INFO:_master_model_container: 3
2024-07-10 15:31:10,676:INFO:_display_container: 2
2024-07-10 15:31:10,676:INFO:Ridge(random_state=9)
2024-07-10 15:31:10,676:INFO:create_model() successfully completed......................................
2024-07-10 15:31:10,732:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:10,732:INFO:Creating metrics dataframe
2024-07-10 15:31:10,735:INFO:Initializing Elastic Net
2024-07-10 15:31:10,735:INFO:Total runtime is 0.06551949580510458 minutes
2024-07-10 15:31:10,736:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:10,736:INFO:Initializing create_model()
2024-07-10 15:31:10,736:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:10,737:INFO:Checking exceptions
2024-07-10 15:31:10,737:INFO:Importing libraries
2024-07-10 15:31:10,737:INFO:Copying training dataset
2024-07-10 15:31:10,741:INFO:Defining folds
2024-07-10 15:31:10,741:INFO:Declaring metric variables
2024-07-10 15:31:10,742:INFO:Importing untrained model
2024-07-10 15:31:10,744:INFO:Elastic Net Imported successfully
2024-07-10 15:31:10,746:INFO:Starting cross validation
2024-07-10 15:31:10,747:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:11,849:INFO:Calculating mean and std
2024-07-10 15:31:11,850:INFO:Creating metrics dataframe
2024-07-10 15:31:11,851:INFO:Uploading results into container
2024-07-10 15:31:11,851:INFO:Uploading model into container now
2024-07-10 15:31:11,851:INFO:_master_model_container: 4
2024-07-10 15:31:11,851:INFO:_display_container: 2
2024-07-10 15:31:11,851:INFO:ElasticNet(random_state=9)
2024-07-10 15:31:11,851:INFO:create_model() successfully completed......................................
2024-07-10 15:31:11,903:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:11,903:INFO:Creating metrics dataframe
2024-07-10 15:31:11,906:INFO:Initializing Least Angle Regression
2024-07-10 15:31:11,906:INFO:Total runtime is 0.08503594398498536 minutes
2024-07-10 15:31:11,907:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:11,907:INFO:Initializing create_model()
2024-07-10 15:31:11,907:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:11,907:INFO:Checking exceptions
2024-07-10 15:31:11,907:INFO:Importing libraries
2024-07-10 15:31:11,908:INFO:Copying training dataset
2024-07-10 15:31:11,912:INFO:Defining folds
2024-07-10 15:31:11,912:INFO:Declaring metric variables
2024-07-10 15:31:11,913:INFO:Importing untrained model
2024-07-10 15:31:11,914:INFO:Least Angle Regression Imported successfully
2024-07-10 15:31:11,916:INFO:Starting cross validation
2024-07-10 15:31:11,917:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:12,922:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.566e+01, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:31:12,922:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.241e+01, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:31:12,922:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=8.101e+00, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:31:12,922:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=5.285e+00, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:31:12,923:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.921e-01, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:31:12,938:INFO:Calculating mean and std
2024-07-10 15:31:12,938:INFO:Creating metrics dataframe
2024-07-10 15:31:12,939:INFO:Uploading results into container
2024-07-10 15:31:12,939:INFO:Uploading model into container now
2024-07-10 15:31:12,940:INFO:_master_model_container: 5
2024-07-10 15:31:12,940:INFO:_display_container: 2
2024-07-10 15:31:12,940:INFO:Lars(random_state=9)
2024-07-10 15:31:12,940:INFO:create_model() successfully completed......................................
2024-07-10 15:31:12,995:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:12,995:INFO:Creating metrics dataframe
2024-07-10 15:31:12,998:INFO:Initializing Lasso Least Angle Regression
2024-07-10 15:31:12,998:INFO:Total runtime is 0.10323249896367392 minutes
2024-07-10 15:31:12,999:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:12,999:INFO:Initializing create_model()
2024-07-10 15:31:12,999:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:12,999:INFO:Checking exceptions
2024-07-10 15:31:12,999:INFO:Importing libraries
2024-07-10 15:31:12,999:INFO:Copying training dataset
2024-07-10 15:31:13,004:INFO:Defining folds
2024-07-10 15:31:13,004:INFO:Declaring metric variables
2024-07-10 15:31:13,005:INFO:Importing untrained model
2024-07-10 15:31:13,007:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 15:31:13,009:INFO:Starting cross validation
2024-07-10 15:31:13,010:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:14,028:INFO:Calculating mean and std
2024-07-10 15:31:14,029:INFO:Creating metrics dataframe
2024-07-10 15:31:14,029:INFO:Uploading results into container
2024-07-10 15:31:14,029:INFO:Uploading model into container now
2024-07-10 15:31:14,030:INFO:_master_model_container: 6
2024-07-10 15:31:14,030:INFO:_display_container: 2
2024-07-10 15:31:14,030:INFO:LassoLars(random_state=9)
2024-07-10 15:31:14,030:INFO:create_model() successfully completed......................................
2024-07-10 15:31:14,082:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:14,082:INFO:Creating metrics dataframe
2024-07-10 15:31:14,085:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 15:31:14,085:INFO:Total runtime is 0.12135452826817832 minutes
2024-07-10 15:31:14,086:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:14,086:INFO:Initializing create_model()
2024-07-10 15:31:14,086:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:14,086:INFO:Checking exceptions
2024-07-10 15:31:14,086:INFO:Importing libraries
2024-07-10 15:31:14,087:INFO:Copying training dataset
2024-07-10 15:31:14,091:INFO:Defining folds
2024-07-10 15:31:14,091:INFO:Declaring metric variables
2024-07-10 15:31:14,092:INFO:Importing untrained model
2024-07-10 15:31:14,093:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 15:31:14,095:INFO:Starting cross validation
2024-07-10 15:31:14,096:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:15,108:INFO:Calculating mean and std
2024-07-10 15:31:15,109:INFO:Creating metrics dataframe
2024-07-10 15:31:15,110:INFO:Uploading results into container
2024-07-10 15:31:15,110:INFO:Uploading model into container now
2024-07-10 15:31:15,110:INFO:_master_model_container: 7
2024-07-10 15:31:15,110:INFO:_display_container: 2
2024-07-10 15:31:15,110:INFO:OrthogonalMatchingPursuit()
2024-07-10 15:31:15,110:INFO:create_model() successfully completed......................................
2024-07-10 15:31:15,162:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:15,162:INFO:Creating metrics dataframe
2024-07-10 15:31:15,165:INFO:Initializing Bayesian Ridge
2024-07-10 15:31:15,165:INFO:Total runtime is 0.13935891389846802 minutes
2024-07-10 15:31:15,166:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:15,167:INFO:Initializing create_model()
2024-07-10 15:31:15,167:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:15,167:INFO:Checking exceptions
2024-07-10 15:31:15,167:INFO:Importing libraries
2024-07-10 15:31:15,167:INFO:Copying training dataset
2024-07-10 15:31:15,171:INFO:Defining folds
2024-07-10 15:31:15,171:INFO:Declaring metric variables
2024-07-10 15:31:15,172:INFO:Importing untrained model
2024-07-10 15:31:15,174:INFO:Bayesian Ridge Imported successfully
2024-07-10 15:31:15,176:INFO:Starting cross validation
2024-07-10 15:31:15,177:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:16,205:INFO:Calculating mean and std
2024-07-10 15:31:16,205:INFO:Creating metrics dataframe
2024-07-10 15:31:16,206:INFO:Uploading results into container
2024-07-10 15:31:16,206:INFO:Uploading model into container now
2024-07-10 15:31:16,207:INFO:_master_model_container: 8
2024-07-10 15:31:16,207:INFO:_display_container: 2
2024-07-10 15:31:16,207:INFO:BayesianRidge()
2024-07-10 15:31:16,207:INFO:create_model() successfully completed......................................
2024-07-10 15:31:16,259:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:16,259:INFO:Creating metrics dataframe
2024-07-10 15:31:16,262:INFO:Initializing Passive Aggressive Regressor
2024-07-10 15:31:16,262:INFO:Total runtime is 0.15763484636942546 minutes
2024-07-10 15:31:16,263:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:16,263:INFO:Initializing create_model()
2024-07-10 15:31:16,263:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:16,263:INFO:Checking exceptions
2024-07-10 15:31:16,263:INFO:Importing libraries
2024-07-10 15:31:16,263:INFO:Copying training dataset
2024-07-10 15:31:16,268:INFO:Defining folds
2024-07-10 15:31:16,268:INFO:Declaring metric variables
2024-07-10 15:31:16,269:INFO:Importing untrained model
2024-07-10 15:31:16,270:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 15:31:16,273:INFO:Starting cross validation
2024-07-10 15:31:16,274:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:17,367:INFO:Calculating mean and std
2024-07-10 15:31:17,368:INFO:Creating metrics dataframe
2024-07-10 15:31:17,369:INFO:Uploading results into container
2024-07-10 15:31:17,369:INFO:Uploading model into container now
2024-07-10 15:31:17,369:INFO:_master_model_container: 9
2024-07-10 15:31:17,369:INFO:_display_container: 2
2024-07-10 15:31:17,369:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 15:31:17,369:INFO:create_model() successfully completed......................................
2024-07-10 15:31:17,421:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:17,421:INFO:Creating metrics dataframe
2024-07-10 15:31:17,425:INFO:Initializing Huber Regressor
2024-07-10 15:31:17,425:INFO:Total runtime is 0.1770151972770691 minutes
2024-07-10 15:31:17,426:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:17,426:INFO:Initializing create_model()
2024-07-10 15:31:17,426:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:17,426:INFO:Checking exceptions
2024-07-10 15:31:17,426:INFO:Importing libraries
2024-07-10 15:31:17,426:INFO:Copying training dataset
2024-07-10 15:31:17,431:INFO:Defining folds
2024-07-10 15:31:17,431:INFO:Declaring metric variables
2024-07-10 15:31:17,432:INFO:Importing untrained model
2024-07-10 15:31:17,433:INFO:Huber Regressor Imported successfully
2024-07-10 15:31:17,435:INFO:Starting cross validation
2024-07-10 15:31:17,436:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:18,766:INFO:Calculating mean and std
2024-07-10 15:31:18,767:INFO:Creating metrics dataframe
2024-07-10 15:31:18,768:INFO:Uploading results into container
2024-07-10 15:31:18,768:INFO:Uploading model into container now
2024-07-10 15:31:18,768:INFO:_master_model_container: 10
2024-07-10 15:31:18,768:INFO:_display_container: 2
2024-07-10 15:31:18,769:INFO:HuberRegressor()
2024-07-10 15:31:18,769:INFO:create_model() successfully completed......................................
2024-07-10 15:31:18,821:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:18,821:INFO:Creating metrics dataframe
2024-07-10 15:31:18,825:INFO:Initializing K Neighbors Regressor
2024-07-10 15:31:18,825:INFO:Total runtime is 0.20034921169281006 minutes
2024-07-10 15:31:18,826:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:18,826:INFO:Initializing create_model()
2024-07-10 15:31:18,826:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:18,826:INFO:Checking exceptions
2024-07-10 15:31:18,826:INFO:Importing libraries
2024-07-10 15:31:18,826:INFO:Copying training dataset
2024-07-10 15:31:18,831:INFO:Defining folds
2024-07-10 15:31:18,831:INFO:Declaring metric variables
2024-07-10 15:31:18,832:INFO:Importing untrained model
2024-07-10 15:31:18,833:INFO:K Neighbors Regressor Imported successfully
2024-07-10 15:31:18,835:INFO:Starting cross validation
2024-07-10 15:31:18,836:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:19,971:INFO:Calculating mean and std
2024-07-10 15:31:19,971:INFO:Creating metrics dataframe
2024-07-10 15:31:19,972:INFO:Uploading results into container
2024-07-10 15:31:19,972:INFO:Uploading model into container now
2024-07-10 15:31:19,973:INFO:_master_model_container: 11
2024-07-10 15:31:19,973:INFO:_display_container: 2
2024-07-10 15:31:19,973:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 15:31:19,973:INFO:create_model() successfully completed......................................
2024-07-10 15:31:20,027:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:20,028:INFO:Creating metrics dataframe
2024-07-10 15:31:20,031:INFO:Initializing Decision Tree Regressor
2024-07-10 15:31:20,031:INFO:Total runtime is 0.2204566995302836 minutes
2024-07-10 15:31:20,032:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:20,032:INFO:Initializing create_model()
2024-07-10 15:31:20,033:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:20,033:INFO:Checking exceptions
2024-07-10 15:31:20,033:INFO:Importing libraries
2024-07-10 15:31:20,033:INFO:Copying training dataset
2024-07-10 15:31:20,038:INFO:Defining folds
2024-07-10 15:31:20,038:INFO:Declaring metric variables
2024-07-10 15:31:20,039:INFO:Importing untrained model
2024-07-10 15:31:20,040:INFO:Decision Tree Regressor Imported successfully
2024-07-10 15:31:20,042:INFO:Starting cross validation
2024-07-10 15:31:20,043:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:21,190:INFO:Calculating mean and std
2024-07-10 15:31:21,190:INFO:Creating metrics dataframe
2024-07-10 15:31:21,191:INFO:Uploading results into container
2024-07-10 15:31:21,191:INFO:Uploading model into container now
2024-07-10 15:31:21,191:INFO:_master_model_container: 12
2024-07-10 15:31:21,191:INFO:_display_container: 2
2024-07-10 15:31:21,192:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 15:31:21,192:INFO:create_model() successfully completed......................................
2024-07-10 15:31:21,245:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:21,245:INFO:Creating metrics dataframe
2024-07-10 15:31:21,249:INFO:Initializing Random Forest Regressor
2024-07-10 15:31:21,249:INFO:Total runtime is 0.24075371424357095 minutes
2024-07-10 15:31:21,250:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:21,250:INFO:Initializing create_model()
2024-07-10 15:31:21,250:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:21,250:INFO:Checking exceptions
2024-07-10 15:31:21,251:INFO:Importing libraries
2024-07-10 15:31:21,251:INFO:Copying training dataset
2024-07-10 15:31:21,255:INFO:Defining folds
2024-07-10 15:31:21,255:INFO:Declaring metric variables
2024-07-10 15:31:21,256:INFO:Importing untrained model
2024-07-10 15:31:21,257:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:31:21,259:INFO:Starting cross validation
2024-07-10 15:31:21,260:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:28,824:INFO:Calculating mean and std
2024-07-10 15:31:28,825:INFO:Creating metrics dataframe
2024-07-10 15:31:28,826:INFO:Uploading results into container
2024-07-10 15:31:28,826:INFO:Uploading model into container now
2024-07-10 15:31:28,827:INFO:_master_model_container: 13
2024-07-10 15:31:28,827:INFO:_display_container: 2
2024-07-10 15:31:28,827:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:31:28,827:INFO:create_model() successfully completed......................................
2024-07-10 15:31:28,880:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:28,880:INFO:Creating metrics dataframe
2024-07-10 15:31:28,883:INFO:Initializing Extra Trees Regressor
2024-07-10 15:31:28,883:INFO:Total runtime is 0.36799281438191733 minutes
2024-07-10 15:31:28,884:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:28,885:INFO:Initializing create_model()
2024-07-10 15:31:28,885:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:28,885:INFO:Checking exceptions
2024-07-10 15:31:28,885:INFO:Importing libraries
2024-07-10 15:31:28,885:INFO:Copying training dataset
2024-07-10 15:31:28,889:INFO:Defining folds
2024-07-10 15:31:28,889:INFO:Declaring metric variables
2024-07-10 15:31:28,890:INFO:Importing untrained model
2024-07-10 15:31:28,892:INFO:Extra Trees Regressor Imported successfully
2024-07-10 15:31:28,893:INFO:Starting cross validation
2024-07-10 15:31:28,895:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:36,805:INFO:Calculating mean and std
2024-07-10 15:31:36,805:INFO:Creating metrics dataframe
2024-07-10 15:31:36,806:INFO:Uploading results into container
2024-07-10 15:31:36,807:INFO:Uploading model into container now
2024-07-10 15:31:36,807:INFO:_master_model_container: 14
2024-07-10 15:31:36,807:INFO:_display_container: 2
2024-07-10 15:31:36,807:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:31:36,807:INFO:create_model() successfully completed......................................
2024-07-10 15:31:36,860:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:36,860:INFO:Creating metrics dataframe
2024-07-10 15:31:36,864:INFO:Initializing AdaBoost Regressor
2024-07-10 15:31:36,864:INFO:Total runtime is 0.5010083317756653 minutes
2024-07-10 15:31:36,865:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:36,866:INFO:Initializing create_model()
2024-07-10 15:31:36,866:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:36,866:INFO:Checking exceptions
2024-07-10 15:31:36,866:INFO:Importing libraries
2024-07-10 15:31:36,866:INFO:Copying training dataset
2024-07-10 15:31:36,871:INFO:Defining folds
2024-07-10 15:31:36,871:INFO:Declaring metric variables
2024-07-10 15:31:36,872:INFO:Importing untrained model
2024-07-10 15:31:36,873:INFO:AdaBoost Regressor Imported successfully
2024-07-10 15:31:36,875:INFO:Starting cross validation
2024-07-10 15:31:36,876:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:38,908:INFO:Calculating mean and std
2024-07-10 15:31:38,909:INFO:Creating metrics dataframe
2024-07-10 15:31:38,909:INFO:Uploading results into container
2024-07-10 15:31:38,910:INFO:Uploading model into container now
2024-07-10 15:31:38,910:INFO:_master_model_container: 15
2024-07-10 15:31:38,910:INFO:_display_container: 2
2024-07-10 15:31:38,910:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 15:31:38,910:INFO:create_model() successfully completed......................................
2024-07-10 15:31:38,962:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:38,962:INFO:Creating metrics dataframe
2024-07-10 15:31:38,966:INFO:Initializing Gradient Boosting Regressor
2024-07-10 15:31:38,966:INFO:Total runtime is 0.5360411961873373 minutes
2024-07-10 15:31:38,967:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:38,968:INFO:Initializing create_model()
2024-07-10 15:31:38,968:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:38,968:INFO:Checking exceptions
2024-07-10 15:31:38,968:INFO:Importing libraries
2024-07-10 15:31:38,968:INFO:Copying training dataset
2024-07-10 15:31:38,972:INFO:Defining folds
2024-07-10 15:31:38,972:INFO:Declaring metric variables
2024-07-10 15:31:38,973:INFO:Importing untrained model
2024-07-10 15:31:38,975:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 15:31:38,976:INFO:Starting cross validation
2024-07-10 15:31:38,978:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:42,173:INFO:Calculating mean and std
2024-07-10 15:31:42,174:INFO:Creating metrics dataframe
2024-07-10 15:31:42,175:INFO:Uploading results into container
2024-07-10 15:31:42,175:INFO:Uploading model into container now
2024-07-10 15:31:42,175:INFO:_master_model_container: 16
2024-07-10 15:31:42,175:INFO:_display_container: 2
2024-07-10 15:31:42,175:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 15:31:42,175:INFO:create_model() successfully completed......................................
2024-07-10 15:31:42,232:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:42,232:INFO:Creating metrics dataframe
2024-07-10 15:31:42,237:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 15:31:42,237:INFO:Total runtime is 0.5905483643213908 minutes
2024-07-10 15:31:42,238:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:42,238:INFO:Initializing create_model()
2024-07-10 15:31:42,238:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:42,238:INFO:Checking exceptions
2024-07-10 15:31:42,238:INFO:Importing libraries
2024-07-10 15:31:42,238:INFO:Copying training dataset
2024-07-10 15:31:42,243:INFO:Defining folds
2024-07-10 15:31:42,243:INFO:Declaring metric variables
2024-07-10 15:31:42,244:INFO:Importing untrained model
2024-07-10 15:31:42,246:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:31:42,248:INFO:Starting cross validation
2024-07-10 15:31:42,249:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:46,007:INFO:Calculating mean and std
2024-07-10 15:31:46,007:INFO:Creating metrics dataframe
2024-07-10 15:31:46,008:INFO:Uploading results into container
2024-07-10 15:31:46,008:INFO:Uploading model into container now
2024-07-10 15:31:46,009:INFO:_master_model_container: 17
2024-07-10 15:31:46,009:INFO:_display_container: 2
2024-07-10 15:31:46,009:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:31:46,009:INFO:create_model() successfully completed......................................
2024-07-10 15:31:46,061:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:46,061:INFO:Creating metrics dataframe
2024-07-10 15:31:46,065:INFO:Initializing Dummy Regressor
2024-07-10 15:31:46,065:INFO:Total runtime is 0.6543588479359945 minutes
2024-07-10 15:31:46,066:INFO:SubProcess create_model() called ==================================
2024-07-10 15:31:46,067:INFO:Initializing create_model()
2024-07-10 15:31:46,067:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x133b8bb50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:46,067:INFO:Checking exceptions
2024-07-10 15:31:46,067:INFO:Importing libraries
2024-07-10 15:31:46,067:INFO:Copying training dataset
2024-07-10 15:31:46,071:INFO:Defining folds
2024-07-10 15:31:46,071:INFO:Declaring metric variables
2024-07-10 15:31:46,072:INFO:Importing untrained model
2024-07-10 15:31:46,073:INFO:Dummy Regressor Imported successfully
2024-07-10 15:31:46,075:INFO:Starting cross validation
2024-07-10 15:31:46,077:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:31:47,168:INFO:Calculating mean and std
2024-07-10 15:31:47,168:INFO:Creating metrics dataframe
2024-07-10 15:31:47,169:INFO:Uploading results into container
2024-07-10 15:31:47,169:INFO:Uploading model into container now
2024-07-10 15:31:47,170:INFO:_master_model_container: 18
2024-07-10 15:31:47,170:INFO:_display_container: 2
2024-07-10 15:31:47,170:INFO:DummyRegressor()
2024-07-10 15:31:47,170:INFO:create_model() successfully completed......................................
2024-07-10 15:31:47,227:INFO:SubProcess create_model() end ==================================
2024-07-10 15:31:47,227:INFO:Creating metrics dataframe
2024-07-10 15:31:47,233:INFO:Initializing create_model()
2024-07-10 15:31:47,234:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x121b205d0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:31:47,234:INFO:Checking exceptions
2024-07-10 15:31:47,234:INFO:Importing libraries
2024-07-10 15:31:47,234:INFO:Copying training dataset
2024-07-10 15:31:47,239:INFO:Defining folds
2024-07-10 15:31:47,239:INFO:Declaring metric variables
2024-07-10 15:31:47,239:INFO:Importing untrained model
2024-07-10 15:31:47,239:INFO:Declaring custom model
2024-07-10 15:31:47,239:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:31:47,240:INFO:Cross validation set to False
2024-07-10 15:31:47,240:INFO:Fitting Model
2024-07-10 15:31:48,366:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:31:48,366:INFO:create_model() successfully completed......................................
2024-07-10 15:31:48,434:INFO:_master_model_container: 18
2024-07-10 15:31:48,434:INFO:_display_container: 2
2024-07-10 15:31:48,434:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:31:48,434:INFO:compare_models() successfully completed......................................
2024-07-10 15:32:24,827:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:32:24,827:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:32:24,827:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:32:24,827:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:32:24,974:INFO:PyCaret RegressionExperiment
2024-07-10 15:32:24,974:INFO:Logging name: reg-default-name
2024-07-10 15:32:24,974:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:32:24,974:INFO:version 3.3.1
2024-07-10 15:32:24,974:INFO:Initializing setup()
2024-07-10 15:32:24,974:INFO:self.USI: 1992
2024-07-10 15:32:24,974:INFO:self._variable_keys: {'X_test', 'transform_target_param', 'exp_id', 'pipeline', 'exp_name_log', 'USI', 'html_param', 'data', 'memory', 'gpu_param', 'target_param', 'log_plots_param', 'seed', 'fold_generator', 'X', 'fold_shuffle_param', 'n_jobs_param', 'gpu_n_jobs_param', 'fold_groups_param', 'y', 'idx', 'X_train', 'y_train', 'logging_param', '_available_plots', 'y_test', '_ml_usecase'}
2024-07-10 15:32:24,974:INFO:Checking environment
2024-07-10 15:32:24,974:INFO:python_version: 3.11.8
2024-07-10 15:32:24,974:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:32:24,974:INFO:machine: arm64
2024-07-10 15:32:24,974:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:32:24,974:INFO:Memory: svmem(total=17179869184, available=6329647104, percent=63.2, used=4323639296, free=2455175168, active=2592391168, inactive=3752771584, wired=1731248128)
2024-07-10 15:32:24,974:INFO:Physical Core: 8
2024-07-10 15:32:24,974:INFO:Logical Core: 8
2024-07-10 15:32:24,974:INFO:Checking libraries
2024-07-10 15:32:24,974:INFO:System:
2024-07-10 15:32:24,974:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:32:24,974:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:32:24,974:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:32:24,974:INFO:PyCaret required dependencies:
2024-07-10 15:32:25,210:INFO:                 pip: 23.3.1
2024-07-10 15:32:25,210:INFO:          setuptools: 68.2.2
2024-07-10 15:32:25,211:INFO:             pycaret: 3.3.1
2024-07-10 15:32:25,211:INFO:             IPython: 8.20.0
2024-07-10 15:32:25,211:INFO:          ipywidgets: 7.6.5
2024-07-10 15:32:25,211:INFO:                tqdm: 4.65.0
2024-07-10 15:32:25,211:INFO:               numpy: 1.26.4
2024-07-10 15:32:25,211:INFO:              pandas: 2.1.4
2024-07-10 15:32:25,211:INFO:              jinja2: 3.1.3
2024-07-10 15:32:25,211:INFO:               scipy: 1.11.4
2024-07-10 15:32:25,211:INFO:              joblib: 1.2.0
2024-07-10 15:32:25,211:INFO:             sklearn: 1.4.2
2024-07-10 15:32:25,211:INFO:                pyod: 1.1.3
2024-07-10 15:32:25,211:INFO:            imblearn: 0.12.2
2024-07-10 15:32:25,211:INFO:   category_encoders: 2.6.3
2024-07-10 15:32:25,211:INFO:            lightgbm: 4.3.0
2024-07-10 15:32:25,211:INFO:               numba: 0.59.0
2024-07-10 15:32:25,211:INFO:            requests: 2.31.0
2024-07-10 15:32:25,211:INFO:          matplotlib: 3.8.0
2024-07-10 15:32:25,211:INFO:          scikitplot: 0.3.7
2024-07-10 15:32:25,211:INFO:         yellowbrick: 1.5
2024-07-10 15:32:25,211:INFO:              plotly: 5.22.0
2024-07-10 15:32:25,211:INFO:    plotly-resampler: Not installed
2024-07-10 15:32:25,211:INFO:             kaleido: 0.2.1
2024-07-10 15:32:25,211:INFO:           schemdraw: 0.15
2024-07-10 15:32:25,211:INFO:         statsmodels: 0.14.0
2024-07-10 15:32:25,211:INFO:              sktime: 0.26.0
2024-07-10 15:32:25,211:INFO:               tbats: 1.1.3
2024-07-10 15:32:25,211:INFO:            pmdarima: 2.0.4
2024-07-10 15:32:25,211:INFO:              psutil: 5.9.0
2024-07-10 15:32:25,211:INFO:          markupsafe: 2.1.3
2024-07-10 15:32:25,211:INFO:             pickle5: Not installed
2024-07-10 15:32:25,211:INFO:         cloudpickle: 2.2.1
2024-07-10 15:32:25,211:INFO:         deprecation: 2.1.0
2024-07-10 15:32:25,211:INFO:              xxhash: 3.4.1
2024-07-10 15:32:25,211:INFO:           wurlitzer: 3.0.2
2024-07-10 15:32:25,211:INFO:PyCaret optional dependencies:
2024-07-10 15:32:25,215:INFO:                shap: Not installed
2024-07-10 15:32:25,215:INFO:           interpret: Not installed
2024-07-10 15:32:25,215:INFO:                umap: 0.5.5
2024-07-10 15:32:25,215:INFO:     ydata_profiling: Not installed
2024-07-10 15:32:25,215:INFO:  explainerdashboard: Not installed
2024-07-10 15:32:25,215:INFO:             autoviz: Not installed
2024-07-10 15:32:25,216:INFO:           fairlearn: Not installed
2024-07-10 15:32:25,216:INFO:          deepchecks: Not installed
2024-07-10 15:32:25,216:INFO:             xgboost: Not installed
2024-07-10 15:32:25,216:INFO:            catboost: Not installed
2024-07-10 15:32:25,216:INFO:              kmodes: Not installed
2024-07-10 15:32:25,216:INFO:             mlxtend: Not installed
2024-07-10 15:32:25,216:INFO:       statsforecast: Not installed
2024-07-10 15:32:25,216:INFO:        tune_sklearn: Not installed
2024-07-10 15:32:25,216:INFO:                 ray: Not installed
2024-07-10 15:32:25,216:INFO:            hyperopt: Not installed
2024-07-10 15:32:25,216:INFO:              optuna: Not installed
2024-07-10 15:32:25,216:INFO:               skopt: Not installed
2024-07-10 15:32:25,216:INFO:              mlflow: 2.13.0
2024-07-10 15:32:25,216:INFO:              gradio: Not installed
2024-07-10 15:32:25,216:INFO:             fastapi: Not installed
2024-07-10 15:32:25,216:INFO:             uvicorn: Not installed
2024-07-10 15:32:25,216:INFO:              m2cgen: Not installed
2024-07-10 15:32:25,216:INFO:           evidently: Not installed
2024-07-10 15:32:25,216:INFO:               fugue: Not installed
2024-07-10 15:32:25,216:INFO:           streamlit: 1.30.0
2024-07-10 15:32:25,216:INFO:             prophet: Not installed
2024-07-10 15:32:25,216:INFO:None
2024-07-10 15:32:25,216:INFO:Set up data.
2024-07-10 15:32:25,225:INFO:Set up folding strategy.
2024-07-10 15:32:25,225:INFO:Set up train/test split.
2024-07-10 15:32:25,231:INFO:Set up index.
2024-07-10 15:32:25,232:INFO:Assigning column types.
2024-07-10 15:32:25,236:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:32:25,236:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,237:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,239:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,267:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,286:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,287:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,287:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,287:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,289:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,291:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,320:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,339:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,339:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,339:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,339:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:32:25,341:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,343:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,372:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,391:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,391:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,391:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,393:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,395:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,425:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,443:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,444:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,444:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,444:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:32:25,447:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,474:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,491:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,492:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,492:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,495:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,521:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,538:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,538:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,539:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:32:25,568:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,586:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,586:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,586:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,616:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,634:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,634:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,634:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,634:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:32:25,663:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,681:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,681:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,711:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:32:25,730:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,730:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,730:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:32:25,777:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,777:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,824:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,824:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:25,826:INFO:Preparing preprocessing pipeline...
2024-07-10 15:32:25,826:INFO:Set up simple imputation.
2024-07-10 15:32:25,829:INFO:Set up encoding of categorical features.
2024-07-10 15:32:25,829:INFO:Set up removing multicollinearity.
2024-07-10 15:32:25,829:INFO:Set up column transformation.
2024-07-10 15:32:25,829:INFO:Set up feature normalization.
2024-07-10 15:32:26,007:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:32:26,012:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms',
                                             'is_exact_address_hidden',
                                             'is_floor_under', 'rent_price',
                                             'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitte...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:32:26,012:INFO:Creating final display dataframe.
2024-07-10 15:32:26,250:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape        (16343, 26)
4        Transformed data shape        (16343, 70)
5   Transformed train set shape        (11440, 70)
6    Transformed test set shape         (4903, 70)
7              Numeric features                 20
8          Categorical features                  5
9      Rows with missing values              65.0%
10                   Preprocess               True
11              Imputation type             simple
12           Numeric imputation               mean
13       Categorical imputation               mode
14     Maximum one-hot encoding                 25
15              Encoding method               None
16     Remove multicollinearity               True
17  Multicollinearity threshold                0.9
18               Transformation               True
19        Transformation method        yeo-johnson
20                    Normalize               True
21             Normalize method             zscore
22               Fold Generator              KFold
23                  Fold Number                 10
24                     CPU Jobs                 -1
25                      Use GPU              False
26               Log Experiment              False
27              Experiment Name   reg-default-name
28                          USI               1992
2024-07-10 15:32:26,300:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:26,300:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:26,347:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:26,347:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:32:26,347:INFO:setup() successfully completed in 1.38s...............
2024-07-10 15:32:26,353:INFO:Initializing compare_models()
2024-07-10 15:32:26,353:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 15:32:26,353:INFO:Checking exceptions
2024-07-10 15:32:26,356:INFO:Preparing display monitor
2024-07-10 15:32:26,379:INFO:Initializing Linear Regression
2024-07-10 15:32:26,379:INFO:Total runtime is 3.564357757568359e-06 minutes
2024-07-10 15:32:26,380:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:26,380:INFO:Initializing create_model()
2024-07-10 15:32:26,380:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:26,380:INFO:Checking exceptions
2024-07-10 15:32:26,381:INFO:Importing libraries
2024-07-10 15:32:26,381:INFO:Copying training dataset
2024-07-10 15:32:26,388:INFO:Defining folds
2024-07-10 15:32:26,388:INFO:Declaring metric variables
2024-07-10 15:32:26,389:INFO:Importing untrained model
2024-07-10 15:32:26,391:INFO:Linear Regression Imported successfully
2024-07-10 15:32:26,393:INFO:Starting cross validation
2024-07-10 15:32:26,397:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:29,550:INFO:Calculating mean and std
2024-07-10 15:32:29,551:INFO:Creating metrics dataframe
2024-07-10 15:32:29,552:INFO:Uploading results into container
2024-07-10 15:32:29,552:INFO:Uploading model into container now
2024-07-10 15:32:29,552:INFO:_master_model_container: 1
2024-07-10 15:32:29,552:INFO:_display_container: 2
2024-07-10 15:32:29,552:INFO:LinearRegression(n_jobs=-1)
2024-07-10 15:32:29,552:INFO:create_model() successfully completed......................................
2024-07-10 15:32:29,612:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:29,612:INFO:Creating metrics dataframe
2024-07-10 15:32:29,615:INFO:Initializing Lasso Regression
2024-07-10 15:32:29,615:INFO:Total runtime is 0.05394638379414877 minutes
2024-07-10 15:32:29,616:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:29,617:INFO:Initializing create_model()
2024-07-10 15:32:29,617:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:29,617:INFO:Checking exceptions
2024-07-10 15:32:29,617:INFO:Importing libraries
2024-07-10 15:32:29,617:INFO:Copying training dataset
2024-07-10 15:32:29,623:INFO:Defining folds
2024-07-10 15:32:29,623:INFO:Declaring metric variables
2024-07-10 15:32:29,624:INFO:Importing untrained model
2024-07-10 15:32:29,625:INFO:Lasso Regression Imported successfully
2024-07-10 15:32:29,627:INFO:Starting cross validation
2024-07-10 15:32:29,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:31,187:INFO:Calculating mean and std
2024-07-10 15:32:31,187:INFO:Creating metrics dataframe
2024-07-10 15:32:31,188:INFO:Uploading results into container
2024-07-10 15:32:31,189:INFO:Uploading model into container now
2024-07-10 15:32:31,189:INFO:_master_model_container: 2
2024-07-10 15:32:31,189:INFO:_display_container: 2
2024-07-10 15:32:31,189:INFO:Lasso(random_state=9)
2024-07-10 15:32:31,189:INFO:create_model() successfully completed......................................
2024-07-10 15:32:31,246:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:31,247:INFO:Creating metrics dataframe
2024-07-10 15:32:31,249:INFO:Initializing Ridge Regression
2024-07-10 15:32:31,249:INFO:Total runtime is 0.08118021488189697 minutes
2024-07-10 15:32:31,250:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:31,251:INFO:Initializing create_model()
2024-07-10 15:32:31,251:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:31,251:INFO:Checking exceptions
2024-07-10 15:32:31,251:INFO:Importing libraries
2024-07-10 15:32:31,251:INFO:Copying training dataset
2024-07-10 15:32:31,257:INFO:Defining folds
2024-07-10 15:32:31,257:INFO:Declaring metric variables
2024-07-10 15:32:31,258:INFO:Importing untrained model
2024-07-10 15:32:31,259:INFO:Ridge Regression Imported successfully
2024-07-10 15:32:31,261:INFO:Starting cross validation
2024-07-10 15:32:31,263:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:32,276:INFO:Calculating mean and std
2024-07-10 15:32:32,276:INFO:Creating metrics dataframe
2024-07-10 15:32:32,277:INFO:Uploading results into container
2024-07-10 15:32:32,277:INFO:Uploading model into container now
2024-07-10 15:32:32,277:INFO:_master_model_container: 3
2024-07-10 15:32:32,278:INFO:_display_container: 2
2024-07-10 15:32:32,278:INFO:Ridge(random_state=9)
2024-07-10 15:32:32,278:INFO:create_model() successfully completed......................................
2024-07-10 15:32:32,328:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:32,328:INFO:Creating metrics dataframe
2024-07-10 15:32:32,331:INFO:Initializing Elastic Net
2024-07-10 15:32:32,331:INFO:Total runtime is 0.0992121974627177 minutes
2024-07-10 15:32:32,332:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:32,332:INFO:Initializing create_model()
2024-07-10 15:32:32,333:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:32,333:INFO:Checking exceptions
2024-07-10 15:32:32,333:INFO:Importing libraries
2024-07-10 15:32:32,333:INFO:Copying training dataset
2024-07-10 15:32:32,339:INFO:Defining folds
2024-07-10 15:32:32,339:INFO:Declaring metric variables
2024-07-10 15:32:32,340:INFO:Importing untrained model
2024-07-10 15:32:32,341:INFO:Elastic Net Imported successfully
2024-07-10 15:32:32,343:INFO:Starting cross validation
2024-07-10 15:32:32,344:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:33,401:INFO:Calculating mean and std
2024-07-10 15:32:33,401:INFO:Creating metrics dataframe
2024-07-10 15:32:33,402:INFO:Uploading results into container
2024-07-10 15:32:33,402:INFO:Uploading model into container now
2024-07-10 15:32:33,403:INFO:_master_model_container: 4
2024-07-10 15:32:33,403:INFO:_display_container: 2
2024-07-10 15:32:33,403:INFO:ElasticNet(random_state=9)
2024-07-10 15:32:33,403:INFO:create_model() successfully completed......................................
2024-07-10 15:32:33,455:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:33,455:INFO:Creating metrics dataframe
2024-07-10 15:32:33,458:INFO:Initializing Least Angle Regression
2024-07-10 15:32:33,458:INFO:Total runtime is 0.11799689531326295 minutes
2024-07-10 15:32:33,459:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:33,460:INFO:Initializing create_model()
2024-07-10 15:32:33,460:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:33,460:INFO:Checking exceptions
2024-07-10 15:32:33,460:INFO:Importing libraries
2024-07-10 15:32:33,460:INFO:Copying training dataset
2024-07-10 15:32:33,466:INFO:Defining folds
2024-07-10 15:32:33,466:INFO:Declaring metric variables
2024-07-10 15:32:33,467:INFO:Importing untrained model
2024-07-10 15:32:33,468:INFO:Least Angle Regression Imported successfully
2024-07-10 15:32:33,470:INFO:Starting cross validation
2024-07-10 15:32:33,471:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:34,508:INFO:Calculating mean and std
2024-07-10 15:32:34,509:INFO:Creating metrics dataframe
2024-07-10 15:32:34,510:INFO:Uploading results into container
2024-07-10 15:32:34,510:INFO:Uploading model into container now
2024-07-10 15:32:34,510:INFO:_master_model_container: 5
2024-07-10 15:32:34,510:INFO:_display_container: 2
2024-07-10 15:32:34,510:INFO:Lars(random_state=9)
2024-07-10 15:32:34,510:INFO:create_model() successfully completed......................................
2024-07-10 15:32:34,562:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:34,562:INFO:Creating metrics dataframe
2024-07-10 15:32:34,565:INFO:Initializing Lasso Least Angle Regression
2024-07-10 15:32:34,565:INFO:Total runtime is 0.13644485076268514 minutes
2024-07-10 15:32:34,566:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:34,566:INFO:Initializing create_model()
2024-07-10 15:32:34,567:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:34,567:INFO:Checking exceptions
2024-07-10 15:32:34,567:INFO:Importing libraries
2024-07-10 15:32:34,567:INFO:Copying training dataset
2024-07-10 15:32:34,573:INFO:Defining folds
2024-07-10 15:32:34,573:INFO:Declaring metric variables
2024-07-10 15:32:34,574:INFO:Importing untrained model
2024-07-10 15:32:34,575:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 15:32:34,577:INFO:Starting cross validation
2024-07-10 15:32:34,578:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:35,623:INFO:Calculating mean and std
2024-07-10 15:32:35,623:INFO:Creating metrics dataframe
2024-07-10 15:32:35,624:INFO:Uploading results into container
2024-07-10 15:32:35,624:INFO:Uploading model into container now
2024-07-10 15:32:35,624:INFO:_master_model_container: 6
2024-07-10 15:32:35,624:INFO:_display_container: 2
2024-07-10 15:32:35,624:INFO:LassoLars(random_state=9)
2024-07-10 15:32:35,625:INFO:create_model() successfully completed......................................
2024-07-10 15:32:35,676:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:35,677:INFO:Creating metrics dataframe
2024-07-10 15:32:35,679:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 15:32:35,680:INFO:Total runtime is 0.15501689910888672 minutes
2024-07-10 15:32:35,681:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:35,681:INFO:Initializing create_model()
2024-07-10 15:32:35,681:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:35,681:INFO:Checking exceptions
2024-07-10 15:32:35,681:INFO:Importing libraries
2024-07-10 15:32:35,681:INFO:Copying training dataset
2024-07-10 15:32:35,687:INFO:Defining folds
2024-07-10 15:32:35,687:INFO:Declaring metric variables
2024-07-10 15:32:35,688:INFO:Importing untrained model
2024-07-10 15:32:35,689:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 15:32:35,691:INFO:Starting cross validation
2024-07-10 15:32:35,693:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:36,785:INFO:Calculating mean and std
2024-07-10 15:32:36,786:INFO:Creating metrics dataframe
2024-07-10 15:32:36,786:INFO:Uploading results into container
2024-07-10 15:32:36,787:INFO:Uploading model into container now
2024-07-10 15:32:36,787:INFO:_master_model_container: 7
2024-07-10 15:32:36,787:INFO:_display_container: 2
2024-07-10 15:32:36,787:INFO:OrthogonalMatchingPursuit()
2024-07-10 15:32:36,787:INFO:create_model() successfully completed......................................
2024-07-10 15:32:36,838:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:36,839:INFO:Creating metrics dataframe
2024-07-10 15:32:36,842:INFO:Initializing Bayesian Ridge
2024-07-10 15:32:36,842:INFO:Total runtime is 0.1743857463200887 minutes
2024-07-10 15:32:36,843:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:36,843:INFO:Initializing create_model()
2024-07-10 15:32:36,843:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:36,843:INFO:Checking exceptions
2024-07-10 15:32:36,843:INFO:Importing libraries
2024-07-10 15:32:36,843:INFO:Copying training dataset
2024-07-10 15:32:36,849:INFO:Defining folds
2024-07-10 15:32:36,849:INFO:Declaring metric variables
2024-07-10 15:32:36,850:INFO:Importing untrained model
2024-07-10 15:32:36,851:INFO:Bayesian Ridge Imported successfully
2024-07-10 15:32:36,853:INFO:Starting cross validation
2024-07-10 15:32:36,855:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:37,965:INFO:Calculating mean and std
2024-07-10 15:32:37,966:INFO:Creating metrics dataframe
2024-07-10 15:32:37,967:INFO:Uploading results into container
2024-07-10 15:32:37,967:INFO:Uploading model into container now
2024-07-10 15:32:37,967:INFO:_master_model_container: 8
2024-07-10 15:32:37,967:INFO:_display_container: 2
2024-07-10 15:32:37,967:INFO:BayesianRidge()
2024-07-10 15:32:37,967:INFO:create_model() successfully completed......................................
2024-07-10 15:32:38,019:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:38,020:INFO:Creating metrics dataframe
2024-07-10 15:32:38,023:INFO:Initializing Passive Aggressive Regressor
2024-07-10 15:32:38,023:INFO:Total runtime is 0.19407679637273154 minutes
2024-07-10 15:32:38,024:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:38,025:INFO:Initializing create_model()
2024-07-10 15:32:38,025:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:38,025:INFO:Checking exceptions
2024-07-10 15:32:38,025:INFO:Importing libraries
2024-07-10 15:32:38,025:INFO:Copying training dataset
2024-07-10 15:32:38,031:INFO:Defining folds
2024-07-10 15:32:38,031:INFO:Declaring metric variables
2024-07-10 15:32:38,032:INFO:Importing untrained model
2024-07-10 15:32:38,033:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 15:32:38,035:INFO:Starting cross validation
2024-07-10 15:32:38,036:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:39,102:INFO:Calculating mean and std
2024-07-10 15:32:39,103:INFO:Creating metrics dataframe
2024-07-10 15:32:39,104:INFO:Uploading results into container
2024-07-10 15:32:39,104:INFO:Uploading model into container now
2024-07-10 15:32:39,104:INFO:_master_model_container: 9
2024-07-10 15:32:39,104:INFO:_display_container: 2
2024-07-10 15:32:39,104:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 15:32:39,104:INFO:create_model() successfully completed......................................
2024-07-10 15:32:39,157:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:39,157:INFO:Creating metrics dataframe
2024-07-10 15:32:39,160:INFO:Initializing Huber Regressor
2024-07-10 15:32:39,160:INFO:Total runtime is 0.21303194761276245 minutes
2024-07-10 15:32:39,162:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:39,162:INFO:Initializing create_model()
2024-07-10 15:32:39,162:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:39,162:INFO:Checking exceptions
2024-07-10 15:32:39,162:INFO:Importing libraries
2024-07-10 15:32:39,162:INFO:Copying training dataset
2024-07-10 15:32:39,168:INFO:Defining folds
2024-07-10 15:32:39,168:INFO:Declaring metric variables
2024-07-10 15:32:39,169:INFO:Importing untrained model
2024-07-10 15:32:39,170:INFO:Huber Regressor Imported successfully
2024-07-10 15:32:39,172:INFO:Starting cross validation
2024-07-10 15:32:39,173:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:40,584:INFO:Calculating mean and std
2024-07-10 15:32:40,584:INFO:Creating metrics dataframe
2024-07-10 15:32:40,585:INFO:Uploading results into container
2024-07-10 15:32:40,585:INFO:Uploading model into container now
2024-07-10 15:32:40,585:INFO:_master_model_container: 10
2024-07-10 15:32:40,586:INFO:_display_container: 2
2024-07-10 15:32:40,586:INFO:HuberRegressor()
2024-07-10 15:32:40,586:INFO:create_model() successfully completed......................................
2024-07-10 15:32:40,637:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:40,637:INFO:Creating metrics dataframe
2024-07-10 15:32:40,640:INFO:Initializing K Neighbors Regressor
2024-07-10 15:32:40,640:INFO:Total runtime is 0.23769424756368002 minutes
2024-07-10 15:32:40,641:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:40,641:INFO:Initializing create_model()
2024-07-10 15:32:40,641:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:40,641:INFO:Checking exceptions
2024-07-10 15:32:40,641:INFO:Importing libraries
2024-07-10 15:32:40,642:INFO:Copying training dataset
2024-07-10 15:32:40,648:INFO:Defining folds
2024-07-10 15:32:40,648:INFO:Declaring metric variables
2024-07-10 15:32:40,649:INFO:Importing untrained model
2024-07-10 15:32:40,650:INFO:K Neighbors Regressor Imported successfully
2024-07-10 15:32:40,652:INFO:Starting cross validation
2024-07-10 15:32:40,653:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:41,828:INFO:Calculating mean and std
2024-07-10 15:32:41,828:INFO:Creating metrics dataframe
2024-07-10 15:32:41,829:INFO:Uploading results into container
2024-07-10 15:32:41,829:INFO:Uploading model into container now
2024-07-10 15:32:41,829:INFO:_master_model_container: 11
2024-07-10 15:32:41,829:INFO:_display_container: 2
2024-07-10 15:32:41,830:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 15:32:41,830:INFO:create_model() successfully completed......................................
2024-07-10 15:32:41,880:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:41,880:INFO:Creating metrics dataframe
2024-07-10 15:32:41,884:INFO:Initializing Decision Tree Regressor
2024-07-10 15:32:41,884:INFO:Total runtime is 0.25842947959899903 minutes
2024-07-10 15:32:41,885:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:41,886:INFO:Initializing create_model()
2024-07-10 15:32:41,886:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:41,886:INFO:Checking exceptions
2024-07-10 15:32:41,886:INFO:Importing libraries
2024-07-10 15:32:41,886:INFO:Copying training dataset
2024-07-10 15:32:41,892:INFO:Defining folds
2024-07-10 15:32:41,892:INFO:Declaring metric variables
2024-07-10 15:32:41,893:INFO:Importing untrained model
2024-07-10 15:32:41,894:INFO:Decision Tree Regressor Imported successfully
2024-07-10 15:32:41,896:INFO:Starting cross validation
2024-07-10 15:32:41,897:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:43,094:INFO:Calculating mean and std
2024-07-10 15:32:43,095:INFO:Creating metrics dataframe
2024-07-10 15:32:43,096:INFO:Uploading results into container
2024-07-10 15:32:43,096:INFO:Uploading model into container now
2024-07-10 15:32:43,096:INFO:_master_model_container: 12
2024-07-10 15:32:43,096:INFO:_display_container: 2
2024-07-10 15:32:43,096:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 15:32:43,096:INFO:create_model() successfully completed......................................
2024-07-10 15:32:43,147:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:43,147:INFO:Creating metrics dataframe
2024-07-10 15:32:43,151:INFO:Initializing Random Forest Regressor
2024-07-10 15:32:43,151:INFO:Total runtime is 0.27954201300938925 minutes
2024-07-10 15:32:43,152:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:43,152:INFO:Initializing create_model()
2024-07-10 15:32:43,152:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:43,152:INFO:Checking exceptions
2024-07-10 15:32:43,152:INFO:Importing libraries
2024-07-10 15:32:43,152:INFO:Copying training dataset
2024-07-10 15:32:43,158:INFO:Defining folds
2024-07-10 15:32:43,158:INFO:Declaring metric variables
2024-07-10 15:32:43,159:INFO:Importing untrained model
2024-07-10 15:32:43,160:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:32:43,162:INFO:Starting cross validation
2024-07-10 15:32:43,163:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:32:51,957:INFO:Calculating mean and std
2024-07-10 15:32:51,958:INFO:Creating metrics dataframe
2024-07-10 15:32:51,959:INFO:Uploading results into container
2024-07-10 15:32:51,959:INFO:Uploading model into container now
2024-07-10 15:32:51,960:INFO:_master_model_container: 13
2024-07-10 15:32:51,960:INFO:_display_container: 2
2024-07-10 15:32:51,960:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:32:51,960:INFO:create_model() successfully completed......................................
2024-07-10 15:32:52,011:INFO:SubProcess create_model() end ==================================
2024-07-10 15:32:52,011:INFO:Creating metrics dataframe
2024-07-10 15:32:52,015:INFO:Initializing Extra Trees Regressor
2024-07-10 15:32:52,015:INFO:Total runtime is 0.427274481455485 minutes
2024-07-10 15:32:52,016:INFO:SubProcess create_model() called ==================================
2024-07-10 15:32:52,016:INFO:Initializing create_model()
2024-07-10 15:32:52,016:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:32:52,017:INFO:Checking exceptions
2024-07-10 15:32:52,017:INFO:Importing libraries
2024-07-10 15:32:52,017:INFO:Copying training dataset
2024-07-10 15:32:52,022:INFO:Defining folds
2024-07-10 15:32:52,022:INFO:Declaring metric variables
2024-07-10 15:32:52,023:INFO:Importing untrained model
2024-07-10 15:32:52,025:INFO:Extra Trees Regressor Imported successfully
2024-07-10 15:32:52,026:INFO:Starting cross validation
2024-07-10 15:32:52,028:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:33:01,474:INFO:Calculating mean and std
2024-07-10 15:33:01,475:INFO:Creating metrics dataframe
2024-07-10 15:33:01,476:INFO:Uploading results into container
2024-07-10 15:33:01,476:INFO:Uploading model into container now
2024-07-10 15:33:01,476:INFO:_master_model_container: 14
2024-07-10 15:33:01,476:INFO:_display_container: 2
2024-07-10 15:33:01,477:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:33:01,477:INFO:create_model() successfully completed......................................
2024-07-10 15:33:01,528:INFO:SubProcess create_model() end ==================================
2024-07-10 15:33:01,528:INFO:Creating metrics dataframe
2024-07-10 15:33:01,532:INFO:Initializing AdaBoost Regressor
2024-07-10 15:33:01,532:INFO:Total runtime is 0.5858862996101379 minutes
2024-07-10 15:33:01,533:INFO:SubProcess create_model() called ==================================
2024-07-10 15:33:01,533:INFO:Initializing create_model()
2024-07-10 15:33:01,533:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:33:01,533:INFO:Checking exceptions
2024-07-10 15:33:01,533:INFO:Importing libraries
2024-07-10 15:33:01,533:INFO:Copying training dataset
2024-07-10 15:33:01,539:INFO:Defining folds
2024-07-10 15:33:01,539:INFO:Declaring metric variables
2024-07-10 15:33:01,540:INFO:Importing untrained model
2024-07-10 15:33:01,541:INFO:AdaBoost Regressor Imported successfully
2024-07-10 15:33:01,543:INFO:Starting cross validation
2024-07-10 15:33:01,545:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:33:04,429:INFO:Calculating mean and std
2024-07-10 15:33:04,430:INFO:Creating metrics dataframe
2024-07-10 15:33:04,431:INFO:Uploading results into container
2024-07-10 15:33:04,431:INFO:Uploading model into container now
2024-07-10 15:33:04,431:INFO:_master_model_container: 15
2024-07-10 15:33:04,431:INFO:_display_container: 2
2024-07-10 15:33:04,431:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 15:33:04,431:INFO:create_model() successfully completed......................................
2024-07-10 15:33:04,482:INFO:SubProcess create_model() end ==================================
2024-07-10 15:33:04,482:INFO:Creating metrics dataframe
2024-07-10 15:33:04,486:INFO:Initializing Gradient Boosting Regressor
2024-07-10 15:33:04,486:INFO:Total runtime is 0.6351272463798523 minutes
2024-07-10 15:33:04,487:INFO:SubProcess create_model() called ==================================
2024-07-10 15:33:04,487:INFO:Initializing create_model()
2024-07-10 15:33:04,487:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:33:04,488:INFO:Checking exceptions
2024-07-10 15:33:04,488:INFO:Importing libraries
2024-07-10 15:33:04,488:INFO:Copying training dataset
2024-07-10 15:33:04,493:INFO:Defining folds
2024-07-10 15:33:04,494:INFO:Declaring metric variables
2024-07-10 15:33:04,495:INFO:Importing untrained model
2024-07-10 15:33:04,496:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 15:33:04,497:INFO:Starting cross validation
2024-07-10 15:33:04,499:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:33:08,266:INFO:Calculating mean and std
2024-07-10 15:33:08,267:INFO:Creating metrics dataframe
2024-07-10 15:33:08,268:INFO:Uploading results into container
2024-07-10 15:33:08,268:INFO:Uploading model into container now
2024-07-10 15:33:08,268:INFO:_master_model_container: 16
2024-07-10 15:33:08,268:INFO:_display_container: 2
2024-07-10 15:33:08,268:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 15:33:08,269:INFO:create_model() successfully completed......................................
2024-07-10 15:33:08,319:INFO:SubProcess create_model() end ==================================
2024-07-10 15:33:08,319:INFO:Creating metrics dataframe
2024-07-10 15:33:08,323:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 15:33:08,323:INFO:Total runtime is 0.6990784645080567 minutes
2024-07-10 15:33:08,324:INFO:SubProcess create_model() called ==================================
2024-07-10 15:33:08,325:INFO:Initializing create_model()
2024-07-10 15:33:08,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:33:08,325:INFO:Checking exceptions
2024-07-10 15:33:08,325:INFO:Importing libraries
2024-07-10 15:33:08,325:INFO:Copying training dataset
2024-07-10 15:33:08,331:INFO:Defining folds
2024-07-10 15:33:08,331:INFO:Declaring metric variables
2024-07-10 15:33:08,332:INFO:Importing untrained model
2024-07-10 15:33:08,333:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:33:08,335:INFO:Starting cross validation
2024-07-10 15:33:08,336:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:33:11,522:INFO:Calculating mean and std
2024-07-10 15:33:11,523:INFO:Creating metrics dataframe
2024-07-10 15:33:11,523:INFO:Uploading results into container
2024-07-10 15:33:11,524:INFO:Uploading model into container now
2024-07-10 15:33:11,524:INFO:_master_model_container: 17
2024-07-10 15:33:11,524:INFO:_display_container: 2
2024-07-10 15:33:11,524:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:33:11,524:INFO:create_model() successfully completed......................................
2024-07-10 15:33:11,575:INFO:SubProcess create_model() end ==================================
2024-07-10 15:33:11,575:INFO:Creating metrics dataframe
2024-07-10 15:33:11,579:INFO:Initializing Dummy Regressor
2024-07-10 15:33:11,579:INFO:Total runtime is 0.75334286292394 minutes
2024-07-10 15:33:11,580:INFO:SubProcess create_model() called ==================================
2024-07-10 15:33:11,580:INFO:Initializing create_model()
2024-07-10 15:33:11,580:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x14e2c2690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:33:11,580:INFO:Checking exceptions
2024-07-10 15:33:11,581:INFO:Importing libraries
2024-07-10 15:33:11,581:INFO:Copying training dataset
2024-07-10 15:33:11,586:INFO:Defining folds
2024-07-10 15:33:11,586:INFO:Declaring metric variables
2024-07-10 15:33:11,587:INFO:Importing untrained model
2024-07-10 15:33:11,588:INFO:Dummy Regressor Imported successfully
2024-07-10 15:33:11,590:INFO:Starting cross validation
2024-07-10 15:33:11,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:33:12,599:INFO:Calculating mean and std
2024-07-10 15:33:12,599:INFO:Creating metrics dataframe
2024-07-10 15:33:12,600:INFO:Uploading results into container
2024-07-10 15:33:12,600:INFO:Uploading model into container now
2024-07-10 15:33:12,600:INFO:_master_model_container: 18
2024-07-10 15:33:12,600:INFO:_display_container: 2
2024-07-10 15:33:12,601:INFO:DummyRegressor()
2024-07-10 15:33:12,601:INFO:create_model() successfully completed......................................
2024-07-10 15:33:12,651:INFO:SubProcess create_model() end ==================================
2024-07-10 15:33:12,651:INFO:Creating metrics dataframe
2024-07-10 15:33:12,658:INFO:Initializing create_model()
2024-07-10 15:33:12,658:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:33:12,658:INFO:Checking exceptions
2024-07-10 15:33:12,659:INFO:Importing libraries
2024-07-10 15:33:12,659:INFO:Copying training dataset
2024-07-10 15:33:12,665:INFO:Defining folds
2024-07-10 15:33:12,665:INFO:Declaring metric variables
2024-07-10 15:33:12,665:INFO:Importing untrained model
2024-07-10 15:33:12,665:INFO:Declaring custom model
2024-07-10 15:33:12,665:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:33:12,666:INFO:Cross validation set to False
2024-07-10 15:33:12,666:INFO:Fitting Model
2024-07-10 15:33:13,060:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-10 15:33:13,065:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002217 seconds.
2024-07-10 15:33:13,065:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-10 15:33:13,065:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-10 15:33:13,065:INFO:[LightGBM] [Info] Total Bins 968
2024-07-10 15:33:13,065:INFO:[LightGBM] [Info] Number of data points in the train set: 11440, number of used features: 67
2024-07-10 15:33:13,066:INFO:[LightGBM] [Info] Start training from score 3747.781469
2024-07-10 15:33:13,372:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:33:13,372:INFO:create_model() successfully completed......................................
2024-07-10 15:33:13,431:INFO:_master_model_container: 18
2024-07-10 15:33:13,432:INFO:_display_container: 2
2024-07-10 15:33:13,432:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:33:13,432:INFO:compare_models() successfully completed......................................
2024-07-10 15:33:13,434:INFO:Initializing predict_model()
2024-07-10 15:33:13,434:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x14eefb060>)
2024-07-10 15:33:13,434:INFO:Checking exceptions
2024-07-10 15:33:13,434:INFO:Preloading libraries
2024-07-10 15:33:13,435:INFO:Set up data.
2024-07-10 15:33:13,440:INFO:Set up index.
2024-07-10 15:33:13,565:INFO:Initializing plot_model()
2024-07-10 15:33:13,565:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=residuals, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:33:13,566:INFO:Checking exceptions
2024-07-10 15:33:13,569:INFO:Preloading libraries
2024-07-10 15:33:13,572:INFO:Copying training dataset
2024-07-10 15:33:13,572:INFO:Plot type: residuals
2024-07-10 15:33:13,837:INFO:Fitting Model
2024-07-10 15:33:13,876:INFO:Scoring test/hold-out set
2024-07-10 15:33:14,150:INFO:Visual Rendered Successfully
2024-07-10 15:33:14,221:INFO:plot_model() successfully completed......................................
2024-07-10 15:33:14,224:INFO:Initializing plot_model()
2024-07-10 15:33:14,224:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:33:14,224:INFO:Checking exceptions
2024-07-10 15:33:14,227:INFO:Preloading libraries
2024-07-10 15:33:14,229:INFO:Copying training dataset
2024-07-10 15:33:14,229:INFO:Plot type: error
2024-07-10 15:33:14,475:INFO:Fitting Model
2024-07-10 15:33:14,475:INFO:Scoring test/hold-out set
2024-07-10 15:33:14,575:INFO:Visual Rendered Successfully
2024-07-10 15:33:14,632:INFO:plot_model() successfully completed......................................
2024-07-10 15:33:14,634:INFO:Initializing plot_model()
2024-07-10 15:33:14,635:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x14ce07b10>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:33:14,635:INFO:Checking exceptions
2024-07-10 15:33:14,638:INFO:Preloading libraries
2024-07-10 15:33:14,641:INFO:Copying training dataset
2024-07-10 15:33:14,641:INFO:Plot type: feature
2024-07-10 15:33:14,641:WARNING:No coef_ found. Trying feature_importances_
2024-07-10 15:33:14,770:INFO:Visual Rendered Successfully
2024-07-10 15:33:14,825:INFO:plot_model() successfully completed......................................
2024-07-10 15:40:39,980:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:40:39,980:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:40:39,980:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:40:39,980:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:41:43,496:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:41:43,496:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:41:43,496:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:41:43,496:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-07-10 15:41:43,588:INFO:PyCaret RegressionExperiment
2024-07-10 15:41:43,588:INFO:Logging name: reg-default-name
2024-07-10 15:41:43,588:INFO:ML Usecase: MLUsecase.REGRESSION
2024-07-10 15:41:43,588:INFO:version 3.3.1
2024-07-10 15:41:43,588:INFO:Initializing setup()
2024-07-10 15:41:43,588:INFO:self.USI: 3257
2024-07-10 15:41:43,588:INFO:self._variable_keys: {'_available_plots', 'gpu_param', 'seed', 'y', 'y_train', 'data', 'X_test', 'X_train', 'gpu_n_jobs_param', 'X', 'fold_generator', 'transform_target_param', 'log_plots_param', '_ml_usecase', 'pipeline', 'target_param', 'USI', 'logging_param', 'fold_groups_param', 'exp_id', 'idx', 'memory', 'fold_shuffle_param', 'y_test', 'exp_name_log', 'n_jobs_param', 'html_param'}
2024-07-10 15:41:43,588:INFO:Checking environment
2024-07-10 15:41:43,588:INFO:python_version: 3.11.8
2024-07-10 15:41:43,588:INFO:python_build: ('main', 'Feb 16 2024 20:49:36')
2024-07-10 15:41:43,588:INFO:machine: arm64
2024-07-10 15:41:43,588:INFO:platform: macOS-14.5-arm64-arm-64bit
2024-07-10 15:41:43,588:INFO:Memory: svmem(total=17179869184, available=5177393152, percent=69.9, used=6614335488, free=170672128, active=5023432704, inactive=4989272064, wired=1590902784)
2024-07-10 15:41:43,588:INFO:Physical Core: 8
2024-07-10 15:41:43,588:INFO:Logical Core: 8
2024-07-10 15:41:43,588:INFO:Checking libraries
2024-07-10 15:41:43,588:INFO:System:
2024-07-10 15:41:43,588:INFO:    python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]
2024-07-10 15:41:43,588:INFO:executable: /opt/anaconda3/bin/python
2024-07-10 15:41:43,588:INFO:   machine: macOS-14.5-arm64-arm-64bit
2024-07-10 15:41:43,588:INFO:PyCaret required dependencies:
2024-07-10 15:41:43,826:INFO:                 pip: 23.3.1
2024-07-10 15:41:43,826:INFO:          setuptools: 68.2.2
2024-07-10 15:41:43,826:INFO:             pycaret: 3.3.1
2024-07-10 15:41:43,827:INFO:             IPython: 8.20.0
2024-07-10 15:41:43,827:INFO:          ipywidgets: 7.6.5
2024-07-10 15:41:43,827:INFO:                tqdm: 4.65.0
2024-07-10 15:41:43,827:INFO:               numpy: 1.26.4
2024-07-10 15:41:43,827:INFO:              pandas: 2.1.4
2024-07-10 15:41:43,827:INFO:              jinja2: 3.1.3
2024-07-10 15:41:43,827:INFO:               scipy: 1.11.4
2024-07-10 15:41:43,827:INFO:              joblib: 1.2.0
2024-07-10 15:41:43,827:INFO:             sklearn: 1.4.2
2024-07-10 15:41:43,827:INFO:                pyod: 1.1.3
2024-07-10 15:41:43,827:INFO:            imblearn: 0.12.2
2024-07-10 15:41:43,827:INFO:   category_encoders: 2.6.3
2024-07-10 15:41:43,827:INFO:            lightgbm: 4.3.0
2024-07-10 15:41:43,827:INFO:               numba: 0.59.0
2024-07-10 15:41:43,827:INFO:            requests: 2.31.0
2024-07-10 15:41:43,827:INFO:          matplotlib: 3.8.0
2024-07-10 15:41:43,827:INFO:          scikitplot: 0.3.7
2024-07-10 15:41:43,827:INFO:         yellowbrick: 1.5
2024-07-10 15:41:43,827:INFO:              plotly: 5.22.0
2024-07-10 15:41:43,827:INFO:    plotly-resampler: Not installed
2024-07-10 15:41:43,827:INFO:             kaleido: 0.2.1
2024-07-10 15:41:43,827:INFO:           schemdraw: 0.15
2024-07-10 15:41:43,827:INFO:         statsmodels: 0.14.0
2024-07-10 15:41:43,827:INFO:              sktime: 0.26.0
2024-07-10 15:41:43,827:INFO:               tbats: 1.1.3
2024-07-10 15:41:43,827:INFO:            pmdarima: 2.0.4
2024-07-10 15:41:43,827:INFO:              psutil: 5.9.0
2024-07-10 15:41:43,827:INFO:          markupsafe: 2.1.3
2024-07-10 15:41:43,827:INFO:             pickle5: Not installed
2024-07-10 15:41:43,827:INFO:         cloudpickle: 2.2.1
2024-07-10 15:41:43,827:INFO:         deprecation: 2.1.0
2024-07-10 15:41:43,827:INFO:              xxhash: 3.4.1
2024-07-10 15:41:43,827:INFO:           wurlitzer: 3.0.2
2024-07-10 15:41:43,827:INFO:PyCaret optional dependencies:
2024-07-10 15:41:43,832:INFO:                shap: Not installed
2024-07-10 15:41:43,832:INFO:           interpret: Not installed
2024-07-10 15:41:43,832:INFO:                umap: 0.5.5
2024-07-10 15:41:43,832:INFO:     ydata_profiling: Not installed
2024-07-10 15:41:43,832:INFO:  explainerdashboard: Not installed
2024-07-10 15:41:43,832:INFO:             autoviz: Not installed
2024-07-10 15:41:43,832:INFO:           fairlearn: Not installed
2024-07-10 15:41:43,832:INFO:          deepchecks: Not installed
2024-07-10 15:41:43,832:INFO:             xgboost: Not installed
2024-07-10 15:41:43,832:INFO:            catboost: Not installed
2024-07-10 15:41:43,832:INFO:              kmodes: Not installed
2024-07-10 15:41:43,832:INFO:             mlxtend: Not installed
2024-07-10 15:41:43,832:INFO:       statsforecast: Not installed
2024-07-10 15:41:43,832:INFO:        tune_sklearn: Not installed
2024-07-10 15:41:43,832:INFO:                 ray: Not installed
2024-07-10 15:41:43,832:INFO:            hyperopt: Not installed
2024-07-10 15:41:43,832:INFO:              optuna: Not installed
2024-07-10 15:41:43,832:INFO:               skopt: Not installed
2024-07-10 15:41:43,832:INFO:              mlflow: 2.13.0
2024-07-10 15:41:43,832:INFO:              gradio: Not installed
2024-07-10 15:41:43,832:INFO:             fastapi: Not installed
2024-07-10 15:41:43,832:INFO:             uvicorn: Not installed
2024-07-10 15:41:43,832:INFO:              m2cgen: Not installed
2024-07-10 15:41:43,832:INFO:           evidently: Not installed
2024-07-10 15:41:43,832:INFO:               fugue: Not installed
2024-07-10 15:41:43,832:INFO:           streamlit: 1.30.0
2024-07-10 15:41:43,832:INFO:             prophet: Not installed
2024-07-10 15:41:43,832:INFO:None
2024-07-10 15:41:43,832:INFO:Set up data.
2024-07-10 15:41:43,837:INFO:Set up folding strategy.
2024-07-10 15:41:43,837:INFO:Set up train/test split.
2024-07-10 15:41:43,842:INFO:Set up index.
2024-07-10 15:41:43,843:INFO:Assigning column types.
2024-07-10 15:41:43,845:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-07-10 15:41:43,845:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,847:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,849:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,873:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,891:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,892:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:43,892:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:43,892:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,894:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,896:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,920:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,938:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,938:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:43,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:43,938:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-07-10 15:41:43,940:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,942:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,966:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,984:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,984:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:43,984:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:43,986:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-07-10 15:41:43,988:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,012:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,030:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,030:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,030:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,030:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-07-10 15:41:44,034:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,058:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,076:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,076:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,076:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,080:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,104:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,122:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,122:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,122:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,122:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-07-10 15:41:44,150:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,168:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,168:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,169:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,196:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,214:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,214:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,215:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,215:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-07-10 15:41:44,242:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,260:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,261:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,289:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-07-10 15:41:44,307:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,307:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-07-10 15:41:44,354:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,354:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,401:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,401:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,402:INFO:Preparing preprocessing pipeline...
2024-07-10 15:41:44,402:INFO:Set up simple imputation.
2024-07-10 15:41:44,404:INFO:Set up encoding of categorical features.
2024-07-10 15:41:44,404:INFO:Set up removing multicollinearity.
2024-07-10 15:41:44,404:INFO:Set up column transformation.
2024-07-10 15:41:44,404:INFO:Set up feature normalization.
2024-07-10 15:41:44,627:INFO:Finished creating preprocessing pipeline.
2024-07-10 15:41:44,631:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/gp/fp0z7sz5063d4vng25fzsxcm0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sq_mt_built', 'n_rooms',
                                             'n_bathrooms',
                                             'is_exact_address_hidden',
                                             'is_floor_under', 'rent_price',
                                             'is_renewal_needed',
                                             'is_new_development',
                                             'has_central_heating',
                                             'has_individual_heating', 'has_ac',
                                             'has_fitte...
                 TransformerWrapper(include=['neighbourhood'],
                                    transformer=TargetEncoder(cols=['neighbourhood'],
                                                              handle_missing='return_nan'))),
                ('remove_multicollinearity',
                 TransformerWrapper(exclude=[],
                                    transformer=RemoveMulticollinearity(threshold=0.9))),
                ('transformation',
                 TransformerWrapper(transformer=PowerTransformer(standardize=False))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2024-07-10 15:41:44,631:INFO:Creating final display dataframe.
2024-07-10 15:41:44,807:INFO:Setup _display_container:                     Description              Value
0                    Session id                  9
1                        Target  buy_price_by_area
2                   Target type         Regression
3           Original data shape         (7576, 26)
4        Transformed data shape         (7576, 69)
5   Transformed train set shape         (5303, 69)
6    Transformed test set shape         (2273, 69)
7              Numeric features                 20
8          Categorical features                  5
9      Rows with missing values              51.0%
10                   Preprocess               True
11              Imputation type             simple
12           Numeric imputation               mean
13       Categorical imputation               mode
14     Maximum one-hot encoding                 25
15              Encoding method               None
16     Remove multicollinearity               True
17  Multicollinearity threshold                0.9
18               Transformation               True
19        Transformation method        yeo-johnson
20                    Normalize               True
21             Normalize method             zscore
22               Fold Generator              KFold
23                  Fold Number                 10
24                     CPU Jobs                 -1
25                      Use GPU              False
26               Log Experiment              False
27              Experiment Name   reg-default-name
28                          USI               3257
2024-07-10 15:41:44,857:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,857:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,903:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,903:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-07-10 15:41:44,903:INFO:setup() successfully completed in 1.32s...............
2024-07-10 15:41:44,909:INFO:Initializing compare_models()
2024-07-10 15:41:44,909:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>})
2024-07-10 15:41:44,909:INFO:Checking exceptions
2024-07-10 15:41:44,911:INFO:Preparing display monitor
2024-07-10 15:41:44,936:INFO:Initializing Linear Regression
2024-07-10 15:41:44,936:INFO:Total runtime is 5.70217768351237e-06 minutes
2024-07-10 15:41:44,938:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:44,938:INFO:Initializing create_model()
2024-07-10 15:41:44,938:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:44,939:INFO:Checking exceptions
2024-07-10 15:41:44,939:INFO:Importing libraries
2024-07-10 15:41:44,939:INFO:Copying training dataset
2024-07-10 15:41:44,945:INFO:Defining folds
2024-07-10 15:41:44,945:INFO:Declaring metric variables
2024-07-10 15:41:44,946:INFO:Importing untrained model
2024-07-10 15:41:44,948:INFO:Linear Regression Imported successfully
2024-07-10 15:41:44,950:INFO:Starting cross validation
2024-07-10 15:41:44,955:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:47,782:INFO:Calculating mean and std
2024-07-10 15:41:47,783:INFO:Creating metrics dataframe
2024-07-10 15:41:47,785:INFO:Uploading results into container
2024-07-10 15:41:47,785:INFO:Uploading model into container now
2024-07-10 15:41:47,785:INFO:_master_model_container: 1
2024-07-10 15:41:47,785:INFO:_display_container: 2
2024-07-10 15:41:47,786:INFO:LinearRegression(n_jobs=-1)
2024-07-10 15:41:47,786:INFO:create_model() successfully completed......................................
2024-07-10 15:41:47,874:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:47,874:INFO:Creating metrics dataframe
2024-07-10 15:41:47,876:INFO:Initializing Lasso Regression
2024-07-10 15:41:47,876:INFO:Total runtime is 0.04901254971822103 minutes
2024-07-10 15:41:47,878:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:47,878:INFO:Initializing create_model()
2024-07-10 15:41:47,878:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:47,878:INFO:Checking exceptions
2024-07-10 15:41:47,878:INFO:Importing libraries
2024-07-10 15:41:47,878:INFO:Copying training dataset
2024-07-10 15:41:47,883:INFO:Defining folds
2024-07-10 15:41:47,883:INFO:Declaring metric variables
2024-07-10 15:41:47,884:INFO:Importing untrained model
2024-07-10 15:41:47,885:INFO:Lasso Regression Imported successfully
2024-07-10 15:41:47,887:INFO:Starting cross validation
2024-07-10 15:41:47,889:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:48,643:INFO:Calculating mean and std
2024-07-10 15:41:48,643:INFO:Creating metrics dataframe
2024-07-10 15:41:48,644:INFO:Uploading results into container
2024-07-10 15:41:48,645:INFO:Uploading model into container now
2024-07-10 15:41:48,645:INFO:_master_model_container: 2
2024-07-10 15:41:48,645:INFO:_display_container: 2
2024-07-10 15:41:48,645:INFO:Lasso(random_state=9)
2024-07-10 15:41:48,645:INFO:create_model() successfully completed......................................
2024-07-10 15:41:48,696:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:48,696:INFO:Creating metrics dataframe
2024-07-10 15:41:48,699:INFO:Initializing Ridge Regression
2024-07-10 15:41:48,699:INFO:Total runtime is 0.0627148707707723 minutes
2024-07-10 15:41:48,700:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:48,700:INFO:Initializing create_model()
2024-07-10 15:41:48,700:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:48,700:INFO:Checking exceptions
2024-07-10 15:41:48,700:INFO:Importing libraries
2024-07-10 15:41:48,700:INFO:Copying training dataset
2024-07-10 15:41:48,704:INFO:Defining folds
2024-07-10 15:41:48,704:INFO:Declaring metric variables
2024-07-10 15:41:48,705:INFO:Importing untrained model
2024-07-10 15:41:48,706:INFO:Ridge Regression Imported successfully
2024-07-10 15:41:48,708:INFO:Starting cross validation
2024-07-10 15:41:48,709:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:49,290:INFO:Calculating mean and std
2024-07-10 15:41:49,291:INFO:Creating metrics dataframe
2024-07-10 15:41:49,292:INFO:Uploading results into container
2024-07-10 15:41:49,292:INFO:Uploading model into container now
2024-07-10 15:41:49,292:INFO:_master_model_container: 3
2024-07-10 15:41:49,292:INFO:_display_container: 2
2024-07-10 15:41:49,292:INFO:Ridge(random_state=9)
2024-07-10 15:41:49,292:INFO:create_model() successfully completed......................................
2024-07-10 15:41:49,343:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:49,343:INFO:Creating metrics dataframe
2024-07-10 15:41:49,346:INFO:Initializing Elastic Net
2024-07-10 15:41:49,346:INFO:Total runtime is 0.07350326776504516 minutes
2024-07-10 15:41:49,347:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:49,347:INFO:Initializing create_model()
2024-07-10 15:41:49,347:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:49,347:INFO:Checking exceptions
2024-07-10 15:41:49,348:INFO:Importing libraries
2024-07-10 15:41:49,348:INFO:Copying training dataset
2024-07-10 15:41:49,351:INFO:Defining folds
2024-07-10 15:41:49,351:INFO:Declaring metric variables
2024-07-10 15:41:49,352:INFO:Importing untrained model
2024-07-10 15:41:49,354:INFO:Elastic Net Imported successfully
2024-07-10 15:41:49,355:INFO:Starting cross validation
2024-07-10 15:41:49,357:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:49,941:INFO:Calculating mean and std
2024-07-10 15:41:49,941:INFO:Creating metrics dataframe
2024-07-10 15:41:49,942:INFO:Uploading results into container
2024-07-10 15:41:49,942:INFO:Uploading model into container now
2024-07-10 15:41:49,942:INFO:_master_model_container: 4
2024-07-10 15:41:49,943:INFO:_display_container: 2
2024-07-10 15:41:49,943:INFO:ElasticNet(random_state=9)
2024-07-10 15:41:49,943:INFO:create_model() successfully completed......................................
2024-07-10 15:41:49,999:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:49,999:INFO:Creating metrics dataframe
2024-07-10 15:41:50,002:INFO:Initializing Least Angle Regression
2024-07-10 15:41:50,002:INFO:Total runtime is 0.08444578250249227 minutes
2024-07-10 15:41:50,004:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:50,004:INFO:Initializing create_model()
2024-07-10 15:41:50,004:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:50,004:INFO:Checking exceptions
2024-07-10 15:41:50,004:INFO:Importing libraries
2024-07-10 15:41:50,004:INFO:Copying training dataset
2024-07-10 15:41:50,009:INFO:Defining folds
2024-07-10 15:41:50,009:INFO:Declaring metric variables
2024-07-10 15:41:50,010:INFO:Importing untrained model
2024-07-10 15:41:50,011:INFO:Least Angle Regression Imported successfully
2024-07-10 15:41:50,013:INFO:Starting cross validation
2024-07-10 15:41:50,015:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:50,341:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.530e+00, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-07-10 15:41:50,389:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:812: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2024-07-10 15:41:50,389:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:812: RuntimeWarning: overflow encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2024-07-10 15:41:50,390:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:771: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2024-07-10 15:41:50,390:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:775: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2024-07-10 15:41:50,390:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:776: RuntimeWarning: overflow encountered in scalar divide
  gamma_ = min(g1, g2, C / AA)

2024-07-10 15:41:50,390:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:780: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2024-07-10 15:41:50,406:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:208: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2024-07-10 15:41:50,413:WARNING:/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(root_mean_squared_log_error, greater_is_better=False, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/containers/metrics/regression.py", line 209, in root_mean_squared_log_error
    metrics.mean_squared_log_error(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 680, in mean_squared_log_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:41:50,413:WARNING:/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(mean_absolute_percentage_error, greater_is_better=False, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/pycaret/containers/metrics/regression.py", line 233, in mean_absolute_percentage_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:41:50,414:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 207, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:41:50,414:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 497, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:41:50,414:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 572, in root_mean_squared_error
    mean_squared_error(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 497, in mean_squared_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:41:50,414:WARNING:/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:1011: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0.0. Details: 
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 137, in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 1180, in r2_score
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
                                          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py", line 104, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1049, in check_array
    _assert_all_finite(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-07-10 15:41:50,582:INFO:Calculating mean and std
2024-07-10 15:41:50,583:INFO:Creating metrics dataframe
2024-07-10 15:41:50,584:INFO:Uploading results into container
2024-07-10 15:41:50,584:INFO:Uploading model into container now
2024-07-10 15:41:50,584:INFO:_master_model_container: 5
2024-07-10 15:41:50,584:INFO:_display_container: 2
2024-07-10 15:41:50,584:INFO:Lars(random_state=9)
2024-07-10 15:41:50,584:INFO:create_model() successfully completed......................................
2024-07-10 15:41:50,636:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:50,636:INFO:Creating metrics dataframe
2024-07-10 15:41:50,639:INFO:Initializing Lasso Least Angle Regression
2024-07-10 15:41:50,639:INFO:Total runtime is 0.0950543999671936 minutes
2024-07-10 15:41:50,640:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:50,640:INFO:Initializing create_model()
2024-07-10 15:41:50,640:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:50,640:INFO:Checking exceptions
2024-07-10 15:41:50,640:INFO:Importing libraries
2024-07-10 15:41:50,640:INFO:Copying training dataset
2024-07-10 15:41:50,644:INFO:Defining folds
2024-07-10 15:41:50,644:INFO:Declaring metric variables
2024-07-10 15:41:50,645:INFO:Importing untrained model
2024-07-10 15:41:50,646:INFO:Lasso Least Angle Regression Imported successfully
2024-07-10 15:41:50,649:INFO:Starting cross validation
2024-07-10 15:41:50,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:51,223:INFO:Calculating mean and std
2024-07-10 15:41:51,224:INFO:Creating metrics dataframe
2024-07-10 15:41:51,225:INFO:Uploading results into container
2024-07-10 15:41:51,225:INFO:Uploading model into container now
2024-07-10 15:41:51,225:INFO:_master_model_container: 6
2024-07-10 15:41:51,225:INFO:_display_container: 2
2024-07-10 15:41:51,225:INFO:LassoLars(random_state=9)
2024-07-10 15:41:51,225:INFO:create_model() successfully completed......................................
2024-07-10 15:41:51,276:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:51,276:INFO:Creating metrics dataframe
2024-07-10 15:41:51,279:INFO:Initializing Orthogonal Matching Pursuit
2024-07-10 15:41:51,279:INFO:Total runtime is 0.10572811762491861 minutes
2024-07-10 15:41:51,281:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:51,281:INFO:Initializing create_model()
2024-07-10 15:41:51,281:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:51,281:INFO:Checking exceptions
2024-07-10 15:41:51,281:INFO:Importing libraries
2024-07-10 15:41:51,281:INFO:Copying training dataset
2024-07-10 15:41:51,285:INFO:Defining folds
2024-07-10 15:41:51,285:INFO:Declaring metric variables
2024-07-10 15:41:51,286:INFO:Importing untrained model
2024-07-10 15:41:51,287:INFO:Orthogonal Matching Pursuit Imported successfully
2024-07-10 15:41:51,289:INFO:Starting cross validation
2024-07-10 15:41:51,290:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:51,836:INFO:Calculating mean and std
2024-07-10 15:41:51,836:INFO:Creating metrics dataframe
2024-07-10 15:41:51,837:INFO:Uploading results into container
2024-07-10 15:41:51,837:INFO:Uploading model into container now
2024-07-10 15:41:51,838:INFO:_master_model_container: 7
2024-07-10 15:41:51,838:INFO:_display_container: 2
2024-07-10 15:41:51,838:INFO:OrthogonalMatchingPursuit()
2024-07-10 15:41:51,838:INFO:create_model() successfully completed......................................
2024-07-10 15:41:51,889:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:51,889:INFO:Creating metrics dataframe
2024-07-10 15:41:51,892:INFO:Initializing Bayesian Ridge
2024-07-10 15:41:51,892:INFO:Total runtime is 0.11594264904657998 minutes
2024-07-10 15:41:51,893:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:51,894:INFO:Initializing create_model()
2024-07-10 15:41:51,894:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:51,894:INFO:Checking exceptions
2024-07-10 15:41:51,894:INFO:Importing libraries
2024-07-10 15:41:51,894:INFO:Copying training dataset
2024-07-10 15:41:51,898:INFO:Defining folds
2024-07-10 15:41:51,898:INFO:Declaring metric variables
2024-07-10 15:41:51,899:INFO:Importing untrained model
2024-07-10 15:41:51,900:INFO:Bayesian Ridge Imported successfully
2024-07-10 15:41:51,902:INFO:Starting cross validation
2024-07-10 15:41:51,903:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:52,502:INFO:Calculating mean and std
2024-07-10 15:41:52,503:INFO:Creating metrics dataframe
2024-07-10 15:41:52,504:INFO:Uploading results into container
2024-07-10 15:41:52,504:INFO:Uploading model into container now
2024-07-10 15:41:52,504:INFO:_master_model_container: 8
2024-07-10 15:41:52,504:INFO:_display_container: 2
2024-07-10 15:41:52,504:INFO:BayesianRidge()
2024-07-10 15:41:52,504:INFO:create_model() successfully completed......................................
2024-07-10 15:41:52,555:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:52,555:INFO:Creating metrics dataframe
2024-07-10 15:41:52,558:INFO:Initializing Passive Aggressive Regressor
2024-07-10 15:41:52,558:INFO:Total runtime is 0.12704593340555825 minutes
2024-07-10 15:41:52,560:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:52,560:INFO:Initializing create_model()
2024-07-10 15:41:52,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:52,560:INFO:Checking exceptions
2024-07-10 15:41:52,560:INFO:Importing libraries
2024-07-10 15:41:52,560:INFO:Copying training dataset
2024-07-10 15:41:52,564:INFO:Defining folds
2024-07-10 15:41:52,564:INFO:Declaring metric variables
2024-07-10 15:41:52,565:INFO:Importing untrained model
2024-07-10 15:41:52,566:INFO:Passive Aggressive Regressor Imported successfully
2024-07-10 15:41:52,568:INFO:Starting cross validation
2024-07-10 15:41:52,569:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:53,234:INFO:Calculating mean and std
2024-07-10 15:41:53,234:INFO:Creating metrics dataframe
2024-07-10 15:41:53,235:INFO:Uploading results into container
2024-07-10 15:41:53,235:INFO:Uploading model into container now
2024-07-10 15:41:53,235:INFO:_master_model_container: 9
2024-07-10 15:41:53,235:INFO:_display_container: 2
2024-07-10 15:41:53,235:INFO:PassiveAggressiveRegressor(random_state=9)
2024-07-10 15:41:53,235:INFO:create_model() successfully completed......................................
2024-07-10 15:41:53,286:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:53,287:INFO:Creating metrics dataframe
2024-07-10 15:41:53,290:INFO:Initializing Huber Regressor
2024-07-10 15:41:53,290:INFO:Total runtime is 0.13923858404159545 minutes
2024-07-10 15:41:53,291:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:53,291:INFO:Initializing create_model()
2024-07-10 15:41:53,291:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:53,291:INFO:Checking exceptions
2024-07-10 15:41:53,291:INFO:Importing libraries
2024-07-10 15:41:53,291:INFO:Copying training dataset
2024-07-10 15:41:53,295:INFO:Defining folds
2024-07-10 15:41:53,295:INFO:Declaring metric variables
2024-07-10 15:41:53,296:INFO:Importing untrained model
2024-07-10 15:41:53,297:INFO:Huber Regressor Imported successfully
2024-07-10 15:41:53,299:INFO:Starting cross validation
2024-07-10 15:41:53,300:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:54,062:INFO:Calculating mean and std
2024-07-10 15:41:54,062:INFO:Creating metrics dataframe
2024-07-10 15:41:54,063:INFO:Uploading results into container
2024-07-10 15:41:54,063:INFO:Uploading model into container now
2024-07-10 15:41:54,063:INFO:_master_model_container: 10
2024-07-10 15:41:54,063:INFO:_display_container: 2
2024-07-10 15:41:54,063:INFO:HuberRegressor()
2024-07-10 15:41:54,063:INFO:create_model() successfully completed......................................
2024-07-10 15:41:54,115:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:54,115:INFO:Creating metrics dataframe
2024-07-10 15:41:54,119:INFO:Initializing K Neighbors Regressor
2024-07-10 15:41:54,119:INFO:Total runtime is 0.15305868387222288 minutes
2024-07-10 15:41:54,120:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:54,120:INFO:Initializing create_model()
2024-07-10 15:41:54,121:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:54,121:INFO:Checking exceptions
2024-07-10 15:41:54,121:INFO:Importing libraries
2024-07-10 15:41:54,121:INFO:Copying training dataset
2024-07-10 15:41:54,125:INFO:Defining folds
2024-07-10 15:41:54,125:INFO:Declaring metric variables
2024-07-10 15:41:54,126:INFO:Importing untrained model
2024-07-10 15:41:54,127:INFO:K Neighbors Regressor Imported successfully
2024-07-10 15:41:54,128:INFO:Starting cross validation
2024-07-10 15:41:54,130:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:54,819:INFO:Calculating mean and std
2024-07-10 15:41:54,820:INFO:Creating metrics dataframe
2024-07-10 15:41:54,820:INFO:Uploading results into container
2024-07-10 15:41:54,821:INFO:Uploading model into container now
2024-07-10 15:41:54,821:INFO:_master_model_container: 11
2024-07-10 15:41:54,821:INFO:_display_container: 2
2024-07-10 15:41:54,821:INFO:KNeighborsRegressor(n_jobs=-1)
2024-07-10 15:41:54,821:INFO:create_model() successfully completed......................................
2024-07-10 15:41:54,879:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:54,880:INFO:Creating metrics dataframe
2024-07-10 15:41:54,884:INFO:Initializing Decision Tree Regressor
2024-07-10 15:41:54,884:INFO:Total runtime is 0.16580013434092203 minutes
2024-07-10 15:41:54,885:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:54,885:INFO:Initializing create_model()
2024-07-10 15:41:54,885:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:54,885:INFO:Checking exceptions
2024-07-10 15:41:54,885:INFO:Importing libraries
2024-07-10 15:41:54,885:INFO:Copying training dataset
2024-07-10 15:41:54,889:INFO:Defining folds
2024-07-10 15:41:54,890:INFO:Declaring metric variables
2024-07-10 15:41:54,891:INFO:Importing untrained model
2024-07-10 15:41:54,892:INFO:Decision Tree Regressor Imported successfully
2024-07-10 15:41:54,894:INFO:Starting cross validation
2024-07-10 15:41:54,895:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:41:55,614:INFO:Calculating mean and std
2024-07-10 15:41:55,615:INFO:Creating metrics dataframe
2024-07-10 15:41:55,616:INFO:Uploading results into container
2024-07-10 15:41:55,616:INFO:Uploading model into container now
2024-07-10 15:41:55,616:INFO:_master_model_container: 12
2024-07-10 15:41:55,616:INFO:_display_container: 2
2024-07-10 15:41:55,616:INFO:DecisionTreeRegressor(random_state=9)
2024-07-10 15:41:55,616:INFO:create_model() successfully completed......................................
2024-07-10 15:41:55,672:INFO:SubProcess create_model() end ==================================
2024-07-10 15:41:55,672:INFO:Creating metrics dataframe
2024-07-10 15:41:55,676:INFO:Initializing Random Forest Regressor
2024-07-10 15:41:55,676:INFO:Total runtime is 0.17900550365447998 minutes
2024-07-10 15:41:55,677:INFO:SubProcess create_model() called ==================================
2024-07-10 15:41:55,677:INFO:Initializing create_model()
2024-07-10 15:41:55,677:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:41:55,677:INFO:Checking exceptions
2024-07-10 15:41:55,677:INFO:Importing libraries
2024-07-10 15:41:55,677:INFO:Copying training dataset
2024-07-10 15:41:55,681:INFO:Defining folds
2024-07-10 15:41:55,681:INFO:Declaring metric variables
2024-07-10 15:41:55,682:INFO:Importing untrained model
2024-07-10 15:41:55,684:INFO:Random Forest Regressor Imported successfully
2024-07-10 15:41:55,686:INFO:Starting cross validation
2024-07-10 15:41:55,687:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:42:00,119:INFO:Calculating mean and std
2024-07-10 15:42:00,119:INFO:Creating metrics dataframe
2024-07-10 15:42:00,120:INFO:Uploading results into container
2024-07-10 15:42:00,120:INFO:Uploading model into container now
2024-07-10 15:42:00,121:INFO:_master_model_container: 13
2024-07-10 15:42:00,121:INFO:_display_container: 2
2024-07-10 15:42:00,121:INFO:RandomForestRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:42:00,121:INFO:create_model() successfully completed......................................
2024-07-10 15:42:00,173:INFO:SubProcess create_model() end ==================================
2024-07-10 15:42:00,173:INFO:Creating metrics dataframe
2024-07-10 15:42:00,176:INFO:Initializing Extra Trees Regressor
2024-07-10 15:42:00,176:INFO:Total runtime is 0.25400980313618976 minutes
2024-07-10 15:42:00,177:INFO:SubProcess create_model() called ==================================
2024-07-10 15:42:00,178:INFO:Initializing create_model()
2024-07-10 15:42:00,178:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:42:00,178:INFO:Checking exceptions
2024-07-10 15:42:00,178:INFO:Importing libraries
2024-07-10 15:42:00,178:INFO:Copying training dataset
2024-07-10 15:42:00,182:INFO:Defining folds
2024-07-10 15:42:00,182:INFO:Declaring metric variables
2024-07-10 15:42:00,183:INFO:Importing untrained model
2024-07-10 15:42:00,184:INFO:Extra Trees Regressor Imported successfully
2024-07-10 15:42:00,186:INFO:Starting cross validation
2024-07-10 15:42:00,187:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:42:04,463:INFO:Calculating mean and std
2024-07-10 15:42:04,463:INFO:Creating metrics dataframe
2024-07-10 15:42:04,464:INFO:Uploading results into container
2024-07-10 15:42:04,464:INFO:Uploading model into container now
2024-07-10 15:42:04,464:INFO:_master_model_container: 14
2024-07-10 15:42:04,465:INFO:_display_container: 2
2024-07-10 15:42:04,465:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:42:04,465:INFO:create_model() successfully completed......................................
2024-07-10 15:42:04,516:INFO:SubProcess create_model() end ==================================
2024-07-10 15:42:04,516:INFO:Creating metrics dataframe
2024-07-10 15:42:04,520:INFO:Initializing AdaBoost Regressor
2024-07-10 15:42:04,520:INFO:Total runtime is 0.3263988335927327 minutes
2024-07-10 15:42:04,521:INFO:SubProcess create_model() called ==================================
2024-07-10 15:42:04,521:INFO:Initializing create_model()
2024-07-10 15:42:04,521:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:42:04,521:INFO:Checking exceptions
2024-07-10 15:42:04,521:INFO:Importing libraries
2024-07-10 15:42:04,521:INFO:Copying training dataset
2024-07-10 15:42:04,525:INFO:Defining folds
2024-07-10 15:42:04,525:INFO:Declaring metric variables
2024-07-10 15:42:04,526:INFO:Importing untrained model
2024-07-10 15:42:04,527:INFO:AdaBoost Regressor Imported successfully
2024-07-10 15:42:04,529:INFO:Starting cross validation
2024-07-10 15:42:04,530:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:42:05,667:INFO:Calculating mean and std
2024-07-10 15:42:05,668:INFO:Creating metrics dataframe
2024-07-10 15:42:05,669:INFO:Uploading results into container
2024-07-10 15:42:05,669:INFO:Uploading model into container now
2024-07-10 15:42:05,669:INFO:_master_model_container: 15
2024-07-10 15:42:05,669:INFO:_display_container: 2
2024-07-10 15:42:05,669:INFO:AdaBoostRegressor(random_state=9)
2024-07-10 15:42:05,669:INFO:create_model() successfully completed......................................
2024-07-10 15:42:05,720:INFO:SubProcess create_model() end ==================================
2024-07-10 15:42:05,720:INFO:Creating metrics dataframe
2024-07-10 15:42:05,724:INFO:Initializing Gradient Boosting Regressor
2024-07-10 15:42:05,724:INFO:Total runtime is 0.34646485249201453 minutes
2024-07-10 15:42:05,725:INFO:SubProcess create_model() called ==================================
2024-07-10 15:42:05,725:INFO:Initializing create_model()
2024-07-10 15:42:05,725:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:42:05,725:INFO:Checking exceptions
2024-07-10 15:42:05,725:INFO:Importing libraries
2024-07-10 15:42:05,725:INFO:Copying training dataset
2024-07-10 15:42:05,729:INFO:Defining folds
2024-07-10 15:42:05,729:INFO:Declaring metric variables
2024-07-10 15:42:05,730:INFO:Importing untrained model
2024-07-10 15:42:05,731:INFO:Gradient Boosting Regressor Imported successfully
2024-07-10 15:42:05,733:INFO:Starting cross validation
2024-07-10 15:42:05,734:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:42:07,625:INFO:Calculating mean and std
2024-07-10 15:42:07,625:INFO:Creating metrics dataframe
2024-07-10 15:42:07,626:INFO:Uploading results into container
2024-07-10 15:42:07,626:INFO:Uploading model into container now
2024-07-10 15:42:07,626:INFO:_master_model_container: 16
2024-07-10 15:42:07,626:INFO:_display_container: 2
2024-07-10 15:42:07,626:INFO:GradientBoostingRegressor(random_state=9)
2024-07-10 15:42:07,627:INFO:create_model() successfully completed......................................
2024-07-10 15:42:07,677:INFO:SubProcess create_model() end ==================================
2024-07-10 15:42:07,677:INFO:Creating metrics dataframe
2024-07-10 15:42:07,681:INFO:Initializing Light Gradient Boosting Machine
2024-07-10 15:42:07,681:INFO:Total runtime is 0.37908986806869505 minutes
2024-07-10 15:42:07,682:INFO:SubProcess create_model() called ==================================
2024-07-10 15:42:07,682:INFO:Initializing create_model()
2024-07-10 15:42:07,682:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:42:07,683:INFO:Checking exceptions
2024-07-10 15:42:07,683:INFO:Importing libraries
2024-07-10 15:42:07,683:INFO:Copying training dataset
2024-07-10 15:42:07,687:INFO:Defining folds
2024-07-10 15:42:07,687:INFO:Declaring metric variables
2024-07-10 15:42:07,688:INFO:Importing untrained model
2024-07-10 15:42:07,689:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:42:07,691:INFO:Starting cross validation
2024-07-10 15:42:07,692:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:42:10,894:INFO:Calculating mean and std
2024-07-10 15:42:10,894:INFO:Creating metrics dataframe
2024-07-10 15:42:10,895:INFO:Uploading results into container
2024-07-10 15:42:10,895:INFO:Uploading model into container now
2024-07-10 15:42:10,896:INFO:_master_model_container: 17
2024-07-10 15:42:10,896:INFO:_display_container: 2
2024-07-10 15:42:10,896:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:42:10,896:INFO:create_model() successfully completed......................................
2024-07-10 15:42:10,946:INFO:SubProcess create_model() end ==================================
2024-07-10 15:42:10,947:INFO:Creating metrics dataframe
2024-07-10 15:42:10,950:INFO:Initializing Dummy Regressor
2024-07-10 15:42:10,951:INFO:Total runtime is 0.4335798859596252 minutes
2024-07-10 15:42:10,952:INFO:SubProcess create_model() called ==================================
2024-07-10 15:42:10,952:INFO:Initializing create_model()
2024-07-10 15:42:10,952:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x17a987a10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:42:10,952:INFO:Checking exceptions
2024-07-10 15:42:10,952:INFO:Importing libraries
2024-07-10 15:42:10,952:INFO:Copying training dataset
2024-07-10 15:42:10,956:INFO:Defining folds
2024-07-10 15:42:10,956:INFO:Declaring metric variables
2024-07-10 15:42:10,957:INFO:Importing untrained model
2024-07-10 15:42:10,958:INFO:Dummy Regressor Imported successfully
2024-07-10 15:42:10,960:INFO:Starting cross validation
2024-07-10 15:42:10,961:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-07-10 15:42:11,533:INFO:Calculating mean and std
2024-07-10 15:42:11,534:INFO:Creating metrics dataframe
2024-07-10 15:42:11,535:INFO:Uploading results into container
2024-07-10 15:42:11,535:INFO:Uploading model into container now
2024-07-10 15:42:11,535:INFO:_master_model_container: 18
2024-07-10 15:42:11,535:INFO:_display_container: 2
2024-07-10 15:42:11,536:INFO:DummyRegressor()
2024-07-10 15:42:11,536:INFO:create_model() successfully completed......................................
2024-07-10 15:42:11,586:INFO:SubProcess create_model() end ==================================
2024-07-10 15:42:11,586:INFO:Creating metrics dataframe
2024-07-10 15:42:11,593:INFO:Initializing create_model()
2024-07-10 15:42:11,593:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-07-10 15:42:11,593:INFO:Checking exceptions
2024-07-10 15:42:11,594:INFO:Importing libraries
2024-07-10 15:42:11,594:INFO:Copying training dataset
2024-07-10 15:42:11,598:INFO:Defining folds
2024-07-10 15:42:11,598:INFO:Declaring metric variables
2024-07-10 15:42:11,598:INFO:Importing untrained model
2024-07-10 15:42:11,598:INFO:Declaring custom model
2024-07-10 15:42:11,598:INFO:Light Gradient Boosting Machine Imported successfully
2024-07-10 15:42:11,599:INFO:Cross validation set to False
2024-07-10 15:42:11,599:INFO:Fitting Model
2024-07-10 15:42:11,805:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-07-10 15:42:11,809:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001808 seconds.
2024-07-10 15:42:11,809:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-07-10 15:42:11,809:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-07-10 15:42:11,809:INFO:[LightGBM] [Info] Total Bins 961
2024-07-10 15:42:11,809:INFO:[LightGBM] [Info] Number of data points in the train set: 5303, number of used features: 66
2024-07-10 15:42:11,809:INFO:[LightGBM] [Info] Start training from score 3719.391099
2024-07-10 15:42:12,107:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:42:12,107:INFO:create_model() successfully completed......................................
2024-07-10 15:42:12,167:INFO:_master_model_container: 18
2024-07-10 15:42:12,167:INFO:_display_container: 2
2024-07-10 15:42:12,168:INFO:LGBMRegressor(n_jobs=-1, random_state=9)
2024-07-10 15:42:12,168:INFO:compare_models() successfully completed......................................
2024-07-10 15:42:12,170:INFO:Initializing predict_model()
2024-07-10 15:42:12,170:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x17d94c180>)
2024-07-10 15:42:12,170:INFO:Checking exceptions
2024-07-10 15:42:12,170:INFO:Preloading libraries
2024-07-10 15:42:12,171:INFO:Set up data.
2024-07-10 15:42:12,175:INFO:Set up index.
2024-07-10 15:42:12,285:INFO:Initializing plot_model()
2024-07-10 15:42:12,285:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=residuals, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:42:12,285:INFO:Checking exceptions
2024-07-10 15:42:12,287:INFO:Preloading libraries
2024-07-10 15:42:12,290:INFO:Copying training dataset
2024-07-10 15:42:12,290:INFO:Plot type: residuals
2024-07-10 15:42:12,493:INFO:Fitting Model
2024-07-10 15:42:12,517:INFO:Scoring test/hold-out set
2024-07-10 15:42:12,765:INFO:Visual Rendered Successfully
2024-07-10 15:42:12,832:INFO:plot_model() successfully completed......................................
2024-07-10 15:42:12,835:INFO:Initializing plot_model()
2024-07-10 15:42:12,835:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=error, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:42:12,835:INFO:Checking exceptions
2024-07-10 15:42:12,838:INFO:Preloading libraries
2024-07-10 15:42:12,840:INFO:Copying training dataset
2024-07-10 15:42:12,840:INFO:Plot type: error
2024-07-10 15:42:13,025:INFO:Fitting Model
2024-07-10 15:42:13,025:INFO:Scoring test/hold-out set
2024-07-10 15:42:13,111:INFO:Visual Rendered Successfully
2024-07-10 15:42:13,166:INFO:plot_model() successfully completed......................................
2024-07-10 15:42:13,169:INFO:Initializing plot_model()
2024-07-10 15:42:13,169:INFO:plot_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x17a425250>, estimator=LGBMRegressor(n_jobs=-1, random_state=9), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-07-10 15:42:13,169:INFO:Checking exceptions
2024-07-10 15:42:13,172:INFO:Preloading libraries
2024-07-10 15:42:13,175:INFO:Copying training dataset
2024-07-10 15:42:13,175:INFO:Plot type: feature
2024-07-10 15:42:13,175:WARNING:No coef_ found. Trying feature_importances_
2024-07-10 15:42:13,280:INFO:Visual Rendered Successfully
2024-07-10 15:42:13,334:INFO:plot_model() successfully completed......................................
