# Import necessary libraries
import pandas as pd
import numpy as np

# Visualizationdf_e.shape
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("fivethirtyeight")

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score




# Load the dataset
df_e = pd.read_csv('../data/processed/additional_data.csv')



import warnings
warnings.filterwarnings("ignore")









sns.histplot(df_e['price'],kde = True,bins=50)



q = df_e['price'].quantile(0.95)

df_e = df_e[df_e['price']<q]

sns.histplot(df_e['price'],kde = True,bins=50)





sns.histplot(df_e['rooms'],kde = True,bins=50)



df_e = df_e[df_e['rooms']<10]

sns.histplot(df_e['rooms'],kde = True,bins=50)





sns.histplot(df_e['m2'], kde = True, bins = 50)


q = df_e['m2'].quantile(0.95)

df_e = df_e[df_e['m2']<q]

sns.histplot(df_e['m2'],kde = True,bins=50)





df_e_without_duplicates = df_e.drop_duplicates()


df_e_without_duplicates.isna().sum()


df_e_without_duplicates_without_na = df_e_without_duplicates.dropna()


df_e_without_duplicates_without_na.isna().sum()


df_e_without_duplicates_without_na.shape


df = df_e_without_duplicates_without_na


# Create the new columns
df['house_type_'] = df['house_type']
df['floor'] = 'n/a'


import re
# Function to categorize house_type_ and floor
def categorize_house_type(house_type):
    match = re.match(r'^planta\s*(-?\d+)$', house_type)
    if match:
        return 'Apartment', match.group(1)
    else:
        return house_type, 'n/a'

# Apply the function to the DataFrame and create new columns
df[['house_type_', 'floor']] = df['house_type'].apply(lambda x: pd.Series(categorize_house_type(x)))

print(df)


df['house_type_'].value_counts()


df.drop(columns=['house_type'], inplace=True)


df['floor'].replace('n/a','0',inplace=True)


df['floor'].value_counts()


# Convert 'floor' column to numeric, setting errors='coerce' to handle invalid values
df['floor'] = pd.to_numeric(df['floor'], errors='coerce')

print(df['floor'].dtype)  # This should now print 'int64'



# Correlation heatmap
numeric_cols = df.select_dtypes(include=['float64','int64']).columns
df[numeric_cols].corr()['price'].sort_values(ascending=False)


df.to_csv('../data/processed/Additional_Dataset_wo_null_ayushya.csv')


#conda install -c conda-forge pycaret


from pycaret.regression import *


# Setup the environment in PyCaret
regression_setup = setup(
    data=df,
    #test_data=test_data,
    target = 'price',
    session_id=9,
    ignore_features=[],
    numeric_imputation = 'mean',
    categorical_imputation = 'mode',
    remove_multicollinearity=True,
    transformation= True,
    transformation_method = 'yeo-johnson',
    normalize = True,
    normalize_method = 'zscore'
    )





# compare baseline models
best_model = compare_models()


# list all models
models()


# Compare tree models
compare_tree_models = compare_models(include = ['dt', 'rf', 'et', 'gbr', 'lightgbm'])


compare_tree_models


# Best 3 models
best_6_models = compare_models(cross_validation = True, n_select=6)


best_6_models





# Create a LightGBM regressor model
lgbm = create_model('lightgbm')

# Define custom grid for tuning
custom_grid = {
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [100, 300, 500],
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 10, 20],
    'min_child_samples': [20, 50, 100]
}

# Tune the LightGBM regressor model with the custom grid
tuned_lgbm = tune_model(lgbm, custom_grid=custom_grid, optimize='R2')





# ensemble with bagging
ensemble_model(lgbm, method = 'Bagging')


# ensemble with boosting
ensemble_model(lgbm, method = 'Boosting',)





# top 3 models based on mae
best_6_models


# blend top 3 models
blend_models(best_6_models)





stack_models(best_6_models)









# predict on test set
holdout_pred = predict_model(best_model)



# show predictions df
holdout_pred[['price','prediction_label']].head()





df.to_csv('../data/processed/Additional_Dataset_wo_null_ayushya.csv')


#### # plot residuals
plot_model(best_model, plot = 'residuals')


# plot error
plot_model(best_model, plot = 'error')


# plot feature importance
plot_model(best_model, plot = 'feature')


#### 
