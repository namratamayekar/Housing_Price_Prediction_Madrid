{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='I' name=\"I\"></a>\n",
    "## [Introduction](#P0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:   - Read the dataset information and details at the source.\n",
    "- Download the dataset using pandas.\n",
    "- List all the columns and try to interpret each one.\n",
    "- Identify artifacts and other unusual things at first.\n",
    "- Use analyse dataframe to do analysis - level 2.\n",
    "- Exploratory data analysis\n",
    "- Feature selection\n",
    "- Scaling and normalization\n",
    "- Data splitting - Train test validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SU' name=\"SU\"></a>\n",
    "## [Set up](#P0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T16:56:09.469904Z",
     "start_time": "2019-01-07T16:56:07.858398Z"
    }
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check pycaret version\n",
    "from pycaret.utils import version\n",
    "version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DataFrame_Analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/ayushyapare/Desktop/Ayushya/Snippets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDataFrame_Analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_dataframe, eda\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DataFrame_Analysis'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ayushyapare/Desktop/Ayushya/Snippets')\n",
    "\n",
    "from DataFrame_Analysis import analyze_dataframe, eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval and introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Telecom customers__: Dataset consisting of different features of customers of a telecom company and based on their usage and other factors, one can cluster the customers into different segments and predict if a customer is going to churn (cancel the subscription)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "df = pd.read_csv('../data/raw/telecom_users.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic:\n",
    "# 1. Shape\n",
    "# 2. Columns - look for artifacts in column name\n",
    "# 3. Info - look for appropriate datatypes \n",
    "# 4. Describe - look for min max mean and std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all duplicate rows\n",
    "# Identify duplicate rows\n",
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced (Separate categorical and numerical)\n",
    "# 1. value counts | Unique values | Missing values\n",
    "# 2. Explore column of interest\n",
    "#    1. Hist / Countplot\n",
    "#    2. Boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform exploratory data analysis on each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Initial observations___   \n",
    "_Interesting to know the correlation between:_\n",
    "1. Paperless_billing may be unimpotrant\n",
    "\n",
    "_Some ambiguous column names can be changed:_   \n",
    "1. Partner --> married\n",
    "2. Dependents --> childern\n",
    "\n",
    "_Total charges must be numerical_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for avoiding ambiguity\n",
    "df.rename(columns={'Partner': 'Married', 'Dependents': 'Children'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all values like ' ' to 0 in numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_values = [\" \"]\n",
    "df.replace(incorrect_values, '0', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCharges'] = pd.to_numeric(df.TotalCharges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Senior citizen 1 with 'yes' and 0 with 'no'\n",
    "#df['SeniorCitizen'] = df['SeniorCitizen'].replace({1:'yes',0: 'no'})\n",
    "\n",
    "# Replace gender with 1:Male and 0:Female\n",
    "#df['gender'] = df['gender'].replace({'Male':1,'Female':0})\n",
    "\n",
    "# Replace churn\n",
    "#df['Churn'] = df['Churn'].replace({'Yes': 1, 'No': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a CSV file\n",
    "df.to_csv('../data/processed/telecom_users.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop Customerid column since it does not give any statistical information\n",
    "# 2. Drop Churn atleast for the clustering\n",
    "\n",
    "df.drop(columns=['customerID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EDA now\n",
    "analyze_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Observations___\n",
    "1. The section of the customers which are categorized as 'No internet service' -> ambiguous for the model.\n",
    "\n",
    "Example:   \n",
    "StreamingMovies    \n",
    "No                     2356   \n",
    "Yes                    2339   \n",
    "No internet service    1291\n",
    "\n",
    "Irrelevant for model. It wants to know if Customer streams movie or not. \n",
    "\n",
    "\n",
    "Suggestions:   \n",
    "- group into new category\n",
    "- remove from this feature as these customers are already segregated in 'Internet service' column.\n",
    "- Change the values 'No internet access' to 'No'. This makes sense that assuming these customers do not stream movies, or even if they do, they do not do it through our client's network. So categorize under 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the no internet section of the customers\n",
    "internet_service_features = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "for feature in internet_service_features:\n",
    "    df[feature] = df[feature].replace('No internet service', 'No')\n",
    "\n",
    "#df['MultipleLines'] = df['MultipleLines'].replace('No phone service', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop PaperlessBilling\n",
    "df.drop(columns=['PaperlessBilling'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOS:\n",
    "1. Scaling and transform\n",
    "2. Clustering - K-Means, K-Medoids, Hierarchical, DBSCAN\n",
    "3. Silhuetter and Elbow methods (Number of clusters)\n",
    "4. Dimensionality reduction for visualization - PCA\n",
    "5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe\n",
    "X = df.drop(columns='Churn', axis = 1)\n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaling for numerical columns\n",
    "# One Hot Encoding for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Select numeric and categorical columns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m      8\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define the preprocessor\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(), categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the KMeans model with preprocessing\n",
    "def fit_kmeans(n_clusters, X):\n",
    "    kmeans_pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"cluster\", KMeans(n_clusters=n_clusters, random_state=9, verbose=0))\n",
    "    ])\n",
    "    kmeans_pipeline.fit(X)\n",
    "    return kmeans_pipeline.named_steps[\"cluster\"].inertia_\n",
    "\n",
    "# Compute WCSS for different numbers of clusters\n",
    "cluster_errors = []\n",
    "\n",
    "for n_clusters in range(2, 11):\n",
    "    wcsse = fit_kmeans(n_clusters,X)\n",
    "    print('K = ', n_clusters, '\\tWCSS Err. = ', wcsse)\n",
    "    cluster_errors.append(wcsse)\n",
    "\n",
    "# Plot the SSE for different numbers of clusters\n",
    "plt.plot(range(2, 11), cluster_errors, \"o-\")\n",
    "plt.xlabel(\"No. Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "silhouette_s = []\n",
    "\n",
    "for n_clusters in range(2, 11):\n",
    "    kmeans_pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"cluster\", KMeans(n_clusters=n_clusters, random_state=9, verbose=0))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline and get the cluster labels\n",
    "    cluster_labels = kmeans_pipeline.fit_predict(X)\n",
    "    \n",
    "    # Get the preprocessed data\n",
    "    X_tr = kmeans_pipeline.named_steps[\"preprocessor\"].transform(X)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X_tr, cluster_labels).round(4)\n",
    "    print(f\"For n_clusters = {n_clusters}, The average silhouette_score is : {silhouette_avg}\")\n",
    "    \n",
    "    silhouette_s.append(silhouette_avg)\n",
    "\n",
    "# Plot the Silhouette Scores for different numbers of clusters\n",
    "plt.plot(range(2, 11), silhouette_s, \"o-\")\n",
    "plt.xlabel(\"No. Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Scores for Different Numbers of Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "| Range       | Interpretation                                |\n",
    "|-------------|-----------------------------------------------|\n",
    "| 0.71 - 1.0  | A strong structure has been found.            |\n",
    "| 0.51 - 0.7  | A reasonable structure has been found.        |\n",
    "| 0.26 - 0.5  | The structure is weak and could be artificial.|\n",
    "| < 0.25      | No substantial structure has been found.      |\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (PCA) before Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cluster_errors = []\n",
    "\n",
    "for n_cluster in range(1, 14):\n",
    "    pipe_pca_kmean = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor), \n",
    "            (\"pca\", PCA(0.90)), \n",
    "            (\"cluster\", KMeans(n_clusters=n_cluster, random_state=9))]\n",
    "    )\n",
    "\n",
    "    pipe_pca_kmean.fit_predict(X)\n",
    "    cluster_errors.append(pipe_pca_kmean.named_steps[\"cluster\"].inertia_) \n",
    "\n",
    "#plt.clf()\n",
    "plt.plot(cluster_errors, \"o-\")\n",
    "plt.xlabel(\"n_clusters\")\n",
    "plt.ylabel(\"wss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "# Define the pipeline with preprocessing, PCA, and KMeans clustering\n",
    "pipe_pca_kmean_f = Pipeline([\n",
    "    (\"preprocessor\", preprocessor), \n",
    "    (\"pca\", PCA(0.90)), \n",
    "    (\"cluster\", KMeans(n_clusters=K, random_state=9))\n",
    "])\n",
    "\n",
    "# Fit the pipeline and get the cluster labels\n",
    "X['kmean_cluster'] = pipe_pca_kmean_f.fit_predict(X)\n",
    "\n",
    "# Get the cluster inertia\n",
    "cluster_errors = []\n",
    "cluster_errors.append(pipe_pca_kmean_f.named_steps[\"cluster\"].inertia_) \n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=X, x='InternetService', hue='kmean_cluster', palette='viridis')\n",
    "plt.xlabel('Customer Segment')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Number of Customers in Each Segment by Internet Service')\n",
    "plt.legend(title='Internet Service', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for customers who stream movies\n",
    "streaming_df = X[X['StreamingMovies'] == 'Yes']\n",
    "\n",
    "# Plotting the number of customers from each segment who stream movies based on the internet service they use\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=streaming_df, x='InternetService', hue='kmean_cluster', palette='viridis')\n",
    "plt.xlabel('Customer Segment')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Number of Customers Who Stream Movies in Each Segment by Internet Service')\n",
    "plt.legend(title='Internet Service', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_charges = X.groupby(['kmean_cluster', 'Contract'])['TotalCharges'].mean().reset_index()\n",
    "\n",
    "# Plotting the mean total charges by contract type for each segment\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=mean_total_charges, x='Contract', y='TotalCharges', hue='kmean_cluster', palette='viridis')\n",
    "plt.xlabel('Customer Segment')\n",
    "plt.ylabel('Mean Total Charges')\n",
    "plt.title('Mean Total Charges by Contract Type for Each Segment')\n",
    "plt.legend(title='Contract Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_charges = X.groupby(['kmean_cluster', 'InternetService'])['TotalCharges'].mean().reset_index()\n",
    "\n",
    "# Plotting the mean total charges by contract type for each segment\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=mean_total_charges, x='InternetService', y='TotalCharges', hue='kmean_cluster', palette='viridis')\n",
    "plt.xlabel('Customer Segment')\n",
    "plt.ylabel('Mean Total Charges')\n",
    "plt.title('Total Charges for Internet Service type for each Segment')\n",
    "plt.legend(title='Contract Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = sns.scatterplot(\n",
    "    x='tenure',\n",
    "    y='MonthlyCharges',\n",
    "    hue='kmean_cluster',\n",
    "    data=X,\n",
    "    palette='viridis'\n",
    ")\n",
    "ax.legend(bbox_to_anchor=(1.04, 1.02), loc='upper left', fontsize='large')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define features and labels\n",
    "# 2. choose features\n",
    "# 3. train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xx = df.drop(columns = 'Churn', axis = 1)\n",
    "yy = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xx, yy, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "('Pycaret only supports python 3.9, 3.10, 3.11. Your actual Python version: ', sys.version_info(major=3, minor=8, micro=18, releaselevel='final', serial=0), 'Please UPGRADE your Python version.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycaret\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/constructor/lib/python3.8/site-packages/pycaret/__init__.py:16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Pycaret only supports python 3.9, 3.10, 3.11\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# This code is to avoid issues with python 3.7 or other not supported versions\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# example (see package versions): https://github.com/pycaret/pycaret/issues/3746\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPycaret only supports python 3.9, 3.10, 3.11. Your actual Python version: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m         sys\u001b[38;5;241m.\u001b[39mversion_info,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease UPGRADE your Python version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPycaret only supports python 3.9, 3.10, 3.11. Your actual Python version: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m         sys\u001b[38;5;241m.\u001b[39mversion_info,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease DOWNGRADE your Python version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ('Pycaret only supports python 3.9, 3.10, 3.11. Your actual Python version: ', sys.version_info(major=3, minor=8, micro=18, releaselevel='final', serial=0), 'Please UPGRADE your Python version.')"
     ]
    }
   ],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MLFLOW_TRACKING_URI=https://2521-2a02-168-57c6-0-68e5-3a-ff22-7e75.ngrok-free.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clsfr = setup(\n",
    "    data=pd.concat([X_train, y_train], axis=1),\n",
    "    target = 'Churn',\n",
    "    session_id=9,\n",
    "    #max_encoding_ohe=600, # columns with 600 or less categories will be One-hot encoded ELSE target encoding\n",
    "    #rare_to_value=0.008, # Categories with less than 0.008 (0.8%) of the data will be grouped into a new category (Other)\n",
    "    #rare_value='Other',\n",
    "    fix_imbalance = True,\n",
    "    fix_imbalance_method = 'SMOTE',\n",
    "    transformation = True,\n",
    "    transformation_method = 'yeo-johnson',\n",
    "    experiment_name='Clsfctn_tel_cust_ayushya_(dm)',\n",
    "    log_experiment = False,\n",
    "    normalize=True,  # True, False\n",
    "    normalize_method='zscore',  # 'zscore', 'minmax', 'maxabs', 'robust'\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train and compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = compare_models(fold = 5,\n",
    "                             n_select=1,\n",
    "                             sort='f1',\n",
    "\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Save ML Flow and analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Chose and analyse the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = create_model('lr')\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01,0.05, 0.1,0.5, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Tune the model\n",
    "tuned_model = tune_model(lr_model, custom_grid=param_grid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gbc_model = create_model('gbc')\n",
    "\n",
    "# Define a custom grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Tune the model for a better F1 score\n",
    "tuned_model = tune_model(gbc_model, custom_grid=param_grid, optimize='F1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Analyse the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model,plot='learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model,plot='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model,plot='confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model,plot='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Finalize and predict and save the chosen model  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
