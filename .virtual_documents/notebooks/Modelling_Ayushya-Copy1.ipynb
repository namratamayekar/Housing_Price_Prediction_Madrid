# Import necessary libraries
import pandas as pd
import numpy as np

# Visualizationdf_e.shape
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("fivethirtyeight")

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score




import warnings
warnings.filterwarnings("ignore")


# Load the dataset
#df_n = pd.read_csv('../data/processed/cleaned_data_Namrata_NAN.csv')
df_e = pd.read_csv('../data/raw/houses_Madrid.csv')




df_e.columns


df_ = df_e[['sq_mt_built','n_rooms', 'n_bathrooms', 'floor', 'buy_price', 'is_renewal_needed',
       'built_year', 'has_lift', 'is_exterior', 'neighborhood_id']]


 # get neightbourhood and district column
chop = df_['neighborhood_id'].str.extract(r'Neighborhood (\d+): (.*?) \(.*\) - District (\d+): (.*)')
df_['neighbourhood']= chop[1]+ ' ' +chop[0]
df_['district']= chop[3] + ' ' + chop[2]


df_.drop(columns=['neighborhood_id','neighbourhood'],inplace=True)


df_.head(2)


# Correlation heatmap
numeric_cols = df_.select_dtypes(include=[np.number]).columns#

df_[numeric_cols].corr()['buy_price'].sort_values(ascending=False)





df_.duplicated()


df_without_duplicates = df_.drop_duplicates()


df_without_duplicates.isna().sum()


mean_year = df_without_duplicates['built_year'].mean()
round(mean_year)
df_without_duplicates['built_year'].fillna(round(mean_year), inplace=True)


df_without_duplicates.isna().sum()


df_without_duplicates_without_na = df_without_duplicates.dropna()


df_without_duplicates_without_na.isna().sum()


df = df_without_duplicates_without_na


sns.histplot(df['buy_price'],kde = True,bins=50)


# This distribution is skewed - this can be improved to make it more normally distributed


q = df['buy_price'].quantile(0.90)

df = df[df['buy_price']<q]

sns.histplot(df['buy_price'],kde = True,bins=50)


df['built_year'].value_counts()



df = df[(df['built_year']>1850) & (df['built_year']< 2024)]
sns.histplot(df['built_year'],kde = True,bins=50)


df.shape


df['floor'].value_counts().index


# Dictionary mapping numbers to words
number_to_word = {
    '1': 'one', '2': 'two', '3': 'three', '4': 'four', '5': 'five',
    '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine', 'Bajo': 'zero',
    'Semi-s贸tano exterior':'semi-basement','Semi-s贸tano interior':'semi-basement',
    'Entreplanta interior':'mezzanine', 'S贸tano interior':'basement',
    'S贸tano exterior':'basement','Entreplanta exterior':'mezzanine'
}

# Replace numbers with words directly using the dictionary
df['floor'] = df['floor'].replace(number_to_word)


df['floor'].value_counts()


#conda install -c conda-forge pycaret


from pycaret.regression import *


# Setup the environment in PyCaret
regression_setup = setup(
    data=df,
    #test_data=test_data,
    target = 'buy_price',
    session_id=9,
    ignore_features=[],
    categorical_features=['district','floor'],
    numeric_imputation = 'mean',
    categorical_imputation = 'mode',
    remove_multicollinearity=True,
    transformation= True,
    transformation_method = 'yeo-johnson',
    normalize = True,
    normalize_method = 'zscore'
    )


# compare baseline models
best_model = compare_models()


# predict on test set
holdout_pred = predict_model(best_model)



# show predictions df
holdout_pred[['buy_price','prediction_label']].head()
